"""
Solar API ì•™ìƒë¸” ëª¨ë“ˆ

Fine-tuned ëª¨ë¸ê³¼ Solar APIë¥¼ ê²°í•©í•œ ì•™ìƒë¸” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ê°€ì¤‘ì¹˜ ê¸°ë°˜ íˆ¬í‘œì™€ ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os
import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass, field
import numpy as np
import pandas as pd
from tqdm import tqdm
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    GenerationConfig
)
from openai import OpenAI
import evaluate

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EnsembleConfig:
    """ì•™ìƒë¸” ì„¤ì •"""
    # Solar API ì„¤ì •
    solar_api_key: str
    solar_model: str = "solar-1-mini-chat"
    solar_base_url: str = "https://api.upstage.ai/v1/solar"
    
    # ê°€ì¤‘ì¹˜ ì„¤ì •
    fine_tuned_weight: float = 0.7
    solar_weight: float = 0.3
    dynamic_weights: bool = False
    
    # API ì„¤ì •
    max_retries: int = 3
    retry_delay: int = 5
    rate_limit_per_minute: int = 100
    timeout: int = 30
    
    # ìƒì„± ì„¤ì •
    temperature: float = 0.3
    top_p: float = 0.9
    max_length: int = 200
    min_length: int = 30
    
    # ë°°ì¹˜ ì„¤ì •
    batch_size: int = 8
    use_async: bool = True
    
    # ìºì‹œ ì„¤ì •
    use_cache: bool = True
    cache_dir: str = "cache/solar_ensemble"


@dataclass
class EnsembleResult:
    """ì•™ìƒë¸” ê²°ê³¼"""
    dialogue: str
    fine_tuned_summary: str
    solar_summary: str
    ensemble_summary: str
    weights_used: Dict[str, float]
    confidence_score: float
    processing_time: float
    metadata: Dict[str, Any] = field(default_factory=dict)


class SolarAPIClient:
    """Solar API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: EnsembleConfig):
        self.config = config
        self.client = OpenAI(
            api_key=config.solar_api_key,
            base_url=config.solar_base_url
        )
        self.request_count = 0
        self.last_request_time = time.time()
        
        # ìºì‹œ ì´ˆê¸°í™”
        if config.use_cache:
            self.cache_dir = Path(config.cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self.cache = self._load_cache()
        else:
            self.cache = {}
    
    def _load_cache(self) -> Dict:
        """ìºì‹œ ë¡œë“œ"""
        cache_file = self.cache_dir / "solar_cache.json"
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_cache(self):
        """ìºì‹œ ì €ì¥"""
        cache_file = self.cache_dir / "solar_cache.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
    
    def _check_rate_limit(self):
        """Rate limit í™•ì¸"""
        current_time = time.time()
        time_since_start = current_time - self.last_request_time
        
        if self.request_count >= self.config.rate_limit_per_minute:
            if time_since_start < 60:
                sleep_time = 60 - time_since_start + 1
                logger.info(f"Rate limit reached. Sleeping for {sleep_time:.1f} seconds...")
                time.sleep(sleep_time)
                self.request_count = 0
                self.last_request_time = time.time()
        
        if time_since_start >= 60:
            self.request_count = 0
            self.last_request_time = current_time
    
    def build_prompt(self, dialogue: str, few_shot_examples: Optional[List[Dict]] = None) -> List[Dict]:
        """í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        system_prompt = (
            "You are an expert in Korean dialogue summarization. "
            "Summarize the given Korean dialogue in a concise and informative manner. "
            "Preserve all important information including speaker identities and key points. "
            "The summary should be in Korean."
        )
        
        if few_shot_examples:
            # Few-shot í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            messages = [{"role": "system", "content": system_prompt}]
            
            for example in few_shot_examples[:2]:  # ìµœëŒ€ 2ê°œ ì˜ˆì œ ì‚¬ìš©
                messages.extend([
                    {
                        "role": "user",
                        "content": f"ëŒ€í™”:\n{example['dialogue']}\n\nìš”ì•½:"
                    },
                    {
                        "role": "assistant",
                        "content": example['summary']
                    }
                ])
            
            messages.append({
                "role": "user",
                "content": f"ëŒ€í™”:\n{dialogue}\n\nìš”ì•½:"
            })
        else:
            # Zero-shot í”„ë¡¬í”„íŠ¸
            user_prompt = (
                "ë‹¤ìŒ í•œêµ­ì–´ ëŒ€í™”ë¥¼ ì½ê³  í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.\n"
                "í™”ì ì •ë³´(#Person1#, #Person2# ë“±)ì™€ ê°œì¸ì •ë³´(#PhoneNumber# ë“±)ë¥¼ ëª¨ë‘ í¬í•¨í•´ì£¼ì„¸ìš”.\n\n"
                f"ëŒ€í™”:\n{dialogue}\n\n"
                "ìš”ì•½:"
            )
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        
        return messages
    
    def summarize(self, dialogue: str, few_shot_examples: Optional[List[Dict]] = None) -> str:
        """ì•ˆì •ì„± ê°•í™”ëœ Solar API ìš”ì•½
        
        ê°œì„ ì‚¬í•­:
        - API í‚¤ ê²€ì¦ ë° ì—°ê²° í…ŒìŠ¤íŠ¸
        - ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
        - ì—°ì† ì‹¤íŒ¨ ì‹œ í´ë°± ë©”ì»¤ë‹ˆì¦˜
        - ë¹„ìš© ìµœì í™”ëœ ìºì‹±
        - ìì„¸í•œ ì˜¤ë¥˜ ë¡œê¹…
        """
        # API í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ ê²€ì¦
        if not hasattr(self, 'client') or not self.client:
            logger.warning("âš ï¸ Solar API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - í´ë°± ëª¨ë“œ")
            return ""
            
        # ìºì‹œ í™•ì¸
        cache_key = dialogue[:100]  # ëŒ€í™”ì˜ ì²˜ìŒ 100ìë¥¼ í‚¤ë¡œ ì‚¬ìš©
        if self.config.use_cache and cache_key in self.cache:
            logger.debug("ğŸ’¾ ìºì‹œëœ Solar ìš”ì•½ ì‚¬ìš©")
            return self.cache[cache_key]
        
        # ë¹„ìœ¨ ì œí•œ í™•ì¸
        self._check_rate_limit()
        
        # API í˜¸ì¶œ ì‹œë„
        messages = self.build_prompt(dialogue, few_shot_examples)
        last_error = None
        
        for attempt in range(self.config.max_retries):
            try:
                start_time = time.time()
                
                response = self.client.chat.completions.create(
                    model=self.config.solar_model,
                    messages=messages,
                    temperature=self.config.temperature,
                    top_p=self.config.top_p,
                    max_tokens=self.config.max_length,
                    timeout=self.config.timeout
                )
                
                if not response.choices or not response.choices[0].message.content:
                    raise Exception("ë¹„ì–´ìˆëŠ” ì‘ë‹µ ìˆ˜ì‹ ")
                
                summary = response.choices[0].message.content.strip()
                self.request_count += 1
                processing_time = time.time() - start_time
                
                # ì„±ê³µ ë¡œê¹…
                logger.debug(f"âœ… Solar API ì„±ê³µ (attempt {attempt + 1}, {processing_time:.2f}s)")
                
                # ìºì‹œ ì €ì¥
                if self.config.use_cache:
                    self.cache[cache_key] = summary
                    if len(self.cache) % 100 == 0:  # 100ê°œë§ˆë‹¤ ì €ì¥
                        self._save_cache()
                
                return summary
                
            except Exception as e:
                last_error = e
                error_type = type(e).__name__
                
                # ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹…
                logger.warning(
                    f"âš ï¸ Solar API ì˜¤ë¥˜ (attempt {attempt + 1}/{self.config.max_retries}): "
                    f"{error_type} - {str(e)}"
                )
                
                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„
                if attempt < self.config.max_retries - 1:
                    # ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©
                    delay = self.config.retry_delay * (2 ** attempt)
                    delay = min(delay, 60)  # ìµœëŒ€ 60ì´ˆ
                    
                    logger.info(f"â±ï¸ {delay}ì´ˆ í›„ ì¬ì‹œë„... ({error_type})")
                    time.sleep(delay)
                    
                    # íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ì¸ ê²½ìš° íƒ€ì„ì•„ì›ƒ ì¦ê°€
                    if "timeout" in str(e).lower() or "timed out" in str(e).lower():
                        original_timeout = self.config.timeout
                        self.config.timeout = min(self.config.timeout + 15, 120)
                        logger.info(f"íƒ€ì„ì•„ì›ƒ ì¦ê°€: {original_timeout}ìˆ˜ â†’ {self.config.timeout}ìˆ˜")
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        logger.error(
            f"âŒ Solar API ì™„ì „ ì‹¤íŒ¨ ({self.config.max_retries}íšŒ ì‹œë„): "
            f"{type(last_error).__name__} - {str(last_error)}"
        )
        
        return ""
    async def summarize_async(self, dialogue: str, few_shot_examples: Optional[List[Dict]] = None) -> str:
        """ë¹„ë™ê¸° Solar API ìš”ì•½"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.summarize, dialogue, few_shot_examples)
    
    def close(self):
        """í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ"""
        if self.config.use_cache:
            self._save_cache()


class WeightedEnsemble:
    """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸”"""
    
    def __init__(
        self,
        fine_tuned_model_path: str,
        ensemble_config: EnsembleConfig,
        device: Optional[str] = None
    ):
        self.config = ensemble_config
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # Fine-tuned ëª¨ë¸ ë¡œë“œ
        logger.info(f"Loading fine-tuned model from {fine_tuned_model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(fine_tuned_model_path)
        self.model.to(self.device)
        self.model.eval()
        
        # Solar API í´ë¼ì´ì–¸íŠ¸
        self.solar_client = SolarAPIClient(ensemble_config)
        
        # ROUGE í‰ê°€ê¸°
        self.rouge = evaluate.load("rouge")
        
        # ë™ì  ê°€ì¤‘ì¹˜ íˆìŠ¤í† ë¦¬
        self.weight_history = []
        
        # Few-shot ì˜ˆì œ (ì„ íƒì )
        self.few_shot_examples = []
    
    def load_few_shot_examples(self, train_file: str, num_examples: int = 3):
        """Few-shot ì˜ˆì œ ë¡œë“œ"""
        df = pd.read_csv(train_file)
        # ë†’ì€ í’ˆì§ˆì˜ ì˜ˆì œ ì„ íƒ (ê¸¸ì´ ê¸°ë°˜)
        df['summary_len'] = df['summary'].str.len()
        df_sorted = df.sort_values('summary_len', ascending=False)
        
        self.few_shot_examples = [
            {
                'dialogue': row['dialogue'],
                'summary': row['summary']
            }
            for _, row in df_sorted.head(num_examples).iterrows()
        ]
        logger.info(f"Loaded {len(self.few_shot_examples)} few-shot examples")
    
    def generate_fine_tuned_summary(self, dialogue: str) -> str:
        """Fine-tuned ëª¨ë¸ì„ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„±"""
        inputs = self.tokenizer(
            dialogue,
            max_length=512,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=self.config.max_length,
                min_length=self.config.min_length,
                num_beams=4,
                length_penalty=1.0,
                no_repeat_ngram_size=3,
                early_stopping=True
            )
        
        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return summary.strip()
    
    def calculate_dynamic_weights(
        self,
        fine_tuned_summary: str,
        solar_summary: str,
        dialogue: str
    ) -> Dict[str, float]:
        """ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        if not self.config.dynamic_weights:
            return {
                'fine_tuned': self.config.fine_tuned_weight,
                'solar': self.config.solar_weight
            }
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = {
            'dialogue_length': len(dialogue.split()),
            'fine_tuned_length': len(fine_tuned_summary.split()),
            'solar_length': len(solar_summary.split()),
            'special_tokens_in_dialogue': len([t for t in ['#Person', '#Phone', '#Address'] 
                                              if t in dialogue]),
            'special_tokens_preserved_ft': len([t for t in ['#Person', '#Phone', '#Address'] 
                                               if t in fine_tuned_summary]),
            'special_tokens_preserved_solar': len([t for t in ['#Person', '#Phone', '#Address'] 
                                                 if t in solar_summary])
        }
        
        # ê°€ì¤‘ì¹˜ ì¡°ì • ê·œì¹™
        weight_ft = self.config.fine_tuned_weight
        weight_solar = self.config.solar_weight
        
        # íŠ¹ìˆ˜ í† í° ë³´ì¡´ ê¸°ë°˜ ì¡°ì •
        if features['special_tokens_preserved_ft'] > features['special_tokens_preserved_solar']:
            weight_ft += 0.1
            weight_solar -= 0.1
        elif features['special_tokens_preserved_solar'] > features['special_tokens_preserved_ft']:
            weight_ft -= 0.1
            weight_solar += 0.1
        
        # ê¸¸ì´ ê· í˜• ê¸°ë°˜ ì¡°ì •
        ideal_length = features['dialogue_length'] * 0.3  # ëŒ€í™”ì˜ 30% ê¸¸ì´ê°€ ì´ìƒì 
        ft_diff = abs(features['fine_tuned_length'] - ideal_length)
        solar_diff = abs(features['solar_length'] - ideal_length)
        
        if ft_diff < solar_diff:
            weight_ft += 0.05
            weight_solar -= 0.05
        else:
            weight_ft -= 0.05
            weight_solar += 0.05
        
        # ì •ê·œí™”
        total = weight_ft + weight_solar
        weights = {
            'fine_tuned': weight_ft / total,
            'solar': weight_solar / total
        }
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.weight_history.append({
            'weights': weights,
            'features': features
        })
        
        return weights
    
    def combine_summaries(
        self,
        fine_tuned_summary: str,
        solar_summary: str,
        weights: Dict[str, float]
    ) -> str:
        """ìš”ì•½ ê²°í•©"""
        # ê°„ë‹¨í•œ ë°©ë²•: ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ ìš”ì•½ ì„ íƒ
        if weights['fine_tuned'] >= weights['solar']:
            primary_summary = fine_tuned_summary
            secondary_summary = solar_summary
            primary_weight = weights['fine_tuned']
        else:
            primary_summary = solar_summary
            secondary_summary = fine_tuned_summary
            primary_weight = weights['solar']
        
        # ê°€ì¤‘ì¹˜ê°€ ì••ë„ì ì´ë©´ ì£¼ìš” ìš”ì•½ë§Œ ì‚¬ìš©
        if primary_weight > 0.8:
            return primary_summary
        
        # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë³´ì¡° ìš”ì•½ì—ì„œ ëˆ„ë½ëœ ì •ë³´ ì¶”ê°€
        primary_tokens = set(primary_summary.split())
        secondary_tokens = set(secondary_summary.split())
        
        # íŠ¹ìˆ˜ í† í° í™•ì¸
        special_tokens = ['#Person1#', '#Person2#', '#Person3#', 
                         '#PhoneNumber#', '#Address#', '#Email#']
        
        missing_special = []
        for token in special_tokens:
            if token in secondary_summary and token not in primary_summary:
                missing_special.append(token)
        
        # ëˆ„ë½ëœ íŠ¹ìˆ˜ í† í°ì´ ìˆìœ¼ë©´ ë³´ì¡° ìš”ì•½ ì‚¬ìš©
        if missing_special:
            return secondary_summary
        
        return primary_summary
    
    def calculate_confidence(
        self,
        fine_tuned_summary: str,
        solar_summary: str,
        ensemble_summary: str
    ) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        # ë‘ ìš”ì•½ ê°„ì˜ ROUGE ì ìˆ˜ë¡œ ì¼ì¹˜ë„ ì¸¡ì •
        rouge_scores = self.rouge.compute(
            predictions=[fine_tuned_summary],
            references=[solar_summary]
        )
        
        # F1 ì ìˆ˜ í‰ê· 
        agreement_score = np.mean([
            rouge_scores['rouge1'],
            rouge_scores['rouge2'],
            rouge_scores['rougeL']
        ])
        
        # íŠ¹ìˆ˜ í† í° ë³´ì¡´ ì ìˆ˜
        special_tokens = ['#Person', '#Phone', '#Address', '#Email']
        preservation_score = sum(
            1 for token in special_tokens if token in ensemble_summary
        ) / len(special_tokens)
        
        # ì¢…í•© ì‹ ë¢°ë„
        confidence = 0.7 * agreement_score + 0.3 * preservation_score
        
        return float(confidence)
    
    def process_single(self, dialogue: str) -> EnsembleResult:
        """ë‹¨ì¼ ëŒ€í™” ì²˜ë¦¬ - í´ë°± ë©”ì»¤ë‹ˆì¦˜ ê°•í™”"""
        start_time = time.time()
        
        # Fine-tuned ëª¨ë¸ ìš”ì•½
        try:
            fine_tuned_summary = self.generate_fine_tuned_summary(dialogue)
        except Exception as e:
            logger.error(f"âŒ Fine-tuned ëª¨ë¸ ì˜¤ë¥˜: {str(e)}")
            fine_tuned_summary = "ëŒ€í™” ìš”ì•½ ì‹¤íŒ¨"  # ê¸°ë³¸ í´ë°±
        
        # Solar API ìš”ì•½
        solar_summary = ""
        solar_failed = False
        
        try:
            solar_summary = self.solar_client.summarize(dialogue, self.few_shot_examples)
            if not solar_summary:  # ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ ì‹œ ì‹¤íŒ¨ë¡œ ê°„ì£¼
                solar_failed = True
                logger.warning("âš ï¸ Solar APIê°€ ë¹ˆ ìš”ì•½ ë°˜í™˜")
        except Exception as e:
            solar_failed = True
            logger.error(f"âŒ Solar API ì˜¤ë¥˜: {str(e)}")
        
        # í´ë°± ë©”ì»¤ë‹ˆì¦˜
        if solar_failed or not solar_summary:
            logger.info("ğŸ”„ Solar API ì‹¤íŒ¨ - Fine-tuned ëª¨ë¸ë§Œ ì‚¬ìš© (Fallback ëª¨ë“œ)")
            ensemble_summary = fine_tuned_summary
            weights = {'fine_tuned': 1.0, 'solar': 0.0}
            confidence = 0.7  # Fine-tuned ëª¨ë¸ë§Œ ì‚¬ìš© ì‹œ ê¸°ë³¸ ì‹ ë¢°ë„
            solar_summary = "ì‚¬ìš© ë¶ˆê°€ (í´ë°± ëª¨ë“œ)"
        else:
            # ì •ìƒ ì•™ìƒë¸” ì²˜ë¦¬
            weights = self.calculate_dynamic_weights(
                fine_tuned_summary,
                solar_summary,
                dialogue
            )
            
            ensemble_summary = self.combine_summaries(
                fine_tuned_summary,
                solar_summary,
                weights
            )
            
            confidence = self.calculate_confidence(
                fine_tuned_summary,
                solar_summary,
                ensemble_summary
            )
        
        processing_time = time.time() - start_time
        return EnsembleResult(
            dialogue=dialogue,
            fine_tuned_summary=fine_tuned_summary,
            solar_summary=solar_summary,
            ensemble_summary=ensemble_summary,
            weights_used=weights,
            confidence_score=confidence,
            processing_time=processing_time,
            metadata={
                'solar_failed': solar_failed,
                'fallback_mode': solar_failed or not solar_summary
            }
        )
    
    async def process_batch_async(self, dialogues: List[str]) -> List[EnsembleResult]:
        """ë°°ì¹˜ ë¹„ë™ê¸° ì²˜ë¦¬"""
        tasks = []
        
        for dialogue in dialogues:
            task = asyncio.create_task(self._process_single_async(dialogue))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results
    
    async def _process_single_async(self, dialogue: str) -> EnsembleResult:
        """ë‹¨ì¼ ëŒ€í™” ë¹„ë™ê¸° ì²˜ë¦¬"""
        start_time = time.time()
        
        # Fine-tuned ëª¨ë¸ ìš”ì•½ (ë™ê¸°)
        loop = asyncio.get_event_loop()
        fine_tuned_summary = await loop.run_in_executor(
            None,
            self.generate_fine_tuned_summary,
            dialogue
        )
        
        # Solar API ìš”ì•½ (ë¹„ë™ê¸°)
        solar_summary = await self.solar_client.summarize_async(
            dialogue,
            self.few_shot_examples
        )
        
        # ë‚˜ë¨¸ì§€ ì²˜ë¦¬ (ë™ê¸°)
        weights = self.calculate_dynamic_weights(
            fine_tuned_summary,
            solar_summary,
            dialogue
        )
        
        ensemble_summary = self.combine_summaries(
            fine_tuned_summary,
            solar_summary,
            weights
        )
        
        confidence = self.calculate_confidence(
            fine_tuned_summary,
            solar_summary,
            ensemble_summary
        )
        
        processing_time = time.time() - start_time
        
        return EnsembleResult(
            dialogue=dialogue,
            fine_tuned_summary=fine_tuned_summary,
            solar_summary=solar_summary,
            ensemble_summary=ensemble_summary,
            weights_used=weights,
            confidence_score=confidence,
            processing_time=processing_time
        )
    
    def process_dataset(
        self,
        data_file: str,
        output_file: str,
        sample_size: Optional[int] = None
    ) -> pd.DataFrame:
        """ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_file)
        if sample_size:
            df = df.head(sample_size)
        
        results = []
        
        # ë°°ì¹˜ ì²˜ë¦¬
        for i in tqdm(range(0, len(df), self.config.batch_size), desc="Processing"):
            batch = df.iloc[i:i + self.config.batch_size]
            dialogues = batch['dialogue'].tolist()
            
            if self.config.use_async:
                # ë¹„ë™ê¸° ì²˜ë¦¬
                batch_results = asyncio.run(self.process_batch_async(dialogues))
            else:
                # ë™ê¸° ì²˜ë¦¬
                batch_results = [self.process_single(d) for d in dialogues]
            
            results.extend(batch_results)
        
        # ê²°ê³¼ ì •ë¦¬
        output_df = pd.DataFrame([
            {
                'fname': df.iloc[i]['fname'],
                'dialogue': r.dialogue,
                'fine_tuned_summary': r.fine_tuned_summary,
                'solar_summary': r.solar_summary,
                'ensemble_summary': r.ensemble_summary,
                'fine_tuned_weight': r.weights_used['fine_tuned'],
                'solar_weight': r.weights_used['solar'],
                'confidence_score': r.confidence_score,
                'processing_time': r.processing_time
            }
            for i, r in enumerate(results)
        ])
        
        # ì €ì¥
        output_df.to_csv(output_file, index=False, encoding='utf-8')
        logger.info(f"Results saved to {output_file}")
        
        # í†µê³„ ì¶œë ¥
        avg_confidence = output_df['confidence_score'].mean()
        avg_time = output_df['processing_time'].mean()
        
        logger.info(f"Average confidence score: {avg_confidence:.3f}")
        logger.info(f"Average processing time: {avg_time:.2f} seconds")
        
        if self.config.dynamic_weights:
            avg_ft_weight = output_df['fine_tuned_weight'].mean()
            avg_solar_weight = output_df['solar_weight'].mean()
            logger.info(f"Average weights - Fine-tuned: {avg_ft_weight:.3f}, Solar: {avg_solar_weight:.3f}")
        
        return output_df
    
    def evaluate(self, predictions_file: str, references_file: str) -> Dict[str, float]:
        """ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€"""
        pred_df = pd.read_csv(predictions_file)
        ref_df = pd.read_csv(references_file)
        
        # ì •ë ¬
        pred_df = pred_df.sort_values('fname').reset_index(drop=True)
        ref_df = ref_df.sort_values('fname').reset_index(drop=True)
        
        # ê° ëª¨ë¸ë³„ í‰ê°€
        results = {}
        
        for col_name, pred_col in [
            ('fine_tuned', 'fine_tuned_summary'),
            ('solar', 'solar_summary'),
            ('ensemble', 'ensemble_summary')
        ]:
            if pred_col in pred_df.columns:
                rouge_scores = self.rouge.compute(
                    predictions=pred_df[pred_col].tolist(),
                    references=ref_df['summary'].tolist()
                )
                
                results[f'{col_name}_rouge1'] = rouge_scores['rouge1']
                results[f'{col_name}_rouge2'] = rouge_scores['rouge2']
                results[f'{col_name}_rougeL'] = rouge_scores['rougeL']
                results[f'{col_name}_rouge_avg'] = np.mean([
                    rouge_scores['rouge1'],
                    rouge_scores['rouge2'],
                    rouge_scores['rougeL']
                ])
        
        # ê°œì„ ìœ¨ ê³„ì‚°
        if 'fine_tuned_rouge_avg' in results and 'ensemble_rouge_avg' in results:
            improvement = (
                (results['ensemble_rouge_avg'] - results['fine_tuned_rouge_avg']) /
                results['fine_tuned_rouge_avg'] * 100
            )
            results['improvement_percent'] = improvement
        
        return results
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.solar_client.close()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Solar API Ensemble System")
    parser.add_argument("--fine_tuned_model", type=str, required=True,
                       help="Path to fine-tuned model")
    parser.add_argument("--solar_api_key", type=str, required=True,
                       help="Solar API key")
    parser.add_argument("--data_file", type=str, required=True,
                       help="Input data file")
    parser.add_argument("--output_file", type=str, default="ensemble_results.csv",
                       help="Output file")
    parser.add_argument("--sample_size", type=int, default=None,
                       help="Number of samples to process")
    parser.add_argument("--batch_size", type=int, default=8,
                       help="Batch size")
    parser.add_argument("--dynamic_weights", action="store_true",
                       help="Use dynamic weights")
    parser.add_argument("--fine_tuned_weight", type=float, default=0.7,
                       help="Weight for fine-tuned model")
    parser.add_argument("--solar_weight", type=float, default=0.3,
                       help="Weight for Solar API")
    parser.add_argument("--train_file", type=str, default=None,
                       help="Training file for few-shot examples")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = EnsembleConfig(
        solar_api_key=args.solar_api_key,
        fine_tuned_weight=args.fine_tuned_weight,
        solar_weight=args.solar_weight,
        dynamic_weights=args.dynamic_weights,
        batch_size=args.batch_size
    )
    
    # ì•™ìƒë¸” ìƒì„±
    ensemble = WeightedEnsemble(
        fine_tuned_model_path=args.fine_tuned_model,
        ensemble_config=config
    )
    
    # Few-shot ì˜ˆì œ ë¡œë“œ (ì„ íƒì )
    if args.train_file:
        ensemble.load_few_shot_examples(args.train_file)
    
    # ì²˜ë¦¬ ì‹¤í–‰
    try:
        results = ensemble.process_dataset(
            data_file=args.data_file,
            output_file=args.output_file,
            sample_size=args.sample_size
        )
        
        # í‰ê°€ (validation setì¸ ê²½ìš°)
        if 'dev' in args.data_file or 'val' in args.data_file:
            eval_results = ensemble.evaluate(args.output_file, args.data_file)
            
            print("\n=== Evaluation Results ===")
            for metric, value in eval_results.items():
                print(f"{metric}: {value:.4f}")
    
    finally:
        ensemble.close()


if __name__ == "__main__":
    main()
