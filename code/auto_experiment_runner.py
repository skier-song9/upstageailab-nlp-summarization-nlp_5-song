#!/usr/bin/env python3
"""
ìë™ ì‹¤í—˜ ì‹¤í–‰ ì‹œìŠ¤í…œ (ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€, MPS/CUDA ìµœì í™”)

ì—¬ëŸ¬ YAML ì„¤ì •ì„ ìˆœì°¨ì ìœ¼ë¡œ ìë™ ì‹¤í–‰í•˜ëŠ” ì‹œìŠ¤í…œ
- ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€ íŒŒì¼ ì²˜ë¦¬
- MPS/CUDA ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ìµœì í™”
- ì‹¤í—˜ ê²°ê³¼ ìë™ ì¶”ì  ë° ë¶„ì„
"""

import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import subprocess
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# code ë””ë ‰í† ë¦¬ì˜ utils ì„í¬íŠ¸
from utils.path_utils import PathManager, path_manager
from utils.device_utils import get_optimal_device, setup_device_config
from utils.experiment_utils import ExperimentTracker, ModelRegistry
from utils.csv_results_saver import CSVResultsSaver
from utils import load_config
# ğŸ†• ì¶”ê°€: ì²´í¬í¬ì¸íŠ¸ íƒìƒ‰ê¸°ì™€ CSV ê´€ë¦¬ì
from utils.checkpoint_finder import CheckpointFinder
from utils.competition_csv_manager import CompetitionCSVManager
from utils import load_config


class AutoExperimentRunner:
    """ìë™ ì‹¤í—˜ ì‹¤í–‰ ê´€ë¦¬ì (ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€)"""
    def __init__(self, 
                 base_config_path: str = "config/base_config.yaml",
                 output_dir: str = "outputs/auto_experiments"):
        """
        Args:
            base_config_path: ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
            output_dir: ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œ)
        """
        # ìƒëŒ€ ê²½ë¡œ í™•ì¸
        if Path(base_config_path).is_absolute() or Path(output_dir).is_absolute():
            raise ValueError("ëª¨ë“  ê²½ë¡œëŠ” ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤")
        
        self.base_config_path = path_manager.resolve_path(base_config_path)
        self.output_dir = path_manager.ensure_dir(output_dir)
        
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        device_tuple = get_optimal_device()
        if isinstance(device_tuple, tuple):
            self.device = device_tuple[0]  # torch.device ê°ì²´
            self.device_info = device_tuple[1] if len(device_tuple) > 1 else None
        else:
            self.device = device_tuple
            self.device_info = None
        
        # ì‹¤í—˜ ì¶”ì  ì´ˆê¸°í™”
        self.tracker = ExperimentTracker(f"{output_dir}/experiments")
        print(f"\nğŸ†— ExperimentTracker ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   log_experiment ë©”ì„œë“œ ì¡´ì¬: {hasattr(self.tracker, 'log_experiment')}")
        # CSV ê²°ê³¼ ì €ì¥ê¸° ì´ˆê¸°í™”
        self.csv_saver = CSVResultsSaver(f"{output_dir}/csv_results")
        
        # ğŸ†• ìƒˆë¡œ ì¶”ê°€: ì²´í¬í¬ì¸íŠ¸ íƒìƒ‰ê¸°ì™€ CSV ê´€ë¦¬ì
        self.checkpoint_finder = CheckpointFinder()
        self.csv_manager = CompetitionCSVManager()
        
        # ë¡œê¹… ì„¤ì •
        self.logger = self._setup_logger()
        
        print(f"ğŸš€ ìë™ ì‹¤í—˜ ì‹¤í–‰ê¸° ì´ˆê¸°í™”")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger = self._setup_logger()
        
        print(f"ğŸš€ ìë™ ì‹¤í—˜ ì‹¤í–‰ê¸° ì´ˆê¸°í™”")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        if self.device_info and hasattr(self.device_info, 'device_name'):
            print(f"   GPU ì •ë³´: {self.device_info.device_name} ({self.device_info.memory_gb:.1f}GB)")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    def _setup_logger(self) -> logging.Logger:
        log_file = path_manager.ensure_dir("logs") / "auto_experiments.log"
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        # ë¡œê·¸ íŒŒì¼ í•¸ë“¤ëŸ¬
        log_file = path_manager.ensure_dir("logs") / "auto_experiments.log"
        file_handler = logging.FileHandler(log_file)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        return logger
    
    def run_experiments(self, 
                       experiment_configs: List[str],
                       dry_run: bool = False,
                       continue_on_error: bool = True,
                       one_epoch: bool = False,
                       disable_eval: bool = False) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        
        Args:
            experiment_configs: ì‹¤í—˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ìƒëŒ€ ê²½ë¡œ)
            dry_run: ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ì„¤ì •ë§Œ í™•ì¸
            continue_on_error: ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ì‹¤í—˜ ê³„ì† ì§„í–‰
            one_epoch: 1ì—í¬í¬ë§Œ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
            disable_eval: í‰ê°€ ë¹„í™œì„±í™” (1ì—í¬í¬ ëª¨ë“œìš©)
            
        Returns:
            ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        
        for i, config_path in enumerate(experiment_configs):
            try:
                print(f"\n{'='*60}")
                print(f"ì‹¤í—˜ {i+1}/{len(experiment_configs)}: {config_path}")
                print(f"{'='*60}")
                
                # ìƒëŒ€ ê²½ë¡œ í™•ì¸
                if Path(config_path).is_absolute():
                    raise ValueError(f"ì„¤ì • ê²½ë¡œëŠ” ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤: {config_path}")
                
                # ì„¤ì • ë¡œë“œ
                full_config = self._load_and_merge_config(config_path)
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì • ì ìš©
                self._apply_device_config(full_config)
                
                # WandB í™˜ê²½ ì„¤ì • (ë¯¸ë¦¬ ì„¤ì •í•˜ì—¬ trainer.pyì—ì„œ ìë™ í™œìš©)
                wandb_enabled = self.setup_wandb_environment(full_config)
                
                if dry_run:
                    print("\n[DRY RUN] ì„¤ì • ë‚´ìš©:")
                    print(json.dumps(full_config, indent=2, ensure_ascii=False))
                    results[config_path] = {"status": "dry_run", "config": full_config}
                    continue
                
                # ì‹¤í—˜ ì‹¤í–‰
                result = self._run_single_experiment(full_config, config_path, one_epoch, disable_eval)
                results[config_path] = result
                
                # ì‹¤í—˜ ì¶”ì  - try-except ë¸”ë¡ ì¶”ê°€
                try:
                    self.tracker.log_experiment(
                        experiment_name=Path(config_path).stem,
                        config=full_config,
                        results=result
                    )
                except Exception as e:
                    self.logger.warning(f"ì‹¤í—˜ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
                    # ë¡œê·¸ ì‹¤íŒ¨ê°€ ì „ì²´ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ì§€ ì•Šë„ë¡ í•¨
                
                # ì‹¤í—˜ ê°„ ëŒ€ê¸° (GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ë“±)
                if i < len(experiment_configs) - 1:
                    print("\në‹¤ìŒ ì‹¤í—˜ ì¤€ë¹„ ì¤‘...")
                    time.sleep(5)
                    
            except Exception as e:
                self.logger.error(f"ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {config_path}", exc_info=True)
                results[config_path] = {"status": "error", "error": str(e)}
                
                if not continue_on_error:
                    raise
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        self._print_summary(results)
        
        # ì „ì²´ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_csv = self.csv_saver.save_batch_results(
            results=results,
            output_filename=f"experiment_summary_{timestamp}.csv"
        )
        print(f"\nì‹¤í—˜ ê²°ê³¼ CSV ì €ì¥: {summary_csv}")
        
        return results
    
    def _load_and_merge_config(self, config_path: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì •ê³¼ ì‹¤í—˜ ì„¤ì •ì„ ë³‘í•©"""
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
        base_config = load_config(self.base_config_path)
        
        # ì‹¤í—˜ ì„¤ì • ë¡œë“œ
        exp_config_path = path_manager.resolve_path(config_path)
        exp_config = load_config(exp_config_path)
        
        # ì„¤ì • ë³‘í•© (ì‹¤í—˜ ì„¤ì •ì´ ìš°ì„ )
        merged = self._deep_merge(base_config, exp_config)
        
        return merged
    
    def _deep_merge(self, base: Dict, override: Dict) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ ê¹Šì€ ë³‘í•©"""
        result = base.copy()
        
        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
        
        return result
    
    def setup_wandb_environment(self, config: Dict[str, Any]) -> bool:
        """
        ì‹¤í—˜ë³„ WandB í™˜ê²½ ì„¤ì •
        
        Args:
            config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            
        Returns:
            WandB í™œì„±í™” ì—¬ë¶€
        """
        import os
        
        # í•­ìƒ WANDB_LOG_MODEL=end ì„¤ì • (best modelë§Œ ì €ì¥)
        os.environ["WANDB_LOG_MODEL"] = "end"
        os.environ["TOKENIZERS_PARALLELISM"] = "true"
        
        # report_to ì„¤ì • í™•ì¸
        report_to = config.get('training', {}).get('report_to', 'wandb')
        
        if report_to in ['all', 'wandb']:
            print("âœ… WandB í™œì„±í™”: best model artifacts ìë™ ì €ì¥")
            # WandB í™œì„±í™”ë¥¼ ìœ„í•œ í™˜ê²½ ì„¸íŒ…
            if "WANDB_MODE" in os.environ:
                del os.environ["WANDB_MODE"]
            
            # ì‹¤í—˜ì„ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ê³ ìœ í•œ ì„¤ì • ì¶”ê°€
            experiment_path = config.get('__config_path__', 'unknown')
            if experiment_path and experiment_path != 'unknown':
                # ì‹¤í—˜ ì´ë¦„ì„ WandB configì— ì¶”ê°€
                config['wandb_experiment_name'] = Path(experiment_path).stem
            
            return True
        else:
            print(f"âš ï¸ WandB ë¹„í™œì„±í™” (report_to={report_to})")
            os.environ["WANDB_MODE"] = "disabled"
            return False
    
    def _apply_device_config(self, config: Dict[str, Any]) -> None:
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ì ìš©"""
        if not self.device_info:
            return
            return
            
        # ëª¨ë¸ í¬ê¸° ì¶”ì •
        model_name = config.get('general', {}).get('model_name', '')
        if 'large' in model_name.lower() or 'xl' in model_name.lower():
            model_size = 'large'
        elif 'small' in model_name.lower() or 'tiny' in model_name.lower():
            model_size = 'small'
        else:
            model_size = 'base'
        
        # ìµœì í™” ì„¤ì • ìƒì„±
        opt_config = setup_device_config(self.device_info, model_size)
        
        # training ì„¹ì…˜ì— ì ìš©
        if 'training' not in config:
            config['training'] = {}
        
        # ê¸°ì¡´ ì„¤ì •ì„ ìœ ì§€í•˜ë©´ì„œ ë””ë°”ì´ìŠ¤ ìµœì í™” ì„¤ì • ì¶”ê°€
        training_config = config['training']
        opt_dict = opt_config.to_dict()
        
        for key, value in opt_dict.items():
            if key not in training_config:
                training_config[key] = value
        config['device'] = str(self.device)
        config['device_info'] = {
            'type': self.device_info.device_type,
            'name': self.device_info.device_name,
            'memory_gb': self.device_info.memory_gb
        } if hasattr(self.device_info, 'device_type') else None
    
    def _run_single_experiment(self, config: Dict[str, Any], config_path: str, one_epoch: bool = False, disable_eval: bool = False) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        print(f"\nğŸ”§ _run_single_experiment ì‹œì‘: {config_path}")
        start_time = time.time()
        
        try:
            # í•­ìƒ í™˜ê²½ ë³€ìˆ˜ ë³µì‚¬
            import os
            env = os.environ.copy()
            
            # WandB í™˜ê²½ ì„¤ì •
            wandb_enabled = self.setup_wandb_environment(config)
            
            # í•œêµ­ ì‹œê°„ ê¸°ë°˜ ì‹¤í—˜ ID ìƒì„±
            try:
                from utils.experiment_utils import get_korean_time_format
                korean_time = get_korean_time_format('MMDDHHMM')
                experiment_name = config.get('experiment_name', Path(config_path).stem)
                experiment_id = f"{experiment_name}_{korean_time}"
                print(f"ğŸ” ì‹¤í—˜ ID: {experiment_id}")
            except ImportError as e:
                print(f"âš ï¸ í•œêµ­ ì‹œê°„ ìœ í‹¸ë¦¬í‹° import ì‹¤íŒ¨: {e}")
                experiment_id = Path(config_path).stem
            
            # 1ì—í¬í¬ ëª¨ë“œë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            if one_epoch:
                env['FORCE_ONE_EPOCH'] = '1'
                print(f"\nğŸš€ 1ì—í¬í¬ ëª¨ë“œë¡œ ì‹¤í–‰: {Path(config_path).stem}")
            
            # trainer.py ì‹¤í–‰
            cmd = [
                sys.executable,
                str(path_manager.resolve_path("code/trainer.py")),
                "--config", config_path
            ]
            
            # 1ì—í¬í¬ ëª¨ë“œ ì˜µì…˜ ì¶”ê°€
            if one_epoch:
                cmd.append("--one-epoch")
            
            # í‰ê°€ ë¹„í™œì„±í™” ì˜µì…˜ ì¶”ê°€
            if disable_eval:
                cmd.append("--disable-eval")
            
            print(f"\nì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
            print(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
            print(f"Python ê²½ë¡œ: {sys.executable}")
            
            # í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                env=env
            )
            
            # ëª¨ë“  ì¶œë ¥ì„ ìˆ˜ì§‘í•˜ë©´ì„œ ì‹¤ì‹œê°„ í‘œì‹œ
            output_lines = []
            for line in process.stdout:
                print(line, end='')
                output_lines.append(line)
            
            # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
            process.wait()
            
            # ê²°ê³¼ ìˆ˜ì§‘
            if process.returncode == 0:
                
                # ğŸ†• í•™ìŠµ ì™„ë£Œ í›„ test.csv ì¶”ë¡  ìˆ˜í–‰
                print(f"\nğŸ“Š Test ì¶”ë¡  ì‹œì‘: {experiment_id}")
                # ğŸ†• í•™cìŠµ ì™„ë£Œ í›„ test.csv ì¶”ë¡  ìˆ˜í–‰
                print(f"\nğŸ“Š Test ì¶”ë¡  ì‹œì‘: {experiment_id}")
                
                try:
                    # ğŸ”§ ìˆ˜ì •: ì •í™•í•œ ì²´í¬í¬ì¸íŠ¸ íƒìƒ‰
                    best_checkpoint = self.checkpoint_finder.find_latest_checkpoint(experiment_id)
                    
                    if best_checkpoint and self.checkpoint_finder.validate_checkpoint(best_checkpoint):
                        print(f"ğŸ¯ ë°œê²¬ëœ ì²´í¬í¬ì¸íŠ¸: {best_checkpoint}")
                        
                        # ğŸ†• ê°•í™”ëœ ì¶”ë¡  ì‹¤í–‰ (2ë‹¨ê³„ í´ë°±)
                        submission_info = self._run_test_inference(
                            experiment_id=experiment_id,
                            checkpoint_path=best_checkpoint,
                            config=config
                        )
                        
                        result = self._collect_results(config, Path(config_path).stem)
                        result.update(submission_info)
                        
                    else:
                        print("âŒ ìœ íš¨í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        result = self._collect_results(config, Path(config_path).stem)
                        result['inference_error'] = "No valid checkpoint found"
                        
                except Exception as inf_e:
                    print(f"âŒ ì¶”ë¡  ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {inf_e}")
                    result = self._collect_results(config, Path(config_path).stem)
                    result['inference_error'] = str(inf_e)
                'error': str(e),
                'duration': time.time() - start_time
            }
        
        return result
    
    def _collect_results(self, config: Dict[str, Any], experiment_name: str) -> Dict[str, Any]:
        """ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘"""
        results = {
            'experiment_name': experiment_name,
            'model_name': config.get('general', {}).get('model_name', 'unknown')
        }
        
        # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        output_dir = Path(config.get('training', {}).get('output_dir', 'outputs'))
        
        # ë©”íŠ¸ë¦­ íŒŒì¼ ì½ê¸°
        metrics_file = output_dir / 'eval_results.json'
        if metrics_file.exists():
            with open(metrics_file, 'r') as f:
                metrics = json.load(f)
                results['metrics'] = metrics
        
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì •ë³´
        checkpoint_dirs = list(output_dir.glob('checkpoint-*'))
        if checkpoint_dirs:
            results['best_checkpoint'] = str(max(checkpoint_dirs, key=lambda p: p.stat().st_mtime))
        
        # CSV ê²°ê³¼ ì €ì¥
        if 'metrics' in results:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_path = self.csv_saver.save_experiment_results(
                experiment_name=experiment_name,
                config=config,
                metrics=results['metrics'],
                timestamp=timestamp
            )
            results['csv_path'] = str(csv_path)
        
        return results
    def _print_summary(self, results: Dict[str, Dict[str, Any]]) -> None:
        """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        success_count = sum(1 for r in results.values() if r.get('status') == 'success')
        total_count = len(results)
        
        print(f"\nì´ ì‹¤í—˜: {total_count}")
        print(f"ì„±ê³µ: {success_count}")
        print(f"ì‹¤íŒ¨: {total_count - success_count}")
        
        # ì„±ê³µí•œ ì‹¤í—˜ì˜ ë©”íŠ¸ë¦­ ë¹„êµ
        print("\në©”íŠ¸ë¦­ ë¹„êµ:")
        print(f"{'ì‹¤í—˜ëª…':<30} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10}")
        print("-" * 60)
        
        for config_path, result in results.items():
            exp_name = Path(config_path).stem[:30]
            if result.get('status') == 'success' and 'metrics' in result:
                metrics = result['metrics']
                rouge1 = metrics.get('eval_rouge1', 0)
                rouge2 = metrics.get('eval_rouge2', 0)
                rougeL = metrics.get('eval_rougeL', 0)
                print(f"{exp_name:<30} {rouge1:<10.4f} {rouge2:<10.4f} {rougeL:<10.4f}")
            else:
                status = result.get('status', 'unknown')
                print(f"{exp_name:<30} {status}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = None
        best_score = 0
        
        for config_path, result in results.items():
            if result.get('status') == 'success' and 'metrics' in result:
                metrics = result['metrics']
                score = (metrics.get('eval_rouge1', 0) + 
                        metrics.get('eval_rouge2', 0) + 
                        metrics.get('eval_rougeL', 0)) / 3
                if score > best_score:
                    best_score = score
                    best_model = Path(config_path).stem
        
        if best_model:
            print(f"\nìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (í‰ê·  ROUGE: {best_score:.4f})")
    
    def run_single_config(self, config_path: str, dry_run: bool = False) -> Dict[str, Any]:
        """ë‹¨ì¼ ì„¤ì • íŒŒì¼ë¡œ ì‹¤í—˜ ì‹¤í–‰"""
        return self.run_experiments([config_path], dry_run=dry_run)


def main():
    """CLI ì§„ì…ì """
    import argparse
    
    parser = argparse.ArgumentParser(description="ìë™ ì‹¤í—˜ ì‹¤í–‰ ì‹œìŠ¤í…œ")
    parser.add_argument(
        '--config', '-c',
        type=str,
        nargs='+',
        help='ì‹¤í—˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ, ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)'
    )
    parser.add_argument(
        '--base-config',
        type=str,
        default='config/base_config.yaml',
        help='ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: config/base_config.yaml)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='outputs/auto_experiments',
        help='ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: outputs/auto_experiments)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ì„¤ì •ë§Œ í™•ì¸'
    )
    parser.add_argument(
        '--stop-on-error',
        action='store_true',
        help='ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë‹¨ (ê¸°ë³¸ê°’: ê³„ì† ì§„í–‰)'
    )
    parser.add_argument(
        '--one-epoch',
        action='store_true',
        help='1ì—í¬í¬ë§Œ ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)'
    )
    parser.add_argument(
        '--disable-eval',
        action='store_true',
        help='í‰ê°€ ë¹„í™œì„±í™” (1ì—í¬í¬ ëª¨ë“œìš©)'
    )
    
    args = parser.parse_args()
    
    if not args.config:
        # ê¸°ë³¸ ì‹¤í—˜ ì„¸íŠ¸
        default_configs = [
            "config/experiments/01_baseline.yaml",
            "config/experiments/02_simple_augmentation.yaml",
            "config/experiments/03_high_learning_rate.yaml"
        ]
        print(f"ì„¤ì • íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:")
        for config in default_configs:
            print(f"  - {config}")
        args.config = default_configs
    
    # ì‹¤í–‰ê¸° ì´ˆê¸°í™”
    runner = AutoExperimentRunner(
        base_config_path=args.base_config,
        output_dir=args.output_dir
    )
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = runner.run_experiments(
        experiment_configs=args.config,
        dry_run=args.dry_run,
        continue_on_error=not args.stop_on_error,
        one_epoch=args.one_epoch,
        disable_eval=args.disable_eval
    )
    
    # ê²°ê³¼ ì €ì¥
    if not args.dry_run:
        result_file = Path(args.output_dir) / f"experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nì‹¤í—˜ ê²°ê³¼ ì €ì¥: {result_file}")
    
    # ì„±ê³µ ì—¬ë¶€ ë°˜í™˜
    success_count = sum(1 for r in results.values() if r.get('status') == 'success')
    return 0 if success_count == len(results) else 1


if __name__ == "__main__":
    sys.exit(main())
