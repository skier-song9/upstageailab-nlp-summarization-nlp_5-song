"""
ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ìœ í‹¸ë¦¬í‹°

QLoRA ë° unslothì˜ ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ë¥¼ ì¸¡ì •í•˜ê³  ëª¨ë‹ˆí„°ë§í•˜ëŠ” ìœ í‹¸ë¦¬í‹°ì…ë‹ˆë‹¤.
"""

import torch
import psutil
import logging
from typing import Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import json
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class MemorySnapshot:
    """ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: str
    cpu_memory_mb: float
    gpu_memory_mb: Optional[float]
    gpu_memory_reserved_mb: Optional[float]
    gpu_memory_allocated_mb: Optional[float]
    system_memory_percent: float
    model_type: str  # "standard", "qlora", "unsloth"
    

class MemoryMonitor:
    """
    ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
    
    QLoRAì™€ unslothì˜ ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ë¥¼ ì¸¡ì •í•˜ê³  ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, save_path: Optional[str] = None):
        """
        MemoryMonitor ì´ˆê¸°í™”
        
        Args:
            save_path: ë©”ëª¨ë¦¬ ë¡œê·¸ ì €ì¥ ê²½ë¡œ
        """
        self.snapshots: Dict[str, MemorySnapshot] = {}
        self.save_path = Path(save_path) if save_path else Path("memory_usage.json")
        self.baseline_memory = None
        
    def take_snapshot(self, label: str, model_type: str = "unknown") -> MemorySnapshot:
        """
        í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ìŠ¤ëƒ…ìƒ· ìƒì„±
        
        Args:
            label: ìŠ¤ëƒ…ìƒ· ë¼ë²¨
            model_type: ëª¨ë¸ íƒ€ì… ("standard", "qlora", "unsloth")
            
        Returns:
            MemorySnapshot: ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·
        """
        # CPU ë©”ëª¨ë¦¬
        process = psutil.Process()
        cpu_memory_mb = process.memory_info().rss / 1024 / 1024
        system_memory_percent = psutil.virtual_memory().percent
        
        # GPU ë©”ëª¨ë¦¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        gpu_memory_mb = None
        gpu_memory_reserved_mb = None  
        gpu_memory_allocated_mb = None
        
        if torch.cuda.is_available():
            gpu_memory_mb = torch.cuda.memory_usage() / 1024 / 1024
            gpu_memory_reserved_mb = torch.cuda.memory_reserved() / 1024 / 1024
            gpu_memory_allocated_mb = torch.cuda.memory_allocated() / 1024 / 1024
        elif torch.backends.mps.is_available():
            # MPS ë©”ëª¨ë¦¬ëŠ” ì§ì ‘ ì¸¡ì •ì´ ì–´ë ¤ì›€
            gpu_memory_mb = 0
            
        snapshot = MemorySnapshot(
            timestamp=datetime.now().isoformat(),
            cpu_memory_mb=cpu_memory_mb,
            gpu_memory_mb=gpu_memory_mb,
            gpu_memory_reserved_mb=gpu_memory_reserved_mb,
            gpu_memory_allocated_mb=gpu_memory_allocated_mb,
            system_memory_percent=system_memory_percent,
            model_type=model_type
        )
        
        self.snapshots[label] = snapshot
        
        logger.info(f"ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· '{label}' ìƒì„±:")
        logger.info(f"  - CPU: {cpu_memory_mb:.1f}MB")
        logger.info(f"  - ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {system_memory_percent:.1f}%")
        if gpu_memory_mb is not None:
            logger.info(f"  - GPU: {gpu_memory_mb:.1f}MB")
            
        return snapshot
    
    def set_baseline(self, label: str = "baseline"):
        """
        ë² ì´ìŠ¤ë¼ì¸ ë©”ëª¨ë¦¬ ì„¤ì •
        
        Args:
            label: ë² ì´ìŠ¤ë¼ì¸ ë¼ë²¨
        """
        self.baseline_memory = self.take_snapshot(label, "standard")
        logger.info(f"ë² ì´ìŠ¤ë¼ì¸ ë©”ëª¨ë¦¬ ì„¤ì •: {label}")
    
    def compare_with_baseline(self, current_label: str) -> Dict[str, float]:
        """
        í˜„ì¬ ë©”ëª¨ë¦¬ì™€ ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ
        
        Args:
            current_label: í˜„ì¬ ìŠ¤ëƒ…ìƒ· ë¼ë²¨
            
        Returns:
            Dict[str, float]: ë©”ëª¨ë¦¬ ë³€í™”ìœ¨ (ìŒìˆ˜ë©´ ê°ì†Œ)
        """
        if self.baseline_memory is None:
            logger.warning("ë² ì´ìŠ¤ë¼ì¸ ë©”ëª¨ë¦¬ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
            
        if current_label not in self.snapshots:
            logger.warning(f"ìŠ¤ëƒ…ìƒ· '{current_label}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
            
        current = self.snapshots[current_label]
        baseline = self.baseline_memory
        
        comparison = {}
        
        # CPU ë©”ëª¨ë¦¬ ë³€í™”ìœ¨
        if baseline.cpu_memory_mb > 0:
            cpu_change = ((current.cpu_memory_mb - baseline.cpu_memory_mb) / baseline.cpu_memory_mb) * 100
            comparison['cpu_memory_change_percent'] = cpu_change
        
        # GPU ë©”ëª¨ë¦¬ ë³€í™”ìœ¨
        if baseline.gpu_memory_mb is not None and current.gpu_memory_mb is not None:
            if baseline.gpu_memory_mb > 0:
                gpu_change = ((current.gpu_memory_mb - baseline.gpu_memory_mb) / baseline.gpu_memory_mb) * 100
                comparison['gpu_memory_change_percent'] = gpu_change
                
        return comparison
    
    def generate_report(self) -> Dict[str, Any]:
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¦¬í¬íŠ¸ ìƒì„±
        
        Returns:
            Dict[str, Any]: ë©”ëª¨ë¦¬ ë¦¬í¬íŠ¸
        """
        report = {
            'snapshots': {},
            'comparisons': {},
            'summary': {}
        }
        
        # ìŠ¤ëƒ…ìƒ· ì •ë³´
        for label, snapshot in self.snapshots.items():
            report['snapshots'][label] = {
                'timestamp': snapshot.timestamp,
                'cpu_memory_mb': snapshot.cpu_memory_mb,
                'gpu_memory_mb': snapshot.gpu_memory_mb,
                'system_memory_percent': snapshot.system_memory_percent,
                'model_type': snapshot.model_type
            }
        
        # ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµ
        if self.baseline_memory:
            for label, snapshot in self.snapshots.items():
                if label != 'baseline':
                    comparison = self.compare_with_baseline(label)
                    if comparison:
                        report['comparisons'][label] = comparison
        
        # ìš”ì•½ í†µê³„
        if self.baseline_memory and len(self.snapshots) > 1:
            cpu_changes = []
            gpu_changes = []
            
            for label, comparison in report['comparisons'].items():
                if 'cpu_memory_change_percent' in comparison:
                    cpu_changes.append(comparison['cpu_memory_change_percent'])
                if 'gpu_memory_change_percent' in comparison:
                    gpu_changes.append(comparison['gpu_memory_change_percent'])
            
            if cpu_changes:
                report['summary']['avg_cpu_memory_change'] = sum(cpu_changes) / len(cpu_changes)
                report['summary']['max_cpu_memory_reduction'] = min(cpu_changes)  # ìŒìˆ˜ê°€ ê°ì†Œ
                
            if gpu_changes:
                report['summary']['avg_gpu_memory_change'] = sum(gpu_changes) / len(gpu_changes)
                report['summary']['max_gpu_memory_reduction'] = min(gpu_changes)  # ìŒìˆ˜ê°€ ê°ì†Œ
        
        return report
    
    def save_report(self, filepath: Optional[str] = None):
        """
        ë©”ëª¨ë¦¬ ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            filepath: ì €ì¥ íŒŒì¼ ê²½ë¡œ
        """
        save_path = Path(filepath) if filepath else self.save_path
        report = self.generate_report()
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        logger.info(f"ë©”ëª¨ë¦¬ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {save_path}")
    
    def print_summary(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìš”ì•½ ì¶œë ¥"""
        report = self.generate_report()
        
        print("\n=== ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìš”ì•½ ===")
        print()
        
        # ìŠ¤ëƒ…ìƒ· ì •ë³´
        print("ğŸ“Š ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·:")
        for label, data in report['snapshots'].items():
            print(f"  {label}:")
            print(f"    - CPU: {data['cpu_memory_mb']:.1f}MB")
            print(f"    - ëª¨ë¸ íƒ€ì…: {data['model_type']}")
            if data['gpu_memory_mb'] is not None:
                print(f"    - GPU: {data['gpu_memory_mb']:.1f}MB")
        
        print()
        
        # ë¹„êµ ê²°ê³¼
        if report['comparisons']:
            print("ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ë³€í™”:")
            for label, comparison in report['comparisons'].items():
                print(f"  {label}:")
                if 'cpu_memory_change_percent' in comparison:
                    change = comparison['cpu_memory_change_percent']
                    symbol = "ğŸ“‰" if change < 0 else "ğŸ“ˆ"
                    print(f"    {symbol} CPU: {change:+.1f}%")
                if 'gpu_memory_change_percent' in comparison:
                    change = comparison['gpu_memory_change_percent']
                    symbol = "ğŸ“‰" if change < 0 else "ğŸ“ˆ"
                    print(f"    {symbol} GPU: {change:+.1f}%")
        
        # ìš”ì•½ í†µê³„
        if report['summary']:
            print()
            print("ğŸ¯ ìš”ì•½ í†µê³„:")
            summary = report['summary']
            if 'max_cpu_memory_reduction' in summary:
                reduction = abs(summary['max_cpu_memory_reduction'])
                print(f"  - ìµœëŒ€ CPU ë©”ëª¨ë¦¬ ì ˆì•½: {reduction:.1f}%")
            if 'max_gpu_memory_reduction' in summary:
                reduction = abs(summary['max_gpu_memory_reduction'])
                print(f"  - ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì ˆì•½: {reduction:.1f}%")


# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_memory_check() -> Dict[str, float]:
    """ë¹ ë¥¸ ë©”ëª¨ë¦¬ í™•ì¸"""
    monitor = MemoryMonitor()
    snapshot = monitor.take_snapshot("quick_check")
    
    return {
        'cpu_memory_mb': snapshot.cpu_memory_mb,
        'gpu_memory_mb': snapshot.gpu_memory_mb or 0,
        'system_memory_percent': snapshot.system_memory_percent
    }


def log_memory_usage(label: str, model_type: str = "unknown"):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹… ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            monitor = MemoryMonitor()
            monitor.take_snapshot(f"{label}_start", model_type)
            
            try:
                result = func(*args, **kwargs)
                monitor.take_snapshot(f"{label}_end", model_type)
                return result
            except Exception as e:
                monitor.take_snapshot(f"{label}_error", model_type)
                raise e
                
        return wrapper
    return decorator
