"""
ë””ë°”ì´ìŠ¤ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°

Mac M1/M2ì˜ MPSì™€ Linux/Windowsì˜ CUDAë¥¼ ìë™ ê°ì§€í•˜ì—¬ 
ìµœì ì˜ ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•˜ê³  í”Œë«í¼ë³„ ìµœì í™” ì„¤ì •ì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import os
import platform
import subprocess
import logging
import torch
import time
import json
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
from dataclasses import dataclass
import time
logger = logging.getLogger(__name__)


@dataclass
class DeviceInfo:
    """ë””ë°”ì´ìŠ¤ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    device_type: str  # 'cuda', 'mps', 'cpu'
    device_name: str
    device_index: int = 0
    memory_gb: Optional[float] = None
    compute_capability: Optional[str] = None
    
    def __str__(self) -> str:
        return f"{self.device_type}:{self.device_index} ({self.device_name})"



class ContainerAwareDeviceDetector:
    """
    ì»´í…Œì´ë„ˆ í™˜ê²½ì— íŠ¹í™”ëœ RTX 3090 ë””ë°”ì´ìŠ¤ ê°ì§€ê¸°
    
    ì‹œìŠ¤í…œ ëª…ë ¹ì–´ ë¶€ì¬ ìƒí™©ì—ì„œë„ ì •í™•í•œ í•˜ë“œì›¨ì–´ ê°ì§€ì™€ ê·¹í•œ ìµœì í™” ì„¤ì • ì ìš©ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    ì»´í…Œì´ë„ˆ í™˜ê²½ì—ì„œëŠ” nvidia-smi, lspci ë“±ì˜ ëª…ë ¹ì–´ê°€ ì—†ì„ ìˆ˜ ìˆì–´ ë‹¤ë‹¨ê³„ ê°ì§€ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, fallback_mode: bool = True):
        """
        Args:
            fallback_mode: ê°ì§€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš© ì—¬ë¶€
        """
        self.fallback_mode = fallback_mode
        self.detection_methods = [
            ('torch_cuda', self._torch_cuda_detection),
            ('nvidia_ml', self._nvidia_ml_detection), 
            ('env_variable', self._env_variable_detection),
            ('proc_gpu', self._proc_gpu_detection)
        ]
        self.detection_results = {}
        
    def detect_device_robust(self) -> Tuple[torch.device, DeviceInfo]:
        """
        ê²¬ê³ í•œ ë””ë°”ì´ìŠ¤ ê°ì§€ (ë‹¤ë‹¨ê³„ ë°©ë²•)
        
        Returns:
            (torch.device, DeviceInfo) íŠœí”Œ
        """
        logger.info("ğŸ” ì»´í…Œì´ë„ˆ íŠ¹í™” ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹œì‘")
        
        # ì»´í…Œì´ë„ˆ í™˜ê²½ì¸ì§€ í™•ì¸
        is_container = self._detect_container_environment()
        if is_container:
            logger.info("ğŸ“¦ ì»´í…Œì´ë„ˆ í™˜ê²½ ê°ì§€ë¨ - ì „ìš© ê°ì§€ ë¡œì§ ì‚¬ìš©")
        
        # ë‹¤ë‹¨ê³„ ê°ì§€ ì‹œë„
        best_detection = None
        
        for method_name, method_func in self.detection_methods:
            try:
                result = method_func()
                self.detection_results[method_name] = result
                
                if result and result.get('success', False):
                    logger.info(f"âœ… {method_name} ë°©ë²•ìœ¼ë¡œ ê°ì§€ ì„±ê³µ: {result.get('device_name', 'Unknown')}")
                    
                    # RTX 3090 ê°ì§€ ì„±ê³µ ì‹œ ìµœê³  ìš°ì„ ìˆœìœ„
                    if 'RTX 3090' in result.get('device_name', ''):
                        best_detection = result
                        break
                    elif not best_detection:
                        best_detection = result
                else:
                    logger.debug(f"âŒ {method_name} ë°©ë²• ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                logger.debug(f"âŒ {method_name} ë°©ë²• ì˜ˆì™¸: {e}")
                self.detection_results[method_name] = {'success': False, 'error': str(e)}
        
        # ê°ì§€ ê²°ê³¼ ì²˜ë¦¬
        if best_detection:
            return self._create_device_from_detection(best_detection)
        else:
            return self._handle_detection_failure()
    
    def _detect_container_environment(self) -> bool:
        """
        ì»´í…Œì´ë„ˆ í™˜ê²½ì¸ì§€ ê°ì§€
        
        Returns:
            ì»´í…Œì´ë„ˆ í™˜ê²½ ì—¬ë¶€
        """
        container_indicators = [
            # Docker ê°ì§€
            os.path.exists('/.dockerenv'),
            # cgroup í™•ì¸
            os.path.exists('/proc/1/cgroup') and 'docker' in open('/proc/1/cgroup').read() if os.path.exists('/proc/1/cgroup') else False,
            # Kubernetes ê°ì§€
            os.environ.get('KUBERNETES_SERVICE_HOST') is not None,
            # Singularity ê°ì§€  
            os.environ.get('SINGULARITY_CONTAINER') is not None,
            # ê¸°íƒ€ ì»´í…Œì´ë„ˆ í™˜ê²½ë³€ìˆ˜
            os.environ.get('CONTAINER') is not None
        ]
        
        return any(container_indicators)
    
    def _torch_cuda_detection(self) -> Dict[str, Any]:
        """
        PyTorch CUDA ê¸°ë°˜ ê°ì§€ (ê°€ì¥ ì•ˆì •ì )
        
        Returns:
            ê°ì§€ ê²°ê³¼
        """
        try:
            if not torch.cuda.is_available():
                return {'success': False, 'error': 'CUDA not available in PyTorch'}
            
            device_count = torch.cuda.device_count()
            if device_count == 0:
                return {'success': False, 'error': 'No CUDA devices found'}
            
            # ëŒ€í‘œ ë””ë°”ì´ìŠ¤ (GPU 0) ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            props = torch.cuda.get_device_properties(0)
            memory_gb = props.total_memory / (1024**3)
            
            return {
                'success': True,
                'method': 'torch_cuda',
                'device_name': props.name,
                'device_count': device_count,
                'memory_gb': memory_gb,
                'compute_capability': f"{props.major}.{props.minor}",
                'multiprocessor_count': props.multi_processor_count
            }
            
        except Exception as e:
            return {'success': False, 'error': f'PyTorch CUDA detection failed: {e}'}
    
    def _nvidia_ml_detection(self) -> Dict[str, Any]:
        """
        nvidia-ml-py ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ ê°ì§€
        
        Returns:
            ê°ì§€ ê²°ê³¼
        """
        try:
            import pynvml
            
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            
            if device_count == 0:
                return {'success': False, 'error': 'No NVIDIA devices found via nvidia-ml'}
            
            # ëŒ€í‘œ ë””ë°”ì´ìŠ¤ ì •ë³´
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            memory_gb = memory_info.total / (1024**3)
            
            # ì¶”ê°€ ì •ë³´
            try:
                major, minor = pynvml.nvmlDeviceGetCudaComputeCapability(handle)
                compute_capability = f"{major}.{minor}"
            except:
                compute_capability = "Unknown"
            
            return {
                'success': True,
                'method': 'nvidia_ml',
                'device_name': name,
                'device_count': device_count,
                'memory_gb': memory_gb,
                'compute_capability': compute_capability
            }
            
        except ImportError:
            return {'success': False, 'error': 'pynvml library not available'}
        except Exception as e:
            return {'success': False, 'error': f'nvidia-ml detection failed: {e}'}
    
    def _env_variable_detection(self) -> Dict[str, Any]:
        """
        í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ê°ì§€
        
        Returns:
            ê°ì§€ ê²°ê³¼
        """
        try:
            # CUDA ê´€ë ¨ í™˜ê²½ë³€ìˆ˜ í™•ì¸
            cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES')
            nvidia_visible = os.environ.get('NVIDIA_VISIBLE_DEVICES')
            
            if cuda_visible is None and nvidia_visible is None:
                return {'success': False, 'error': 'No CUDA environment variables found'}
            
            # ë””ë°”ì´ìŠ¤ ì •ë³´ ì¶”ë¡ 
            device_ids = cuda_visible or nvidia_visible
            
            # GPU ëª¨ë¸ëª… ì¶”ë¡  (ì»´í…Œì´ë„ˆì—ì„œ ê³µí†µì ì¸ íŒ¨í„´)
            gpu_model = os.environ.get('GPU_MODEL', 'Unknown GPU')
            gpu_memory = os.environ.get('GPU_MEMORY_GB', '24')  # RTX 3090 ê¸°ë³¸ê°’
            
            # RTX 3090 ì¶”ë¡  ë¡œì§ (ì»´í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì¼ë°˜ì ì¸ íŒ¨í„´)
            if gpu_model == 'Unknown GPU' and gpu_memory == '24':
                gpu_model = 'NVIDIA GeForce RTX 3090'  # 24GB VRAMì€ RTX 3090ì˜ íŠ¹ì§•
            
            return {
                'success': True,
                'method': 'env_variable',
                'device_name': gpu_model,
                'device_ids': device_ids,
                'memory_gb': float(gpu_memory),
                'inferred': True  # ì¶”ë¡ ëœ ì •ë³´ì„ì„ í‘œì‹œ
            }
            
        except Exception as e:
            return {'success': False, 'error': f'Environment variable detection failed: {e}'}
    
    def _proc_gpu_detection(self) -> Dict[str, Any]:
        """
        /proc íŒŒì¼ì‹œìŠ¤í…œ ê¸°ë°˜ ê°ì§€
        
        Returns:
            ê°ì§€ ê²°ê³¼
        """
        try:
            # /proc/driver/nvidia/gpus í™•ì¸
            nvidia_proc_path = Path('/proc/driver/nvidia/gpus')
            
            if not nvidia_proc_path.exists():
                return {'success': False, 'error': '/proc/driver/nvidia not found'}
            
            # GPU ë””ë ‰í† ë¦¬ ì—´ê±°
            gpu_dirs = [d for d in nvidia_proc_path.iterdir() if d.is_dir()]
            
            if not gpu_dirs:
                return {'success': False, 'error': 'No GPU directories in /proc/driver/nvidia/gpus'}
            
            # ëŒ€í‘œ GPU ì •ë³´ ì½ê¸°
            gpu_dir = gpu_dirs[0]
            info_file = gpu_dir / 'information'
            
            if not info_file.exists():
                return {'success': False, 'error': 'GPU information file not found'}
            
            # ì •ë³´ íŒŒì‹±
            gpu_info = {}
            with open(info_file, 'r') as f:
                for line in f:
                    if ':' in line:
                        key, value = line.strip().split(':', 1)
                        gpu_info[key.strip()] = value.strip()
            
            device_name = gpu_info.get('Model', 'Unknown GPU')
            
            # ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ë¡  (RTX 3090 = 24GB)
            memory_gb = 24.0 if 'RTX 3090' in device_name else 8.0
            
            return {
                'success': True,
                'method': 'proc_gpu',
                'device_name': device_name,
                'device_count': len(gpu_dirs),
                'memory_gb': memory_gb,
                'gpu_info': gpu_info
            }
            
        except Exception as e:
            return {'success': False, 'error': f'/proc GPU detection failed: {e}'}
    
    def _create_device_from_detection(self, detection: Dict[str, Any]) -> Tuple[torch.device, DeviceInfo]:
        """
        ê°ì§€ ê²°ê³¼ë¡œë¶€í„° ë””ë°”ì´ìŠ¤ ê°ì²´ ìƒì„±
        
        Args:
            detection: ê°ì§€ ê²°ê³¼
            
        Returns:
            (torch.device, DeviceInfo) íŠœí”Œ
        """
        device_name = detection.get('device_name', 'Unknown GPU')
        memory_gb = detection.get('memory_gb', 0.0)
        compute_capability = detection.get('compute_capability', 'Unknown')
        
        device_info = DeviceInfo(
            device_type='cuda',
            device_name=device_name,
            device_index=0,
            memory_gb=memory_gb,
            compute_capability=compute_capability
        )
        
        device = torch.device('cuda:0')
        
        # RTX 3090 ê°ì§€ ì„±ê³µ ì‹œ ê·¹í•œ ìµœì í™” ì„¤ì • ìë™ ì ìš©
        if 'RTX 3090' in device_name:
            logger.info(f"ğŸ”¥ RTX 3090 ê°ì§€ ì„±ê³µ! ê·¹í•œ ìµœì í™” ëª¨ë“œ í™œì„±í™”")
            logger.info(f"   ê°ì§€ ë°©ë²•: {detection.get('method', 'unknown')}")
            logger.info(f"   VRAM: {memory_gb:.1f}GB")
            logger.info(f"   ì»´í“¨íŠ¸ ëŠ¥ë ¥: {compute_capability}")
        
        return device, device_info
    
    def _handle_detection_failure(self) -> Tuple[torch.device, DeviceInfo]:
        """
        ëª¨ë“  ê°ì§€ ë°©ë²• ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬
        
        Returns:
            (torch.device, DeviceInfo) íŠœí”Œ
        """
        if not self.fallback_mode:
            logger.error("ëª¨ë“  ë””ë°”ì´ìŠ¤ ê°ì§€ ë°©ë²• ì‹¤íŒ¨ - fallback_mode ë¹„í™œì„±í™”")
            raise RuntimeError("ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨ ë° í´ë°± ëª¨ë“œ ë¹„í™œì„±í™”")
        
        logger.warning("âš ï¸  ëª¨ë“  GPU ê°ì§€ ë°©ë²• ì‹¤íŒ¨ - CPU ëª¨ë“œë¡œ ëŒ€ì²´")
        
        # ê¸°ë³¸ CPU ë””ë°”ì´ìŠ¤ ì •ë³´
        cpu_info = DeviceInfo(
            device_type='cpu',
            device_name=platform.processor() or 'CPU',
            device_index=0
        )
        
        device = torch.device('cpu')
        
        # ê°ì§€ ê²°ê³¼ë¥¼ ë¡œê·¸ì— ê¸°ë¡
        logger.info("ğŸ“„ ë””ë°”ì´ìŠ¤ ê°ì§€ ê²°ê³¼ ìš”ì•½:")
        for method, result in self.detection_results.items():
            status = "âœ…" if result.get('success', False) else "âŒ"
            error = result.get('error', 'No error') if not result.get('success', False) else ''
            logger.info(f"  {status} {method}: {error}")
        
        return device, cpu_info
    
    def get_detection_summary(self) -> Dict[str, Any]:
        """
        ê°ì§€ ê²°ê³¼ ìš”ì•½ ë°˜í™˜
        
        Returns:
            ê°ì§€ ê²°ê³¼ ìš”ì•½
        """
        return {
            'container_environment': self._detect_container_environment(),
            'detection_methods_tried': len(self.detection_methods),
            'successful_detections': sum(1 for r in self.detection_results.values() if r.get('success', False)),
            'detection_results': self.detection_results,
            'fallback_mode': self.fallback_mode
        }


# ì „ì—­ ContainerAwareDeviceDetector ì¸ìŠ¤í„´ìŠ¤
_container_device_detector = ContainerAwareDeviceDetector()


def get_robust_optimal_device() -> Tuple[torch.device, DeviceInfo]:
    """
    ê²¬ê³ í•œ ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ (ì „ì—­ í•¨ìˆ˜)
    
    ê¸°ì¡´ get_optimal_device()ì˜ ì•ˆì „í•œ ë²„ì „.
    ì»´í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì‹œìŠ¤í…œ ëª…ë ¹ì–´ ë¶€ì¬ ì‹œì—ë„ ì •í™•í•œ ë””ë°”ì´ìŠ¤ ê°ì§€ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
    
    Returns:
        (torch.device, DeviceInfo) íŠœí”Œ
        
    Example:
        # ê¸°ì¡´ ë°©ì‹
        device, device_info = get_optimal_device()
        
        # ì•ˆì „í•œ ë°©ì‹  
        device, device_info = get_robust_optimal_device()
    """
    return _container_device_detector.detect_device_robust()







@dataclass 
class OptimizationConfig:
    """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •"""
    fp16: bool = False
    fp16_opt_level: str = "O1"
    batch_size: int = 4
    gradient_accumulation_steps: int = 1
    num_workers: int = 0
    pin_memory: bool = False
    mixed_precision: str = "no"  # 'no', 'fp16', 'bf16'
    gradient_checkpointing: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'fp16': self.fp16,
            'fp16_opt_level': self.fp16_opt_level,
            'per_device_train_batch_size': self.batch_size,
            'gradient_accumulation_steps': self.gradient_accumulation_steps,
            'dataloader_num_workers': self.num_workers,
            'dataloader_pin_memory': self.pin_memory,
            'mixed_precision': self.mixed_precision,
            'gradient_checkpointing': self.gradient_checkpointing
        }


def get_system_info() -> Dict[str, Any]:
    """
    ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    
    Returns:
        ì‹œìŠ¤í…œ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    info = {
        'platform': platform.system(),
        'platform_release': platform.release(),
        'platform_version': platform.version(),
        'architecture': platform.machine(),
        'processor': platform.processor(),
        'python_version': platform.python_version(),
        'torch_version': torch.__version__,
    }
    
    # PyTorch ë¹Œë“œ ì •ë³´
    info['torch_cuda_available'] = torch.cuda.is_available()
    info['torch_cuda_version'] = torch.version.cuda if torch.cuda.is_available() else None
    info['torch_cudnn_version'] = torch.backends.cudnn.version() if torch.cuda.is_available() else None
    
    # MPS ì§€ì› (Mac M1/M2)
    info['torch_mps_available'] = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    
    # CPU ì •ë³´
    info['cpu_count'] = os.cpu_count()
    
    logger.info(f"ì‹œìŠ¤í…œ ì •ë³´: {info}")
    return info


def detect_cuda_devices() -> Dict[int, DeviceInfo]:
    """
    CUDA ë””ë°”ì´ìŠ¤ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    
    Returns:
        ë””ë°”ì´ìŠ¤ ì¸ë±ìŠ¤ë¥¼ í‚¤ë¡œ í•˜ëŠ” DeviceInfo ë”•ì…”ë„ˆë¦¬
    """
    devices = {}
    
    if not torch.cuda.is_available():
        return devices
    
    device_count = torch.cuda.device_count()
    logger.info(f"CUDA ë””ë°”ì´ìŠ¤ {device_count}ê°œ ê°ì§€ë¨")
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        
        # ë©”ëª¨ë¦¬ í¬ê¸° (GB)
        memory_gb = props.total_memory / (1024**3)
        
        # ì»´í“¨íŠ¸ ëŠ¥ë ¥
        compute_capability = f"{props.major}.{props.minor}"
        
        device_info = DeviceInfo(
            device_type='cuda',
            device_name=props.name,
            device_index=i,
            memory_gb=memory_gb,
            compute_capability=compute_capability
        )
        
        devices[i] = device_info
        logger.info(f"  GPU {i}: {device_info}")
    
    return devices


def detect_mps_device() -> Optional[DeviceInfo]:
    """
    MPS (Metal Performance Shaders) ë””ë°”ì´ìŠ¤ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    
    Returns:
        MPS ë””ë°”ì´ìŠ¤ ì •ë³´ ë˜ëŠ” None
    """
    if not hasattr(torch.backends, 'mps') or not torch.backends.mps.is_available():
        return None
    
    # macOS ë²„ì „ í™•ì¸
    try:
        macos_version = platform.mac_ver()[0]
        logger.info(f"macOS ë²„ì „: {macos_version}")
    except:
        macos_version = "Unknown"
    
    # Apple Silicon í™•ì¸
    processor = platform.processor()
    if 'arm' in processor.lower():
        chip_name = "Apple Silicon (M1/M2)"
    else:
        chip_name = "Unknown"
    
    device_info = DeviceInfo(
        device_type='mps',
        device_name=f"{chip_name} - macOS {macos_version}",
        device_index=0
    )
    
    logger.info(f"MPS ë””ë°”ì´ìŠ¤ ê°ì§€ë¨: {device_info}")
    return device_info


def get_optimal_device() -> Tuple[torch.device, DeviceInfo]:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì ì˜ ë””ë°”ì´ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    ìš°ì„ ìˆœìœ„: CUDA > MPS > CPU
    
    Returns:
        (torch.device, DeviceInfo) íŠœí”Œ
    """
    # CUDA í™•ì¸
    cuda_devices = detect_cuda_devices()
    if cuda_devices:
        # ë©”ëª¨ë¦¬ê°€ ê°€ì¥ í° GPU ì„ íƒ
        best_gpu = max(cuda_devices.values(), key=lambda d: d.memory_gb or 0)
        device = torch.device(f'cuda:{best_gpu.device_index}')
        logger.info(f"ìµœì  ë””ë°”ì´ìŠ¤ë¡œ CUDA ì„ íƒ: {best_gpu}")
        return device, best_gpu
    
    # MPS í™•ì¸
    mps_device = detect_mps_device()
    if mps_device:
        device = torch.device('mps')
        logger.info(f"ìµœì  ë””ë°”ì´ìŠ¤ë¡œ MPS ì„ íƒ: {mps_device}")
        return device, mps_device
    
    # CPU ì‚¬ìš©
    cpu_info = DeviceInfo(
        device_type='cpu',
        device_name=platform.processor() or 'CPU',
        device_index=0
    )
    device = torch.device('cpu')
    logger.info(f"GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPU ì‚¬ìš©: {cpu_info}")
    return device, cpu_info
def get_rtx3090_extreme_config(model_architecture: str = 'base', use_unsloth: bool = True) -> Dict[str, Any]:
    """
    RTX 3090 + CUDA 12.2 + Unsloth ê·¹í•œ ìµœì í™” ì„¤ì •
    
    Args:
        model_architecture: ëª¨ë¸ ì•„í‚¤í…ì²˜ ('mt5', 'bart', 't5', 'kobart', 'base')
        use_unsloth: Unsloth ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        RTX 3090 ê·¹í•œ ìµœì í™” ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    
    # RTX 3090 ê¸°ë³¸ ì‚¬ì–‘: 24GB VRAM, 48ì½”ì–´ CPU, 251GB RAM
    base_config = {
        'device_memory_gb': 24.0,
        'cpu_cores': 48,
        'system_ram_gb': 251.0,
        'cuda_version': '12.2',
        'use_unsloth': use_unsloth
    }
    
    # ì•„í‚¤í…ì²˜ë³„ ê·¹í•œ ìµœì í™” ë§¤íŠ¸ë¦­ìŠ¤
    optimization_matrix = {
        'mt5': {
            'per_device_train_batch_size': 12 if use_unsloth else 6,
            'per_device_eval_batch_size': 24 if use_unsloth else 12,
            'gradient_accumulation_steps': 4,
            'encoder_max_len': 1536 if use_unsloth else 1024,
            'decoder_max_len': 256,
            'dataloader_num_workers': 36,
            'lora_rank': 256 if use_unsloth else 128,
            'lora_alpha': 512 if use_unsloth else 256,
            'generation_num_beams': 16 if use_unsloth else 10
        },
        'bart': {
            'per_device_train_batch_size': 20 if use_unsloth else 8,
            'per_device_eval_batch_size': 32 if use_unsloth else 16,
            'gradient_accumulation_steps': 3,
            'encoder_max_len': 1280 if use_unsloth else 1024,
            'decoder_max_len': 256,
            'dataloader_num_workers': 36,
            'lora_rank': 192 if use_unsloth else 96,
            'lora_alpha': 384 if use_unsloth else 192,
            'generation_num_beams': 12 if use_unsloth else 8
        },
        'kobart': {
            'per_device_train_batch_size': 20 if use_unsloth else 8,
            'per_device_eval_batch_size': 32 if use_unsloth else 16,
            'gradient_accumulation_steps': 3,
            'encoder_max_len': 1280 if use_unsloth else 1024,
            'decoder_max_len': 256,
            'dataloader_num_workers': 36,
            'lora_rank': 192 if use_unsloth else 96,
            'lora_alpha': 384 if use_unsloth else 192,
            'generation_num_beams': 12 if use_unsloth else 8
        },
        't5': {
            'per_device_train_batch_size': 16 if use_unsloth else 8,
            'per_device_eval_batch_size': 28 if use_unsloth else 14,
            'gradient_accumulation_steps': 3,
            'encoder_max_len': 1024,
            'decoder_max_len': 200,
            'dataloader_num_workers': 36,
            'lora_rank': 128 if use_unsloth else 64,
            'lora_alpha': 256 if use_unsloth else 128,
            'generation_num_beams': 10 if use_unsloth else 6
        },
        'base': {
            'per_device_train_batch_size': 16 if use_unsloth else 8,
            'per_device_eval_batch_size': 24 if use_unsloth else 12,
            'gradient_accumulation_steps': 4,
            'encoder_max_len': 1024,
            'decoder_max_len': 200,
            'dataloader_num_workers': 32,
            'lora_rank': 128 if use_unsloth else 64,
            'lora_alpha': 256 if use_unsloth else 128,
            'generation_num_beams': 8
        }
    }
    
    # CUDA 12.2 + RTX 3090 íŠ¹í™” ì„¤ì •
    cuda_optimizations = {
        'fp16': False,  # RTX 3090ì—ì„œëŠ” bf16 ì„ í˜¸
        'bf16': True,   # RTX 3090 + CUDA 12.2 ìµœì 
        'tf32': True,   # Ampere ì•„í‚¤í…ì²˜ ìµœì í™”
        'torch_compile': True,  # PyTorch 2.7.1 ì»´íŒŒì¼ ìµœì í™”
        'gradient_checkpointing': False if use_unsloth else True,  # UnslothëŠ” ìì²´ ìµœì í™”
        'dataloader_pin_memory': True,  # 251GB RAM í™œìš©
        'dataloader_persistent_workers': True,  # ì›Œì»¤ ì¬ì‚¬ìš©
        'group_by_length': True,  # íš¨ìœ¨ì„± ì¦ëŒ€
        'remove_unused_columns': False,  # ì „ì²´ ì •ë³´ í™œìš©
        'optim': 'adamw_8bit' if use_unsloth else 'adamw_hf',  # Unsloth í˜¸í™˜ ì˜µí‹°ë§ˆì´ì €
        'adam_beta1': 0.9,
        'adam_beta2': 0.95 if use_unsloth else 0.999,  # Unsloth ì¶”ì²œê°’
        'max_grad_norm': 0.3 if use_unsloth else 1.0   # Unslothì™€ í•¨ê»˜ ë‚®ì€ ê°’
    }
    
    # ì•„í‚¤í…ì²˜ë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    arch_config = optimization_matrix.get(model_architecture.lower(), optimization_matrix['base'])
    
    # ìµœì¢… ì„¤ì • ì¡°í•©
    final_config = {
        **base_config,
        **arch_config,
        **cuda_optimizations
    }
    
    # ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°
    memory_efficiency = 40 if use_unsloth else 20  # Unsloth ë©”ëª¨ë¦¬ ì ˆì•½
    speed_improvement = 2.5 if use_unsloth else 1.5  # Unsloth ì†ë„ í–¥ìƒ
    
    final_config.update({
        'expected_memory_efficiency_percent': memory_efficiency,
        'expected_speed_improvement': speed_improvement,
        'effective_batch_size': arch_config['per_device_train_batch_size'] * arch_config['gradient_accumulation_steps'],
        'total_vram_utilization_target': 90.0,  # 24GB ì¤‘ 90% ëª©í‘œ
        'cpu_utilization_target': 75.0,  # 48ì½”ì–´ ì¤‘ 75% ëª©í‘œ
        'optimization_level': 'extreme',
        'architecture': model_architecture,
        'profile_name': f'RTX3090_Extreme_{model_architecture.upper()}_{"Unsloth" if use_unsloth else "Standard"}'
    })
    
    logger.info(f"RTX 3090 ê·¹í•œ ìµœì í™” í”„ë¡œíŒŒì¼ ìƒì„±: {final_config['profile_name']}")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {arch_config['per_device_train_batch_size']}, "
                f"ìœ íš¨ ë°°ì¹˜: {final_config['effective_batch_size']}, "
                f"ì›Œì»¤ ìˆ˜: {arch_config['dataloader_num_workers']}")
    
    return final_config


def apply_rtx3090_extreme_optimizations(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê¸°ì¡´ ì„¤ì •ì— RTX 3090 ê·¹í•œ ìµœì í™”ë¥¼ ì ìš©
    
    Args:
        config: ê¸°ì¡´ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        RTX 3090 ê·¹í•œ ìµœì í™”ê°€ ì ìš©ëœ ì„¤ì •
    """
    # GPU ì •ë³´ í™•ì¸
    if not torch.cuda.is_available():
        logger.warning("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ RTX 3090 ìµœì í™”ë¥¼ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return config
    
    device_props = torch.cuda.get_device_properties(0)
    if 'RTX 3090' not in device_props.name:
        logger.warning(f"RTX 3090ì´ ì•„ë‹ˆë¯€ë¡œ ê·¹í•œ ìµœì í™”ë¥¼ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {device_props.name}")
        return config
    
    # ì•„í‚¤í…ì²˜ ì¶”ë¡ 
    model_arch = 'base'
    if 'mt5' in config.get('model_name', '').lower():
        model_arch = 'mt5'
    elif 'bart' in config.get('model_name', '').lower():
        model_arch = 'kobart' if 'kobart' in config.get('model_name', '').lower() else 'bart'
    elif 't5' in config.get('model_name', '').lower():
        model_arch = 't5'
    
    # Unsloth ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_unsloth = config.get('qlora', {}).get('use_unsloth', True)
    
    # ê·¹í•œ ìµœì í™” ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    extreme_config = get_rtx3090_extreme_config(model_arch, use_unsloth)
    
    # ê¸°ì¡´ ì„¤ì •ì— ê·¹í•œ ìµœì í™” ì ìš©
    optimized_config = config.copy()
    
    # training ì„¹ì…˜ ì—…ë°ì´íŠ¸
    if 'training' not in optimized_config:
        optimized_config['training'] = {}
    
    training_updates = {
        'per_device_train_batch_size': extreme_config['per_device_train_batch_size'],
        'per_device_eval_batch_size': extreme_config['per_device_eval_batch_size'],
        'gradient_accumulation_steps': extreme_config['gradient_accumulation_steps'],
        'dataloader_num_workers': extreme_config['dataloader_num_workers'],
        'dataloader_pin_memory': extreme_config['dataloader_pin_memory'],
        'bf16': extreme_config['bf16'],
        'fp16': extreme_config['fp16'],
        'gradient_checkpointing': extreme_config['gradient_checkpointing'],
        'group_by_length': extreme_config['group_by_length'],
        'optim': extreme_config['optim'],
        'adam_beta1': extreme_config['adam_beta1'],
        'adam_beta2': extreme_config['adam_beta2'],
        'max_grad_norm': extreme_config['max_grad_norm']
    }
    
    optimized_config['training'].update(training_updates)
    
    # tokenizer ì„¹ì…˜ ì—…ë°ì´íŠ¸
    if 'tokenizer' not in optimized_config:
        optimized_config['tokenizer'] = {}
    
    tokenizer_updates = {
        'encoder_max_len': extreme_config['encoder_max_len'],
        'decoder_max_len': extreme_config['decoder_max_len']
    }
    
    optimized_config['tokenizer'].update(tokenizer_updates)
    
    # qlora ì„¹ì…˜ ì—…ë°ì´íŠ¸
    if 'qlora' not in optimized_config:
        optimized_config['qlora'] = {}
    
    qlora_updates = {
        'lora_rank': extreme_config['lora_rank'],
        'lora_alpha': extreme_config['lora_alpha'],
        'use_unsloth': use_unsloth
    }
    
    optimized_config['qlora'].update(qlora_updates)
    
    # generation ì„¤ì • ì—…ë°ì´íŠ¸
    if 'generation' in optimized_config or 'training' in optimized_config:
        generation_updates = {
            'generation_num_beams': extreme_config['generation_num_beams'],
            'generation_max_length': extreme_config['decoder_max_len']
        }
        
        if 'training' in optimized_config:
            optimized_config['training'].update(generation_updates)
        if 'generation' in optimized_config:
            optimized_config['generation'].update(generation_updates)
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    optimized_config['_rtx3090_extreme_optimization'] = {
        'applied': True,
        'profile_name': extreme_config['profile_name'],
        'expected_memory_efficiency': extreme_config['expected_memory_efficiency_percent'],
        'expected_speed_improvement': extreme_config['expected_speed_improvement'],
        'effective_batch_size': extreme_config['effective_batch_size'],
        'optimization_timestamp': time.time()
    }
    
    logger.info(f"RTX 3090 ê·¹í•œ ìµœì í™” ì ìš© ì™„ë£Œ: {extreme_config['profile_name']}")
    
    return optimized_config


def setup_device_config(device_info: DeviceInfo, model_size: str = 'base', use_qlora: bool = False) -> OptimizationConfig:
    """
    ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        device_info: ë””ë°”ì´ìŠ¤ ì •ë³´
        model_size: ëª¨ë¸ í¬ê¸° ('small', 'base', 'large')
        use_qlora: QLoRA ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        ìµœì í™” ì„¤ì •
    """
    config = OptimizationConfig()
    
    # ëª¨ë¸ í¬ê¸°ë³„ ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
    # QLoRA ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ë†’ì•„ ë°°ì¹˜ í¬ê¸°ë¥¼ í¬ê²Œ ì„¤ì •
    if use_qlora:
        # ì¡°ì¥ë‹˜ ê¶Œì¥ì‚¬í•­: QLoRAë¡œ batch_size 24~32
        base_batch_sizes = {
            'small': {'cuda': 32, 'mps': 16, 'cpu': 8},
            'base': {'cuda': 24, 'mps': 12, 'cpu': 6},
            'large': {'cuda': 16, 'mps': 8, 'cpu': 4}
        }
    else:
        # Full fine-tuning ë°°ì¹˜ í¬ê¸° (ë³´ìˆ˜ì )
        base_batch_sizes = {
            'small': {'cuda': 16, 'mps': 8, 'cpu': 4},
            'base': {'cuda': 8, 'mps': 4, 'cpu': 2},
            'large': {'cuda': 4, 'mps': 2, 'cpu': 1}
        }
    
    base_batch_size = base_batch_sizes.get(model_size, base_batch_sizes['base'])
    
    if device_info.device_type == 'cuda':
        # RTX 3090 ìë™ ê°ì§€ ë° ê·¹í•œ ìµœì í™” ì ìš©
        if 'RTX 3090' in device_info.device_name:
            logger.info(f"RTX 3090 ê°ì§€ë¨: {device_info.device_name} - ê·¹í•œ ìµœì í™” ëª¨ë“œ í™œì„±í™”")
            
            # RTX 3090 ê·¹í•œ ìµœì í™” ì„¤ì • ì‚¬ìš©
            extreme_config = get_rtx3090_extreme_config(model_size, use_qlora)
            
            # OptimizationConfigì— ê·¹í•œ ìµœì í™” ì ìš©
            config.fp16 = extreme_config['fp16']
            config.mixed_precision = 'bf16' if extreme_config['bf16'] else ('fp16' if extreme_config['fp16'] else 'no')
            config.batch_size = extreme_config['per_device_train_batch_size']
            config.gradient_accumulation_steps = extreme_config['gradient_accumulation_steps']
            config.num_workers = extreme_config['dataloader_num_workers']
            config.pin_memory = extreme_config['dataloader_pin_memory']
            config.gradient_checkpointing = extreme_config['gradient_checkpointing']
            
            logger.info(f"RTX 3090 ê·¹í•œ ìµœì í™” ì ìš©: ë°°ì¹˜={config.batch_size}, ì›Œì»¤={config.num_workers}, ìœ íš¨ë°°ì¹˜={extreme_config['effective_batch_size']}")
            
            return config
        # CUDA ìµœì í™” ì„¤ì •
        config.fp16 = True
        config.mixed_precision = 'fp16'
        config.batch_size = base_batch_size['cuda']
        config.num_workers = 4
        config.pin_memory = True
        
        # ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¥¸ ì¡°ì •
        if device_info.memory_gb:
            if device_info.memory_gb < 8:
                # ë©”ëª¨ë¦¬ê°€ ì ì„ ë•Œ
                config.batch_size = max(1, config.batch_size // 2)
                config.gradient_checkpointing = True
            elif device_info.memory_gb >= 16 and device_info.memory_gb < 24:
                # ì¤‘ê°„ ë©”ëª¨ë¦¬ (16-24GB)
                if use_qlora:
                    config.batch_size = min(32, config.batch_size)
                else:
                    config.batch_size = config.batch_size
            elif device_info.memory_gb >= 24:
                # ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ (24GB ì´ìƒ)
                if use_qlora:
                    config.batch_size = min(48, int(config.batch_size * 1.5))
                else:
                    config.batch_size = min(32, config.batch_size * 2)
        
        # ì»´í“¨íŠ¸ ëŠ¥ë ¥ì— ë”°ë¥¸ ì¡°ì •
        if device_info.compute_capability:
            major, minor = map(int, device_info.compute_capability.split('.'))
            if major >= 7:  # Volta ì´ìƒ
                config.fp16_opt_level = "O2"
            if major >= 8:  # Ampere ì´ìƒ
                config.mixed_precision = 'bf16'  # BF16 ì§€ì›
                
    elif device_info.device_type == 'mps':
        # MPS ìµœì í™” ì„¤ì •
        config.fp16 = False  # MPSëŠ” í˜„ì¬ fp16 ì§€ì›ì´ ì œí•œì 
        config.mixed_precision = 'no'
        config.batch_size = base_batch_size['mps']
        config.num_workers = 0  # MPSì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ ìˆìŒ
        config.pin_memory = False
        
        # MPS íŠ¹í™” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
        
    else:  # CPU
        # CPU ìµœì í™” ì„¤ì •
        config.fp16 = False
        config.mixed_precision = 'no'
        config.batch_size = base_batch_size['cpu']
        config.num_workers = min(2, os.cpu_count() or 1)
        config.pin_memory = False
        config.gradient_checkpointing = True  # ë©”ëª¨ë¦¬ ì ˆì•½
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ìœ¼ë¡œ ìœ íš¨ ë°°ì¹˜ í¬ê¸° ìœ ì§€
    # QLoRA ì‚¬ìš© ì‹œëŠ” í° ë°°ì¹˜ í¬ê¸°ë¥¼ í—ˆìš©í•˜ë¯€ë¡œ gradient accumulation ìµœì†Œí™”
    if use_qlora:
        target_batch_size = config.batch_size  # QLoRAëŠ” í° ë°°ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    else:
        target_batch_size = base_batch_sizes['base']['cuda']
        
    if config.batch_size < target_batch_size and not use_qlora:
        config.gradient_accumulation_steps = target_batch_size // config.batch_size
    
    logger.info(f"{device_info.device_type} ìµœì í™” ì„¤ì • ({'QLoRA' if use_qlora else 'Full FT'}): {config}")
    return config


def print_device_summary():
    """ë””ë°”ì´ìŠ¤ ìš”ì•½ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "="*60)
    print("ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ì„¤ì • ìš”ì•½ (RTX 3090 ê·¹í•œ ìµœì í™” í¬í•¨)")
    print("="*60)
    
    # ì‹œìŠ¤í…œ ì •ë³´
    sys_info = get_system_info()
    print(f"\ní”Œë«í¼: {sys_info['platform']} {sys_info['platform_release']}")
    print(f"ì•„í‚¤í…ì²˜: {sys_info['architecture']}")
    print(f"PyTorch ë²„ì „: {sys_info['torch_version']}")
    
    # CUDA ì •ë³´
    if sys_info['torch_cuda_available']:
        print(f"\nCUDA ì‚¬ìš© ê°€ëŠ¥: Yes")
        print(f"CUDA ë²„ì „: {sys_info['torch_cuda_version']}")
        cuda_devices = detect_cuda_devices()
        for device in cuda_devices.values():
            print(f"  - {device}")
    else:
        print(f"\nCUDA ì‚¬ìš© ê°€ëŠ¥: No")
    
    # MPS ì •ë³´
    if sys_info['torch_mps_available']:
        print(f"\nMPS ì‚¬ìš© ê°€ëŠ¥: Yes")
        mps_device = detect_mps_device()
        if mps_device:
            print(f"  - {mps_device}")
    else:
        print(f"\nMPS ì‚¬ìš© ê°€ëŠ¥: No")
    
    # ìµœì  ë””ë°”ì´ìŠ¤
    device, device_info = get_optimal_device()
    print(f"\nì„ íƒëœ ë””ë°”ì´ìŠ¤: {device}")
    
    # ìµœì í™” ì„¤ì •
    print(f"\nğŸš€ ìµœì í™” ì„¤ì •:")
    
    # RTX 3090 ê·¹í•œ ìµœì í™” í™•ì¸
    if device_info.device_type == 'cuda' and 'RTX 3090' in device_info.device_name:
        print(f"\nğŸ”¥ RTX 3090 ê·¹í•œ ìµœì í™” ëª¨ë“œ í™œì„±í™”!")
        print(f"   ë””ë°”ì´ìŠ¤: {device_info.device_name}")
        print(f"   VRAM: {device_info.memory_gb:.1f}GB")
        
        # ì•„í‚¤í…ì²˜ë³„ ê·¹í•œ ìµœì í™” ì„¤ì • í‘œì‹œ
        for arch in ['mt5', 'bart', 't5', 'kobart']:
            extreme_config = get_rtx3090_extreme_config(arch, True)  # Unsloth ì‚¬ìš© ê°€ì •
            print(f"\n   {arch.upper()} ê·¹í•œ ìµœì í™”:")
            print(f"     - ë°°ì¹˜ í¬ê¸°: {extreme_config['per_device_train_batch_size']} (ìœ íš¨: {extreme_config['effective_batch_size']})")
            print(f"     - ì‹œí€€ìŠ¤ ê¸¸ì´: {extreme_config['encoder_max_len']}")
            print(f"     - LoRA Rank: {extreme_config['lora_rank']}")
            print(f"     - ì›Œì»¤ ìˆ˜: {extreme_config['dataloader_num_workers']}")
            print(f"     - ì˜ˆìƒ ì†ë„ í–¥ìƒ: {extreme_config['expected_speed_improvement']:.1f}x")
            print(f"     - ë©”ëª¨ë¦¬ íš¨ìœ¨: {extreme_config['expected_memory_efficiency_percent']}% ì ˆì•½")
    
    print(f"\nğŸ“Š ê¸°ë³¸ ëª¨ë¸ í¬ê¸°ë³„ ìµœì í™” ì„¤ì •:")
    for model_size in ['small', 'base', 'large']:
        config = setup_device_config(device_info, model_size)
        print(f"\n{model_size.upper()} ëª¨ë¸ ìµœì í™” ì„¤ì •:")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {config.batch_size}")
        print(f"  - Mixed Precision: {config.mixed_precision}")
        print(f"  - Gradient Accumulation: {config.gradient_accumulation_steps}")
        print(f"  - DataLoader Workers: {config.num_workers}")
    
    print("\n" + "="*60)


def test_rtx3090_extreme_config():
    """
    RTX 3090 ê·¹í•œ ìµœì í™” ì„¤ì • í…ŒìŠ¤íŠ¸
    """
    print("\n" + "="*50)
    print("ğŸ§ª RTX 3090 ê·¹í•œ ìµœì í™” í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    # ì•„í‚¤í…ì²˜ë³„ í…ŒìŠ¤íŠ¸
    architectures = ['mt5', 'bart', 't5', 'kobart', 'base']
    
    for arch in architectures:
        print(f"\nğŸ” {arch.upper()} ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸:")
        
        # Unsloth ì‚¬ìš©/ë¯¸ì‚¬ìš© ë¹„êµ
        for use_unsloth in [True, False]:
            config = get_rtx3090_extreme_config(arch, use_unsloth)
            mode = "Unsloth" if use_unsloth else "Standard"
            
            print(f"  {mode} ëª¨ë“œ:")
            print(f"    ë°°ì¹˜: {config['per_device_train_batch_size']} | ìœ íš¨: {config['effective_batch_size']}")
            print(f"    ì‹œí€€ìŠ¤: {config['encoder_max_len']} | LoRA: {config['lora_rank']}")
            print(f"    ì›Œì»¤: {config['dataloader_num_workers']} | ì†ë„: {config['expected_speed_improvement']:.1f}x")
    
    # GPU ê°ì§€ í…ŒìŠ¤íŠ¸
    if torch.cuda.is_available():
        device_props = torch.cuda.get_device_properties(0)
        print(f"\nğŸ“Š í˜„ì¬ GPU: {device_props.name}")
        print(f"VRAM: {device_props.total_memory / (1024**3):.1f}GB")
        
        if 'RTX 3090' in device_props.name:
            print("âœ… RTX 3090 ê°ì§€ë¨ - ê·¹í•œ ìµœì í™” ì‚¬ìš© ê°€ëŠ¥!")
        else:
            print("âš ï¸  RTX 3090ì´ ì•„ë‹ˆë¯€ë¡œ ê·¹í•œ ìµœì í™” ë¹„í™œì„±í™”")
    else:
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    print("\n" + "="*50)


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ê¸°ë³¸ ë””ë°”ì´ìŠ¤ ìš”ì•½ ì¶œë ¥
    print_device_summary()
    
    # RTX 3090 ê·¹í•œ ìµœì í™” í…ŒìŠ¤íŠ¸
    test_rtx3090_extreme_config()
