#!/usr/bin/env python3
"""
ëŒ€íšŒ ì œì¶œìš© CSV ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°

ëŒ€íšŒ ì±„ì ìš© CSV íŒŒì¼ì„ baseline.pyì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ìƒì„±í•˜ë©´ì„œ,
ë‹¤ì¤‘ ì‹¤í—˜ ì§€ì› ë° ì‹¤í—˜ ì¶”ì  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import logging
import shutil
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime
import pandas as pd

logger = logging.getLogger(__name__)


class CompetitionCSVManager:
    """ëŒ€íšŒ ì œì¶œìš© CSV ê´€ë¦¬ì"""
    
    def __init__(self, prediction_base: str = "./prediction"):
        """
        Args:
            prediction_base: ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬
        """
        self.prediction_base = Path(prediction_base)
        self._ensure_directories()
        
    def _ensure_directories(self):
        """í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±"""
        self.prediction_base.mkdir(exist_ok=True)
        (self.prediction_base / "history").mkdir(exist_ok=True)
        
    def save_experiment_submission(self, 
                                 experiment_name: str, 
                                 result_df: pd.DataFrame,
                                 config: Dict = None,
                                 metrics: Dict = None,
                                 timestamp: str = None) -> Dict[str, str]:
        """
        ì‹¤í—˜ ê²°ê³¼ë¥¼ ëŒ€íšŒ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        
        Args:
            experiment_name: ì‹¤í—˜ëª…
            result_df: ê²°ê³¼ DataFrame (fname, summary ì»¬ëŸ¼ í•„ìˆ˜)
            config: ì‹¤í—˜ ì„¤ì • (ì„ íƒ)
            metrics: ì„±ëŠ¥ ì§€í‘œ (ì„ íƒ)
            timestamp: íƒ€ì„ìŠ¤íƒ¬í”„ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            
        Returns:
            ìƒì„±ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        # ì…ë ¥ ê²€ì¦
        if 'fname' not in result_df.columns or 'summary' not in result_df.columns:
            raise ValueError("result_df must have 'fname' and 'summary' columns")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì œì¶œìš© DataFrame ì¤€ë¹„ (fname, summaryë§Œ)
        submission_df = result_df[['fname', 'summary']].copy()
        
        # ì‹¤í—˜ë³„ í´ë” ìƒì„±
        experiment_folder = f"{experiment_name}_{timestamp}"
        experiment_path = self.prediction_base / experiment_folder
        experiment_path.mkdir(exist_ok=True)
        
        # 1. ì‹¤í—˜ë³„ output.csv ì €ì¥ (ëŒ€íšŒ í‘œì¤€ í˜•ì‹)
        output_path = experiment_path / "output.csv"
        submission_df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ“¤ ì‹¤í—˜ë³„ ì œì¶œ íŒŒì¼ ì €ì¥: {output_path}")
        
        # 2. ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = experiment_path / "experiment_metadata.json"
        self._save_experiment_metadata(
            metadata_path, experiment_name, config, metrics, timestamp
        )
        
        # 3. latest_output.csv ì—…ë°ì´íŠ¸ (ë®ì–´ì“°ê¸°)
        latest_path = self.prediction_base / "latest_output.csv"
        submission_df.to_csv(latest_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ“¤ ìµœì‹  ì œì¶œ íŒŒì¼ ì—…ë°ì´íŠ¸: {latest_path}")
        
        # 4. íˆìŠ¤í† ë¦¬ ë°±ì—…
        history_path = self._save_to_history(submission_df, experiment_name, timestamp)
        
        # 5. ì‹¤í—˜ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self._update_experiment_index(
            experiment_name, experiment_folder, timestamp, metrics
        )
        
        # 6. ìƒì„± ìš”ì•½ ì¶œë ¥
        result_paths = {
            'experiment_path': str(output_path),
            'latest_path': str(latest_path),
            'history_path': str(history_path),
            'metadata_path': str(metadata_path),
            'experiment_folder': experiment_folder
        }
        
        self._print_generation_summary(result_paths, len(submission_df))
        
        return result_paths
    
    def _save_experiment_metadata(self, 
                                metadata_path: Path,
                                experiment_name: str,
                                config: Dict,
                                metrics: Dict,
                                timestamp: str):
        """ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata = {
            "experiment_name": experiment_name,
            "timestamp": timestamp,
            "created_at": datetime.now().isoformat(),
            "submission_info": {
                "format": "fname,summary",
                "encoding": "utf-8"
            }
        }
        
        # ì„¤ì • ì •ë³´ ì¶”ê°€
        if config:
            metadata["model_name"] = config.get('general', {}).get('model_name', 'unknown')
            metadata["config_summary"] = {
                "learning_rate": config.get('training', {}).get('learning_rate'),
                "batch_size": config.get('training', {}).get('per_device_train_batch_size'),
                "num_epochs": config.get('training', {}).get('num_train_epochs')
            }
        
        # ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ê°€
        if metrics:
            metadata["metrics"] = {
                "eval_rouge1_f1": metrics.get('eval_rouge1', 0),
                "eval_rouge2_f1": metrics.get('eval_rouge2', 0),
                "eval_rougeL_f1": metrics.get('eval_rougeL', 0),
                "eval_rouge_combined_f1": (
                    metrics.get('eval_rouge1', 0) + 
                    metrics.get('eval_rouge2', 0) + 
                    metrics.get('eval_rougeL', 0)
                )
            }
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“„ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    def _save_to_history(self, submission_df: pd.DataFrame, 
                        experiment_name: str, timestamp: str) -> str:
        """íˆìŠ¤í† ë¦¬ ë°±ì—…"""
        history_filename = f"output_{experiment_name}_{timestamp}.csv"
        history_path = self.prediction_base / "history" / history_filename
        submission_df.to_csv(history_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ’¾ íˆìŠ¤í† ë¦¬ ë°±ì—…: {history_path}")
        return str(history_path)
    
    def _update_experiment_index(self, 
                               experiment_name: str,
                               experiment_folder: str,
                               timestamp: str,
                               metrics: Dict = None):
        """ì‹¤í—˜ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        index_path = self.prediction_base / "experiment_index.csv"
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
        if index_path.exists():
            index_df = pd.read_csv(index_path)
        else:
            index_df = pd.DataFrame(columns=[
                'experiment_name', 'folder_name', 'timestamp', 
                'submission_file', 'latest_file', 'created_at',
                'rouge_combined', 'rouge1', 'rouge2', 'rougeL'
            ])
        
        # ìƒˆ ì‹¤í—˜ ì •ë³´ ì¶”ê°€
        new_row = {
            'experiment_name': experiment_name,
            'folder_name': experiment_folder,
            'timestamp': timestamp,
            'submission_file': f"./prediction/{experiment_folder}/output.csv",
            'latest_file': "./prediction/latest_output.csv",
            'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ê°€
        if metrics:
            new_row['rouge1'] = metrics.get('eval_rouge1', 0)
            new_row['rouge2'] = metrics.get('eval_rouge2', 0)
            new_row['rougeL'] = metrics.get('eval_rougeL', 0)
            new_row['rouge_combined'] = (
                new_row['rouge1'] + new_row['rouge2'] + new_row['rougeL']
            )
        else:
            new_row['rouge1'] = 0
            new_row['rouge2'] = 0
            new_row['rougeL'] = 0
            new_row['rouge_combined'] = 0
        
        # DataFrameì— ì¶”ê°€ (concat ì‚¬ìš©)
        index_df = pd.concat([index_df, pd.DataFrame([new_row])], ignore_index=True)
        
        # ì‹œê°„ìˆœ ì •ë ¬ (ìµœì‹ ì´ ìœ„ë¡œ)
        index_df = index_df.sort_values('created_at', ascending=False)
        
        # ì €ì¥
        index_df.to_csv(index_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ“‹ ì‹¤í—˜ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸: {index_path}")
    
    def _print_generation_summary(self, result_paths: Dict[str, str], num_samples: int):
        """ìƒì„± ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\nâœ… ì±„ì ìš© CSV íŒŒì¼ ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜: {num_samples}")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print(f"  ğŸ“¤ ì‹¤í—˜ë³„ ì œì¶œ: {result_paths['experiment_path']}")
        print(f"  ğŸ“¤ ìµœì‹  ì œì¶œ: {result_paths['latest_path']}")
        print(f"  ğŸ“‹ ì‹¤í—˜ ì¸ë±ìŠ¤: {self.prediction_base}/experiment_index.csv")
        print(f"  ğŸ’¾ íˆìŠ¤í† ë¦¬ ë°±ì—…: {result_paths['history_path']}")
        print(f"  ğŸ“„ ë©”íƒ€ë°ì´í„°: {result_paths['metadata_path']}")
    
    def get_latest_experiment(self) -> Optional[Dict]:
        """ê°€ì¥ ìµœê·¼ ì‹¤í—˜ ì •ë³´ ì¡°íšŒ"""
        index_path = self.prediction_base / "experiment_index.csv"
        if not index_path.exists():
            return None
        
        index_df = pd.read_csv(index_path)
        if len(index_df) == 0:
            return None
        
        # ì²« ë²ˆì§¸ í–‰ì´ ê°€ì¥ ìµœê·¼ (ì´ë¯¸ ì •ë ¬ë¨)
        latest = index_df.iloc[0].to_dict()
        return latest
    
    def list_all_experiments(self) -> pd.DataFrame:
        """ëª¨ë“  ì‹¤í—˜ ëª©ë¡ ì¡°íšŒ"""
        index_path = self.prediction_base / "experiment_index.csv"
        if not index_path.exists():
            return pd.DataFrame()
        
        return pd.read_csv(index_path)
    
    def get_best_experiment_by_rouge(self) -> Optional[Dict]:
        """ROUGE ì ìˆ˜ ê¸°ì¤€ ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì¡°íšŒ"""
        index_path = self.prediction_base / "experiment_index.csv"
        if not index_path.exists():
            return None
        
        index_df = pd.read_csv(index_path)
        if len(index_df) == 0:
            return None
        
        # ROUGE combined ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        best_idx = index_df['rouge_combined'].idxmax()
        best = index_df.loc[best_idx].to_dict()
        return best
    
    def cleanup_old_experiments(self, keep_latest: int = 10):
        """ì˜¤ë˜ëœ ì‹¤í—˜ ê²°ê³¼ ì •ë¦¬"""
        index_path = self.prediction_base / "experiment_index.csv"
        if not index_path.exists():
            return
        
        index_df = pd.read_csv(index_path)
        if len(index_df) <= keep_latest:
            return
        
        # ë³´ê´€í•  ì‹¤í—˜ê³¼ ì‚­ì œí•  ì‹¤í—˜ ë¶„ë¦¬
        keep_df = index_df.head(keep_latest)
        remove_df = index_df.iloc[keep_latest:]
        
        # ì‚­ì œí•  ì‹¤í—˜ í´ë” ì œê±°
        for _, row in remove_df.iterrows():
            folder_path = self.prediction_base / row['folder_name']
            if folder_path.exists():
                shutil.rmtree(folder_path)
                logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ì‹¤í—˜ ì‚­ì œ: {folder_path}")
        
        # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        keep_df.to_csv(index_path, index=False, encoding='utf-8')
        logger.info(f"âœ¨ ìµœê·¼ {keep_latest}ê°œ ì‹¤í—˜ë§Œ ìœ ì§€")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import sys
    from pathlib import Path
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s'
    )
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_df = pd.DataFrame({
        'fname': [f'TEST_{i:03d}' for i in range(1, 6)],
        'summary': [f'í…ŒìŠ¤íŠ¸ ìš”ì•½ {i}' for i in range(1, 6)]
    })
    
    # CSV ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
    manager = CompetitionCSVManager()
    
    print("\n=== CSV ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ ===")
    
    # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
    result_paths = manager.save_experiment_submission(
        experiment_name="test_experiment",
        result_df=test_df,
        config={
            'general': {'model_name': 'test-model'},
            'training': {
                'learning_rate': 1e-5,
                'per_device_train_batch_size': 16,
                'num_train_epochs': 3
            }
        },
        metrics={
            'eval_rouge1': 0.254,
            'eval_rouge2': 0.095,
            'eval_rougeL': 0.230
        }
    )
    
    print("\nìƒì„±ëœ íŒŒì¼ ê²½ë¡œ:")
    for key, path in result_paths.items():
        print(f"  {key}: {path}")
    
    # ìµœê·¼ ì‹¤í—˜ ì¡°íšŒ
    print("\n=== ìµœê·¼ ì‹¤í—˜ ì¡°íšŒ ===")
    latest = manager.get_latest_experiment()
    if latest:
        print(f"ìµœê·¼ ì‹¤í—˜: {latest['experiment_name']}")
        print(f"ì œì¶œ íŒŒì¼: {latest['submission_file']}")
        print(f"ROUGE ì ìˆ˜: {latest['rouge_combined']}")
    
    # ì „ì²´ ì‹¤í—˜ ëª©ë¡
    print("\n=== ì „ì²´ ì‹¤í—˜ ëª©ë¡ ===")
    all_experiments = manager.list_all_experiments()
    if not all_experiments.empty:
        print(all_experiments[['experiment_name', 'rouge_combined', 'created_at']].head())
