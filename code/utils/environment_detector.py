"""
í™˜ê²½ ìë™ ê°ì§€ ë° Unsloth í™œì„±í™” ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ í˜„ì¬ ì‹¤í–‰ í™˜ê²½ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³ ,
Ubuntu + CUDA í™˜ê²½ì—ì„œëŠ” ìë™ìœ¼ë¡œ Unslothë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.
"""

import platform
import subprocess
import os
import logging
from typing import Dict, Any, Tuple
import torch

logger = logging.getLogger(__name__)

class EnvironmentDetector:
    """í™˜ê²½ ìë™ ê°ì§€ ë° ìµœì í™” ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.os_type = platform.system()
        self.os_release = platform.release()
        self.machine = platform.machine()
        self.python_version = platform.python_version()
        
    def detect_environment(self) -> Dict[str, Any]:
        """í˜„ì¬ í™˜ê²½ì„ ìë™ ê°ì§€í•©ë‹ˆë‹¤."""
        env_info = {
            'os': self.os_type,
            'os_release': self.os_release,
            'machine': self.machine,
            'python_version': self.python_version,
            'is_cuda_available': torch.cuda.is_available(),
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'is_ubuntu': self._is_ubuntu(),
            'is_macos': self._is_macos(),
            'is_windows': self._is_windows(),
            'cuda_version': self._get_cuda_version(),
            'gpu_info': self._get_gpu_info(),
            'available_memory_gb': self._get_available_memory_gb(),
            'cpu_count': os.cpu_count(),
        }
        
        # Unsloth ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨
        env_info['unsloth_recommended'] = self._should_use_unsloth(env_info)
        env_info['unsloth_available'] = self._check_unsloth_availability()
        
        return env_info
    
    def _is_ubuntu(self) -> bool:
        """Ubuntu í™˜ê²½ì¸ì§€ í™•ì¸"""
        if self.os_type != 'Linux':
            return False
        
        try:
            with open('/etc/os-release', 'r') as f:
                content = f.read().lower()
                return 'ubuntu' in content
        except FileNotFoundError:
            return False
    
    def _is_macos(self) -> bool:
        """macOS í™˜ê²½ì¸ì§€ í™•ì¸"""
        return self.os_type == 'Darwin'
    
    def _is_windows(self) -> bool:
        """Windows í™˜ê²½ì¸ì§€ í™•ì¸"""
        return self.os_type == 'Windows'
    
    def _get_cuda_version(self) -> str:
        """CUDA ë²„ì „ í™•ì¸"""
        if not torch.cuda.is_available():
            return "N/A"
        
        try:
            result = subprocess.run(['nvidia-smi'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                for line in lines:
                    if 'CUDA Version:' in line:
                        return line.split('CUDA Version:')[1].strip().split()[0]
            
            # PyTorchì—ì„œ CUDA ë²„ì „ í™•ì¸
            return torch.version.cuda or "Unknown"
        except Exception as e:
            logger.warning(f"CUDA ë²„ì „ í™•ì¸ ì‹¤íŒ¨: {e}")
            return "Unknown"
    
    def _get_gpu_info(self) -> Dict[str, Any]:
        """GPU ì •ë³´ ìˆ˜ì§‘"""
        if not torch.cuda.is_available():
            return {"count": 0, "devices": []}
        
        gpu_info = {
            "count": torch.cuda.device_count(),
            "devices": []
        }
        
        for i in range(torch.cuda.device_count()):
            device_props = torch.cuda.get_device_properties(i)
            gpu_info["devices"].append({
                "id": i,
                "name": device_props.name,
                "memory_gb": device_props.total_memory / (1024**3),
                "compute_capability": f"{device_props.major}.{device_props.minor}"
            })
        
        return gpu_info
    
    def _get_available_memory_gb(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬(GB) í™•ì¸"""
        try:
            if self._is_ubuntu() or self.os_type == 'Linux':
                with open('/proc/meminfo', 'r') as f:
                    for line in f:
                        if 'MemTotal:' in line:
                            # kB to GB ë³€í™˜
                            memory_kb = int(line.split()[1])
                            return memory_kb / (1024**2)
            elif self._is_macos():
                result = subprocess.run(['sysctl', 'hw.memsize'], 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    memory_bytes = int(result.stdout.split()[1])
                    return memory_bytes / (1024**3)
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        return 0.0
    
    def _should_use_unsloth(self, env_info: Dict[str, Any]) -> bool:
        """Unsloth ì‚¬ìš© ê¶Œì¥ ì—¬ë¶€ íŒë‹¨"""
        # ê¸°ë³¸ ì¡°ê±´: Ubuntu + CUDA
        if not (env_info['is_ubuntu'] and env_info['is_cuda_available']):
            return False
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸ (ìµœì†Œ 6GB ê¶Œì¥)
        if env_info['gpu_info']['count'] > 0:
            max_gpu_memory = max(
                device['memory_gb'] for device in env_info['gpu_info']['devices']
            )
            if max_gpu_memory < 6.0:
                logger.warning(f"GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {max_gpu_memory:.1f}GB < 6GB")
                return False
        
        # CUDA ë²„ì „ í™•ì¸ (11.8+ ê¶Œì¥)
        cuda_version = env_info['cuda_version']
        if cuda_version not in ['Unknown', 'N/A']:
            try:
                major, minor = map(int, cuda_version.split('.')[:2])
                if major < 11 or (major == 11 and minor < 8):
                    logger.warning(f"CUDA ë²„ì „ì´ ë‚®ìŠµë‹ˆë‹¤: {cuda_version} < 11.8")
                    return False
            except ValueError:
                logger.warning(f"CUDA ë²„ì „ íŒŒì‹± ì‹¤íŒ¨: {cuda_version}")
        
        return True
    
    def _check_unsloth_availability(self) -> bool:
        """Unsloth íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸"""
        try:
            import unsloth
            return True
        except ImportError:
            return False
    
    def get_recommended_config(self) -> Dict[str, Any]:
        """í™˜ê²½ì— ë§ëŠ” ê¶Œì¥ ì„¤ì • ë°˜í™˜"""
        env_info = self.detect_environment()
        
        config = {
            'use_unsloth': False,
            'use_qlora': True,
            'fp16': True,
            'bf16': False,
            'gradient_checkpointing': True,
            'dataloader_num_workers': min(4, env_info['cpu_count'] // 2),
            'recommended_batch_size': 4,
        }
        
        # Ubuntu + CUDA í™˜ê²½ì—ì„œ Unsloth ìë™ í™œì„±í™”
        if env_info['unsloth_recommended']:
            config['use_unsloth'] = True
            config['gradient_checkpointing'] = False  # UnslothëŠ” ìì²´ ìµœì í™”
            
            # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
            if env_info['gpu_info']['count'] > 0:
                max_gpu_memory = max(
                    device['memory_gb'] for device in env_info['gpu_info']['devices']
                )
                
                if max_gpu_memory >= 24:  # RTX 3090/4090ê¸‰
                    config['recommended_batch_size'] = 12
                    config['dataloader_num_workers'] = min(8, env_info['cpu_count'] // 2)
                    config['bf16'] = True
                    config['fp16'] = False
                elif max_gpu_memory >= 16:  # RTX 4080ê¸‰
                    config['recommended_batch_size'] = 8
                    config['dataloader_num_workers'] = 6
                elif max_gpu_memory >= 12:  # RTX 4070ê¸‰
                    config['recommended_batch_size'] = 6
                elif max_gpu_memory >= 8:   # RTX 4060ê¸‰
                    config['recommended_batch_size'] = 4
        
        # macOSì—ì„œëŠ” íŠ¹ë³„ ì„¤ì •
        elif env_info['is_macos']:
            config['use_unsloth'] = False
            config['fp16'] = False  # macOS MPSëŠ” fp16 ì´ìŠˆ
            config['bf16'] = False
            config['dataloader_num_workers'] = 0  # macOS ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ
            config['recommended_batch_size'] = 2
        
        # Windowsì—ì„œëŠ” ë³´ìˆ˜ì  ì„¤ì •
        elif env_info['is_windows']:
            config['dataloader_num_workers'] = 0  # Windows ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ
            config['recommended_batch_size'] = 2
        
        return config
    
    def print_environment_summary(self):
        """í™˜ê²½ ì •ë³´ ìš”ì•½ ì¶œë ¥"""
        env_info = self.detect_environment()
        config = self.get_recommended_config()
        
        print("ğŸ” í™˜ê²½ ìë™ ê°ì§€ ê²°ê³¼")
        print("=" * 50)
        print(f"OS: {env_info['os']} ({env_info['os_release']})")
        print(f"Architecture: {env_info['machine']}")
        print(f"Python: {env_info['python_version']}")
        print(f"CPU Cores: {env_info['cpu_count']}")
        print(f"System Memory: {env_info['available_memory_gb']:.1f}GB")
        
        print(f"\nğŸ® GPU ì •ë³´")
        if env_info['is_cuda_available']:
            print(f"CUDA: Available (v{env_info['cuda_version']})")
            print(f"GPU Count: {env_info['gpu_info']['count']}")
            for device in env_info['gpu_info']['devices']:
                print(f"  - {device['name']}: {device['memory_gb']:.1f}GB")
        else:
            print("CUDA: Not Available")
        
        print(f"\nâš¡ Unsloth ì§€ì›")
        print(f"ê¶Œì¥ ì—¬ë¶€: {'âœ… ì˜ˆ' if env_info['unsloth_recommended'] else 'âŒ ì•„ë‹ˆì˜¤'}")
        print(f"ì„¤ì¹˜ ìƒíƒœ: {'âœ… ì„¤ì¹˜ë¨' if env_info['unsloth_available'] else 'âŒ ë¯¸ì„¤ì¹˜'}")
        
        print(f"\nğŸš€ ê¶Œì¥ ì„¤ì •")
        print(f"use_unsloth: {config['use_unsloth']}")
        print(f"recommended_batch_size: {config['recommended_batch_size']}")
        print(f"fp16: {config['fp16']}, bf16: {config['bf16']}")
        print(f"dataloader_num_workers: {config['dataloader_num_workers']}")
        print("=" * 50)


def get_auto_config() -> Dict[str, Any]:
    """í™˜ê²½ì— ë§ëŠ” ìë™ ì„¤ì • ë°˜í™˜ (ì „ì—­ í•¨ìˆ˜)"""
    detector = EnvironmentDetector()
    return detector.get_recommended_config()


def should_use_unsloth() -> bool:
    """Unsloth ì‚¬ìš© ì—¬ë¶€ ìë™ íŒë‹¨ (ì „ì—­ í•¨ìˆ˜)"""
    detector = EnvironmentDetector()
    env_info = detector.detect_environment()
    return env_info['unsloth_recommended'] and env_info['unsloth_available']


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    detector = EnvironmentDetector()
    detector.print_environment_summary()
