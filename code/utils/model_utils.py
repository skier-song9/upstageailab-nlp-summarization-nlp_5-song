"""
λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € νΈν™μ„±μ„ μ„ν• μ ν‹Έλ¦¬ν‹° ν•¨μλ“¤
"""

import logging
import torch
from typing import Optional, Union
from transformers import PreTrainedModel, PreTrainedTokenizer

logger = logging.getLogger(__name__)


def resize_model_embeddings(model: PreTrainedModel, 
                          tokenizer: PreTrainedTokenizer,
                          pad_to_multiple_of: Optional[int] = None) -> PreTrainedModel:
    """
    λ¨λΈμ embedding ν¬κΈ°λ¥Ό ν† ν¬λ‚μ΄μ €μ vocab sizeμ— λ§κ² μ΅°μ •
    
    Args:
        model: μ΅°μ •ν•  λ¨λΈ
        tokenizer: ν† ν¬λ‚μ΄μ €
        pad_to_multiple_of: embedding ν¬κΈ°λ¥Ό μ΄ κ°’μ λ°°μλ΅ ν¨λ”©
        
    Returns:
        μ΅°μ •λ λ¨λΈ
    """
    vocab_size = len(tokenizer)
    model_vocab_size = model.get_input_embeddings().weight.shape[0]
    
    if vocab_size != model_vocab_size:
        logger.info(f"ν† ν¬λ‚μ΄μ € vocab size ({vocab_size})μ™€ λ¨λΈ vocab size ({model_vocab_size})κ°€ λ‹¤λ¦…λ‹λ‹¤.")
        logger.info(f"λ¨λΈ embeddingμ„ λ¦¬μ‚¬μ΄μ§•ν•©λ‹λ‹¤...")
        
        # embedding λ¦¬μ‚¬μ΄μ§•
        model.resize_token_embeddings(vocab_size, pad_to_multiple_of=pad_to_multiple_of)
        
        # μƒλ΅μ΄ vocab size ν™•μΈ
        new_vocab_size = model.get_input_embeddings().weight.shape[0]
        logger.info(f"β… λ¨λΈ vocab sizeκ°€ {new_vocab_size}λ΅ μ΅°μ •λμ—μµλ‹λ‹¤.")
    else:
        logger.info(f"β… ν† ν¬λ‚μ΄μ €μ™€ λ¨λΈμ vocab sizeκ°€ μ΄λ―Έ μΌμΉν•©λ‹λ‹¤: {vocab_size}")
    
    return model


def check_special_tokens_in_vocab(tokenizer: PreTrainedTokenizer, 
                                special_tokens: list) -> dict:
    """
    νΉμ ν† ν°μ΄ vocabμ— μλ”μ§€ ν™•μΈ
    
    Args:
        tokenizer: ν† ν¬λ‚μ΄μ €
        special_tokens: ν™•μΈν•  νΉμ ν† ν° λ¦¬μ¤νΈ
        
    Returns:
        ν† ν°λ³„ μ΅΄μ¬ μ—¬λ¶€μ™€ μΈλ±μ¤
    """
    results = {}
    vocab = tokenizer.get_vocab()
    
    for token in special_tokens:
        if token in vocab:
            results[token] = {
                'exists': True,
                'index': vocab[token]
            }
        else:
            results[token] = {
                'exists': False,
                'index': None
            }
    
    return results


def safe_add_special_tokens(tokenizer: PreTrainedTokenizer,
                          model: PreTrainedModel,
                          special_tokens: list,
                          model_name: str) -> tuple:
    """
    μ•μ „ν•κ² νΉμ ν† ν°μ„ μ¶”κ°€ν•κ³  λ¨λΈμ„ μ΅°μ •
    
    Args:
        tokenizer: ν† ν¬λ‚μ΄μ €
        model: λ¨λΈ
        special_tokens: μ¶”κ°€ν•  νΉμ ν† ν° λ¦¬μ¤νΈ
        model_name: λ¨λΈ μ΄λ¦„
        
    Returns:
        (μ΅°μ •λ ν† ν¬λ‚μ΄μ €, μ΅°μ •λ λ¨λΈ)
    """
    # ν„μ¬ μƒνƒ ν™•μΈ
    logger.info(f"π” νΉμ ν† ν° μ¶”κ°€ μ „ μƒνƒ:")
    logger.info(f"   ν† ν¬λ‚μ΄μ € vocab size: {len(tokenizer)}")
    logger.info(f"   λ¨λΈ vocab size: {model.get_input_embeddings().weight.shape[0]}")
    
    # κΈ°μ΅΄μ— μ—†λ” ν† ν°λ§ ν•„ν„°λ§
    new_tokens = [token for token in special_tokens if token not in tokenizer.get_vocab()]
    
    if new_tokens:
        logger.info(f"π“ {len(new_tokens)}κ°μ μƒλ΅μ΄ νΉμ ν† ν° μ¶”κ°€: {new_tokens[:5]}...")
        
        # ν† ν¬λ‚μ΄μ €μ— ν† ν° μ¶”κ°€
        num_added = tokenizer.add_tokens(new_tokens)
        logger.info(f"β… {num_added}κ°μ ν† ν°μ΄ ν† ν¬λ‚μ΄μ €μ— μ¶”κ°€λμ—μµλ‹λ‹¤.")
        
        # λ¨λΈ embedding λ¦¬μ‚¬μ΄μ§•
        model = resize_model_embeddings(model, tokenizer)
    else:
        logger.info("β… λ¨λ“  νΉμ ν† ν°μ΄ μ΄λ―Έ vocabμ— μμµλ‹λ‹¤.")
    
    # μµμΆ… μƒνƒ ν™•μΈ
    logger.info(f"π” νΉμ ν† ν° μ¶”κ°€ ν›„ μƒνƒ:")
    logger.info(f"   ν† ν¬λ‚μ΄μ € vocab size: {len(tokenizer)}")
    logger.info(f"   λ¨λΈ vocab size: {model.get_input_embeddings().weight.shape[0]}")
    
    # λ¶μΌμΉ κ²€μ‚¬
    if len(tokenizer) != model.get_input_embeddings().weight.shape[0]:
        raise ValueError(f"ν† ν¬λ‚μ΄μ €μ™€ λ¨λΈμ vocab sizeκ°€ μ—¬μ „ν λ¶μΌμΉν•©λ‹λ‹¤!")
    
    return tokenizer, model


def validate_tokenizer_model_compatibility(tokenizer: PreTrainedTokenizer,
                                         model: PreTrainedModel) -> bool:
    """
    ν† ν¬λ‚μ΄μ €μ™€ λ¨λΈμ νΈν™μ„± κ²€μ¦
    
    Args:
        tokenizer: ν† ν¬λ‚μ΄μ €
        model: λ¨λΈ
        
    Returns:
        νΈν™ μ—¬λ¶€
    """
    vocab_size = len(tokenizer)
    model_vocab_size = model.get_input_embeddings().weight.shape[0]
    
    if vocab_size > model_vocab_size:
        logger.error(f"β ν† ν¬λ‚μ΄μ € vocab size ({vocab_size})κ°€ λ¨λΈ vocab size ({model_vocab_size})λ³΄λ‹¤ ν½λ‹λ‹¤!")
        logger.error("μ΄λ” μΈλ±μ‹± μ—λ¬λ¥Ό μΌμΌν‚¬ μ μμµλ‹λ‹¤.")
        return False
    
    logger.info(f"β… ν† ν¬λ‚μ΄μ €μ™€ λ¨λΈμ΄ νΈν™λ©λ‹λ‹¤.")
    return True
