"""
NLP ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ ëª¨ë“ˆ

baseline.ipynbì˜ í•µì‹¬ í•™ìŠµ ë¡œì§ì„ ëª¨ë“ˆí™”í•œ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤.
WandB Sweepê³¼ì˜ í†µí•©ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì„¤ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
from dataclasses import dataclass, field
import warnings

warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
import pandas as pd
from tqdm import tqdm

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    EarlyStoppingCallback,
    TrainerCallback,
    TrainerState,
    TrainerControl,
    PreTrainedModel,
    PreTrainedTokenizer,
)

# QLoRA ë° unsloth ê´€ë ¨ import (ì„ íƒì )
try:
    from unsloth import FastLanguageModel
    from peft import LoraConfig, get_peft_model, TaskType

    UNSLOTH_AVAILABLE = True
except ImportError:
    # macOS í™˜ê²½ì´ë‚˜ unslothê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
    FastLanguageModel = None
    LoraConfig = None
    get_peft_model = None
    TaskType = None
    UNSLOTH_AVAILABLE = False

from datasets import Dataset, DatasetDict
import evaluate
import wandb

# ë¡œì»¬ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
try:
    from utils import load_config
    from utils.data_utils import DataProcessor
    from utils.metrics import RougeCalculator
    from utils.experiment_utils import ExperimentTracker, ModelRegistry
    from utils.environment_detector import EnvironmentDetector, get_auto_config, should_use_unsloth
    from utils.path_utils import PathManager, path_manager
    # í†µí•© ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ import
    from utils.error_handling import (
        handle_error, log_structured, log_performance_metric, log_experiment_event,
        safe_execute, get_logging_manager, get_error_handler
    )
except ImportError:
    # code ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ê²½ìš°
    import sys

    sys.path.append("..")
    from utils import load_config
    from utils.data_utils import DataProcessor
    from utils.metrics import RougeCalculator
    from utils.experiment_utils import ExperimentTracker, ModelRegistry
    from utils.environment_detector import EnvironmentDetector, get_auto_config, should_use_unsloth
    from utils.path_utils import PathManager, path_manager
    # í†µí•© ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ import
    from utils.error_handling import (
        handle_error, log_structured, log_performance_metric, log_experiment_event,
        safe_execute, get_logging_manager, get_error_handler
    )
logger = logging.getLogger(__name__)


@dataclass
class TrainingResult:
    """í•™ìŠµ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""

    best_metrics: Dict[str, float]
    final_metrics: Dict[str, float]
    model_path: str
    config_used: Dict[str, Any]
    training_history: List[Dict[str, Any]] = field(default_factory=list)
    wandb_run_id: Optional[str] = None
    experiment_id: Optional[str] = None


class WandbCallback(TrainerCallback):
    """WandB ë¡œê¹…ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°±"""

    def __init__(self, trainer_instance: "DialogueSummarizationTrainer") -> None:
        self.trainer_instance = trainer_instance
        self.best_metrics = {}

    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics: Dict[str, float], **kwargs):
        """í‰ê°€ ì‹œ WandBì— ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if wandb.run is not None:
            # ROUGE ì ìˆ˜ ê²°í•© (F1 ê¸°ì¤€)
            rouge_combined = (
                metrics.get("eval_rouge1", 0) * 0.33
                + metrics.get("eval_rouge2", 0) * 0.33
                + metrics.get("eval_rougeL", 0) * 0.34
            )

            log_metrics = {
                "eval/rouge1_f1": metrics.get("eval_rouge1", 0),
                "eval/rouge2_f1": metrics.get("eval_rouge2", 0),
                "eval/rougeL_f1": metrics.get("eval_rougeL", 0),
                "eval/rouge_combined_f1": rouge_combined,
                "eval/loss": metrics.get("eval_loss", 0),
                "epoch": state.epoch,
                "step": state.global_step,
            }

            # ë² ìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            if rouge_combined > self.best_metrics.get("rouge_combined_f1", 0):
                self.best_metrics = {
                    "rouge1_f1": metrics.get("eval_rouge1", 0),
                    "rouge2_f1": metrics.get("eval_rouge2", 0),
                    "rougeL_f1": metrics.get("eval_rougeL", 0),
                    "rouge_combined_f1": rouge_combined,
                    "loss": metrics.get("eval_loss", 0),
                }
                log_metrics["best/rouge_combined_f1"] = rouge_combined

            wandb.log(log_metrics)

            # ì‹¤í—˜ ì¶”ì ê¸°ì—ë„ ë¡œê¹…
            if self.trainer_instance.experiment_tracker:
                self.trainer_instance.experiment_tracker.log_metrics(metrics, step=state.global_step)

    def on_train_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        """í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì¢… ê²°ê³¼ ë¡œê¹…"""
        if wandb.run is not None:
            wandb.run.summary.update(self.best_metrics)


class DialogueSummarizationTrainer:
    """
    ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ íŠ¸ë ˆì´ë„ˆ

    baseline.ipynbì˜ í•™ìŠµ ë¡œì§ì„ ëª¨ë“ˆí™”í•˜ê³  WandB Sweepê³¼ í†µí•©í•˜ì—¬
    ìƒì‚°ì„± ë†’ì€ ì‹¤í—˜ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.

    Features:
        - ë‹¤ì¤‘ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì› (BART, T5, KoBART ë“±)
        - ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ìµœì í™” (CUDA, MPS, CPU)
        - ì‹¤í—˜ ì¶”ì  ë° ëª¨ë¸ ë“±ë¡ ì‹œìŠ¤í…œ
        - ì»¤ìŠ¤í…€ ì½œë°± ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        - í¬ê´„ì  ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
        - WandB í†µí•© ì‹¤í—˜ ê´€ë¦¬

    Example:
        >>> config = load_config('configs/bart_base.yaml')
        >>> trainer = DialogueSummarizationTrainer(config)
        >>> datasets = trainer.prepare_data()
        >>> result = trainer.train(datasets)
    """

    def __init__(self, config: Dict[str, Any], sweep_mode: bool = False, experiment_name: Optional[str] = None):
        """
        íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ConfigManagerë¡œë¶€í„°)
            sweep_mode: WandB Sweep ëª¨ë“œ ì—¬ë¶€
            experiment_name: ì‹¤í—˜ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        """
        # í†µí•© ë¡œê¹… ì´ˆê¸°í™”
        self.logging_manager = get_logging_manager()
        
        # ì‹¤í—˜ ì‹œì‘ ë¡œê¹…
        log_experiment_event(
            event_type="trainer_init",
            event_data={
                "sweep_mode": sweep_mode,
                "experiment_name": experiment_name,
                "config_keys": list(config.keys()) if config else []
            },
            component="trainer"
        )
        
        self.config = config
        self.sweep_mode = sweep_mode
        self.experiment_name = experiment_name or config.get("meta", {}).get(
            "experiment_name", "dialogue_summarization"
        )
        
        # ì‹¤í—˜ IDë¥¼ ë¡œê¹… ê´€ë¦¬ìì— ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ì™€ ì—°ë™)
        self.logging_manager.set_experiment_id(self.experiment_name)

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._setup_device()

        # ê²½ë¡œ ì„¤ì •
        self.setup_paths()

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.model = None
        self.tokenizer = None
        self.data_processor = None
        self.rouge_calculator = None
        self.trainer = None

        # ì‹¤í—˜ ê´€ë¦¬
        self.experiment_tracker = None
        self.model_registry = None

        # ë¡œê¹… ì„¤ì •
        self._setup_logging()

        logger.info(f"Trainer initialized with config: {self.experiment_name}")

    def setup_paths(self) -> None:
        """ê²½ë¡œ ì„¤ì •"""
        # ê²½ë¡œ ê´€ë¦¬ìë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ë¡œ ê´€ë¦¬
        experiment_name = self.experiment_name

        # Sweep ëª¨ë“œì¼ ë•ŒëŠ” run IDë¥¼ í¬í•¨
        if self.sweep_mode and wandb.run:
            experiment_name = f"sweep_{wandb.run.id}"
        else:
            from datetime import datetime

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            experiment_name = f"{self.experiment_name}_{timestamp}"

        # ê²½ë¡œ ê´€ë¦¬ìë¥¼ í†µí•œ ê²½ë¡œ ì„¤ì •
        self.output_dir = path_manager.get_output_path(experiment_name)
        self.model_save_dir = path_manager.get_model_path(experiment_name)
        self.results_dir = path_manager.ensure_dir(self.output_dir / "results")

        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.log_dir = path_manager.get_log_path(experiment_name)

        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (output_dirì´ ì„¤ì •ëœ í›„)
        self._add_file_handler()

    def initialize_components(self) -> None:
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        logger.info("Initializing components...")

        # ì‹¤í—˜ ì¶”ì ê¸° ì´ˆê¸°í™”
        if self.config.get("experiment_tracking", {}).get("enabled", True):
            self.experiment_tracker = ExperimentTracker(experiments_dir=self.output_dir / "experiments")
            self.model_registry = ModelRegistry(registry_dir=self.output_dir / "models")

        # í† í¬ë‚˜ì´ì € ë¡œë”©
        self._load_tokenizer()

        # ëª¨ë¸ ë¡œë”©
        self._load_model()

        # ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.data_processor = DataProcessor(tokenizer=self.tokenizer, config=self.config)

        # ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™”
        self.rouge_calculator = RougeCalculator(
            use_korean_tokenizer=self.config.get("evaluation", {}).get("rouge_tokenize_korean", True),
            use_stemmer=self.config.get("evaluation", {}).get("rouge_use_stemmer", True),
        )
        logger.info("All components initialized successfully")

    def prepare_data(
        self, train_path: Optional[str] = None, val_path: Optional[str] = None, test_path: Optional[str] = None
    ) -> DatasetDict:
        """
        ë°ì´í„° ì¤€ë¹„ - baseline.py ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
        """
        # ê²½ë¡œ ê²°ì •
        data_paths = self.config.get("data", self.config.get("general", {}))
        train_path = train_path or data_paths.get("train_path")
        val_path = val_path or data_paths.get("val_path")
        test_path = test_path or data_paths.get("test_path")

        logger.info("Loading and processing datasets (baseline style)...")

        datasets = {}

        # Train ë°ì´í„° ì²˜ë¦¬
        if train_path:
            logger.info(f"Loading train data from: {train_path}")

            # pandasë¡œ CSV ì½ê¸°
            train_df = pd.read_csv(train_path)
            train_df = train_df[["fname", "dialogue", "summary"]]

            # baselineì˜ make_input ë¡œì§
            encoder_inputs = []
            decoder_inputs = []
            decoder_outputs = []

            if self.tokenizer is None:
                raise ValueError("Tokenizer failed to load")
            bos_token = self.tokenizer.bos_token
            eos_token = self.tokenizer.eos_token

            for dialogue, summary in zip(train_df["dialogue"], train_df["summary"]):
                encoder_input = dialogue
                decoder_input = f"{bos_token} {summary}"
                decoder_output = f"{summary} {eos_token}"

                encoder_inputs.append(encoder_input)
                decoder_inputs.append(decoder_input)
                decoder_outputs.append(decoder_output)

            # í† í¬ë‚˜ì´ì§•
            tokenized_encoder = self.tokenizer(
                encoder_inputs,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.config["tokenizer"]["encoder_max_len"],
                return_token_type_ids=False,
            )

            tokenized_decoder_inputs = self.tokenizer(
                decoder_inputs,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.config["tokenizer"]["decoder_max_len"],
                return_token_type_ids=False,
            )

            tokenized_decoder_outputs = self.tokenizer(
                decoder_outputs,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.config["tokenizer"]["decoder_max_len"],
                return_token_type_ids=False,
            )

            # HuggingFace Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            train_dataset = Dataset.from_dict(
                {
                    "input_ids": tokenized_encoder["input_ids"],
                    "attention_mask": tokenized_encoder["attention_mask"],
                    "decoder_input_ids": tokenized_decoder_inputs["input_ids"],
                    "labels": tokenized_decoder_outputs["input_ids"],
                }
            )

            datasets["train"] = train_dataset
            logger.info(f"Train dataset size: {len(train_dataset)}")

        # Validation ë°ì´í„° ì²˜ë¦¬
        if val_path:
            logger.info(f"Loading validation data from: {val_path}")

            # pandasë¡œ CSV ì½ê¸°
            val_df = pd.read_csv(val_path)
            val_df = val_df[["fname", "dialogue", "summary"]]

            # baselineì˜ make_input ë¡œì§
            encoder_inputs = []
            decoder_inputs = []
            decoder_outputs = []

            bos_token = self.tokenizer.bos_token
            eos_token = self.tokenizer.eos_token

            for dialogue, summary in zip(val_df["dialogue"], val_df["summary"]):
                encoder_input = dialogue
                decoder_input = f"{bos_token} {summary}"
                decoder_output = f"{summary} {eos_token}"

                encoder_inputs.append(encoder_input)
                decoder_inputs.append(decoder_input)
                decoder_outputs.append(decoder_output)

            # í† í¬ë‚˜ì´ì§•
            tokenized_encoder = self.tokenizer(
                encoder_inputs,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.config["tokenizer"]["encoder_max_len"],
                return_token_type_ids=False,
            )

            tokenized_decoder_inputs = self.tokenizer(
                decoder_inputs,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.config["tokenizer"]["decoder_max_len"],
                return_token_type_ids=False,
            )

            tokenized_decoder_outputs = self.tokenizer(
                decoder_outputs,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.config["tokenizer"]["decoder_max_len"],
                return_token_type_ids=False,
            )

            # HuggingFace Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            val_dataset = Dataset.from_dict(
                {
                    "input_ids": tokenized_encoder["input_ids"],
                    "attention_mask": tokenized_encoder["attention_mask"],
                    "decoder_input_ids": tokenized_decoder_inputs["input_ids"],
                    "labels": tokenized_decoder_outputs["input_ids"],
                }
            )

            datasets["validation"] = val_dataset
            logger.info(f"Validation dataset size: {len(val_dataset)}")

        # Test ë°ì´í„° ì²˜ë¦¬
        if test_path:
            logger.info(f"Loading test data from: {test_path}")

            # pandasë¡œ CSV ì½ê¸°
            test_df = pd.read_csv(test_path)
            test_df = test_df[["fname", "dialogue"]]  # testëŠ” summary ì—†ìŒ

            # í† í¬ë‚˜ì´ì§• (encoderë§Œ)
            tokenized_encoder = self.tokenizer(
                test_df["dialogue"].tolist(),
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.config["tokenizer"]["encoder_max_len"],
                return_token_type_ids=False,
            )

            # HuggingFace Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            test_dataset = Dataset.from_dict(
                {"input_ids": tokenized_encoder["input_ids"], "attention_mask": tokenized_encoder["attention_mask"]}
            )

            datasets["test"] = test_dataset
            logger.info(f"Test dataset size: {len(test_dataset)}")

        return DatasetDict(datasets)

    def train(self, dataset: DatasetDict, resume_from_checkpoint: Optional[str] = None) -> TrainingResult:
        """
        ëª¨ë¸ í•™ìŠµ

        Args:
            dataset: í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹
            resume_from_checkpoint: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì¬ê°œ ì‹œ)

        Returns:
            í•™ìŠµ ê²°ê³¼
        """
        # ì‹¤í—˜ ì—°ì†ì„± ì¶”ì  ì‹œì‘
        from utils.experiment_continuity import get_continuity_manager
        continuity_manager = get_continuity_manager()
        
        # ì‹¤í—˜ ID ìƒì„± (ê¸°ì¡´ experiment_tracker ID ì‚¬ìš©)
        experiment_id = f"exp_{int(time.time())}"
        
        # ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™”
        checkpoint = continuity_manager.start_experiment(
            experiment_id=experiment_id,
            experiment_name=self.experiment_name,
            config=self.config
        )
        
        logger.info(f"ğŸ“ ì‹¤í—˜ ì—°ì†ì„± ì¶”ì  ì‹œì‘: {experiment_id}")
        
        # ì‹¤í—˜ ì‹œì‘
        if self.experiment_tracker:
            # WandB ì´ˆê¸°í™” (ì¡°ì¥ë‹˜ ì§€ì‹œì‚¬í•­ ë°˜ì˜)
            if self.config["training"].get("report_to", "none") in ["all", "wandb"]:
                from utils.experiment_utils import get_korean_time_format

                # í•œêµ­ ì‹œê°„ ê¸°ì¤€ MMDDHHMM í˜•ì‹
                korean_time = get_korean_time_format("MMDDHHMM")

                # ëª¨ë¸ ì•„í‚¤í…ì²˜ ì²« ê¸€ì ì¶”ì¶œ
                model_arch = self.config.get("model", {}).get("architecture", "unknown")
                model_prefix = model_arch[0] if model_arch != "unknown" else "x"

                # WandB run name ìƒì„±
                run_name = f"{model_prefix}_{self.experiment_name}_{korean_time}"

                # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                os.environ["WANDB_LOG_MODEL"] = "false"  # mT5ëŠ” í¬ê¸°ê°€ ì»¤ì„œ ë¡œì»¬ë§Œ ì €ì¥
                os.environ["TOKENIZERS_PARALLELISM"] = "true"

                # ê²¬ê³ í•œ WandB ì´ˆê¸°í™” (ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨ ì‹œ ì˜¤í”„ë¼ì¸ ëª¨ë“œ ìë™ ì „í™˜)
                from utils.wandb_utils import safe_setup_wandb_for_experiment
                
                wandb_result = safe_setup_wandb_for_experiment(
                    config=self.config,
                    experiment_name=self.experiment_name,
                    sweep_mode=self.sweep_mode
                )
                
                # ì—°ê²° ìƒíƒœ ë¡œê¹…
                if wandb_result.get('offline_mode', False):
                    logger.warning("ğŸ”„ WandB ì˜¤í”„ë¼ì¸ ëª¨ë“œì—ì„œ ì‹¤í—˜ ì§„í–‰")
                else:
                    logger.info("âœ… WandB ì˜¨ë¼ì¸ ëª¨ë“œë¡œ ì‹¤í—˜ ì‹œì‘")

            experiment_id = self.experiment_tracker.start_experiment(
                name=self.experiment_name,
                description=f"Training {self.config.get('model', {}).get('architecture', 'unknown')} model",
                config=self.config,
                model_type=self.config.get("model", {}).get("architecture", "unknown"),
                dataset_info={
                    "train_size": len(dataset.get("train", [])),
                    "val_size": len(dataset.get("validation", [])),
                },
                wandb_run_id=wandb.run.id if wandb.run else None,
            )
        else:
            experiment_id = None

        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = self._get_training_arguments()

        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            padding=True,
            max_length=self.config["tokenizer"]["encoder_max_len"],
        )

        # í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ - HuggingFace Trainerì˜ ì½œë°±ìœ¼ë¡œ ì‚¬ìš©ë¨
        def compute_metrics(eval_preds: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:
            """
            í•™ìŠµ ì¤‘ í‰ê°€ ë‹¨ê³„ì—ì„œ ROUGE ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” ì¤‘ì²© í•¨ìˆ˜

            Args:
                eval_preds: (predictions, labels) íŠœí”Œ

            Returns:
                ROUGE ì ìˆ˜ë“¤ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
            """
            preds, labels = eval_preds

            # ğŸ›¡ï¸ ì•ˆì „í•œ í† í° ë””ì½”ë”© (IndexError ë°©ì§€)
            def safe_decode(token_ids, tokenizer):
                """í† í° ID ë²”ìœ„ë¥¼ ì²´í¬í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë””ì½”ë”©"""
                try:
                    # í† í° IDê°€ vocab ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ëŠ”ì§€ ì²´í¬
                    if hasattr(tokenizer, 'vocab_size'):
                        vocab_size = tokenizer.vocab_size
                    else:
                        vocab_size = len(tokenizer.get_vocab())
                    
                    # ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ í† í° IDë¥¼ í´ë¨í•‘
                    if isinstance(token_ids, np.ndarray):
                        token_ids = np.clip(token_ids, 0, vocab_size - 1)
                    
                    return tokenizer.batch_decode(token_ids, skip_special_tokens=True)
                except Exception as e:
                    logger.warning(f"Token decoding failed: {e}. Using fallback.")
                    # í´ë°±: ë¹ˆ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                    return [""] * len(token_ids)
            
            # ì•ˆì „í•œ í† í° ë””ì½”ë”© ì ìš©
            decoded_preds = safe_decode(preds, self.tokenizer)
            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)

            # HuggingFaceì—ì„œ ì‚¬ìš©í•˜ëŠ” -100 íŒ¨ë”© í† í°ì„ ì •ìƒ í† í°ìœ¼ë¡œ ë³€í™˜
            # -100ì€ loss ê³„ì‚°ì—ì„œ ë¬´ì‹œë˜ëŠ” ë¼ë²¨ì´ì§€ë§Œ ë””ì½”ë”©ì—ì„œëŠ” ë¬¸ì œê°€ ë¨
            labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
            decoded_labels = safe_decode(labels, self.tokenizer)

            # ëŒ€í™” ìš”ì•½ì— íŠ¹í™”ëœ ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° (Multi-reference ì§€ì›)
            rouge_scores = []
            for pred, ref in zip(decoded_preds, decoded_labels):
                score = self.rouge_calculator.calculate_single_reference(pred, ref)
                rouge_scores.append(score)
            
            # í‰ê·  ì ìˆ˜ ê³„ì‚°
            avg_rouge1_f1 = np.mean([score.rouge1.f1 for score in rouge_scores])
            avg_rouge2_f1 = np.mean([score.rouge2.f1 for score in rouge_scores])
            avg_rougeL_f1 = np.mean([score.rougeL.f1 for score in rouge_scores])
            avg_combined_f1 = avg_rouge1_f1 + avg_rouge2_f1 + avg_rougeL_f1
            
            result = {
                'rouge1_f1': avg_rouge1_f1,
                'rouge2_f1': avg_rouge2_f1,
                'rougeL_f1': avg_rougeL_f1,
                'rouge_combined_f1': avg_combined_f1,
                'rouge1_precision': np.mean([score.rouge1.precision for score in rouge_scores]),
                'rouge1_recall': np.mean([score.rouge1.recall for score in rouge_scores]),
                'rouge2_precision': np.mean([score.rouge2.precision for score in rouge_scores]),
                'rouge2_recall': np.mean([score.rouge2.recall for score in rouge_scores]),
                'rougeL_precision': np.mean([score.rougeL.precision for score in rouge_scores]),
                'rougeL_recall': np.mean([score.rougeL.recall for score in rouge_scores])
            }

            return result

        # ì½œë°± ì„¤ì •
        callbacks = [WandbCallback(self)]

        # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
        if self.config["training"].get("early_stopping_patience"):
            callbacks.append(
                EarlyStoppingCallback(
                    early_stopping_patience=self.config["training"]["early_stopping_patience"],
                    early_stopping_threshold=0.001,
                )
            )

        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        self.trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=dataset.get("train"),
            eval_dataset=dataset.get("validation"),
            tokenizer=self.tokenizer,
            compute_metrics=compute_metrics,
            callbacks=callbacks,
        )

        # í•™ìŠµ ì‹œì‘
        logger.info("Starting training...")
        
        # ì‹¤í—˜ ì—°ì†ì„± import
        from utils.experiment_continuity import save_experiment_checkpoint
        
        # í•™ìŠµ ì‹œì‘ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        training_info = {
            'total_epochs': self.config['training'].get('num_train_epochs', 0),
            'train_dataset_size': len(dataset.get('train', [])),
            'eval_dataset_size': len(dataset.get('validation', [])),
            'batch_size': self.config['training'].get('per_device_train_batch_size', 0)
        }
        save_experiment_checkpoint('training_started', progress_info=training_info)
        logger.info("ğŸ“‹ í•™ìŠµ ì‹œì‘ ì²´í¬í¬ì¸íŠ¸ ì €ì¥")

        try:
            train_result = self.trainer.train(resume_from_checkpoint=resume_from_checkpoint)

            # ìµœì¢… í‰ê°€
            logger.info("Running final evaluation...")
            eval_results = self.trainer.evaluate()

            # ëª¨ë¸ ì €ì¥
            best_model_path = self.model_save_dir / "best_model"
            self.trainer.save_model(str(best_model_path))
            self.tokenizer.save_pretrained(str(best_model_path))

            # ê²°ê³¼ ì •ë¦¬
            wandb_callback = callbacks[0]
            training_result = TrainingResult(
                best_metrics=wandb_callback.best_metrics,
                final_metrics=eval_results,
                model_path=str(best_model_path),
                config_used=self.config,
                training_history=[],  # í–¥í›„ êµ¬í˜„
                wandb_run_id=wandb.run.id if wandb.run else None,
                experiment_id=experiment_id,
            )

            # ì‹¤í—˜ ì¢…ë£Œ
            if self.experiment_tracker:
                self.experiment_tracker.end_experiment(
                    experiment_id=experiment_id,
                    final_metrics=eval_results,
                    best_metrics=wandb_callback.best_metrics,
                    status="completed",
                )

            # ëª¨ë¸ ë“±ë¡
            if self.model_registry:
                model_id = self.model_registry.register_model(
                    name=f"{self.config.get('model', {}).get('architecture', 'unknown')}_{self.experiment_name}",
                    architecture=self.config.get("model", {}).get("architecture", "unknown"),
                    checkpoint=self.config.get("model", {}).get(
                        "checkpoint", self.config.get("general", {}).get("model_name", "")
                    ),
                    config=self.config,
                    performance=wandb_callback.best_metrics,
                    training_info={
                        "epochs": self.config["training"]["num_train_epochs"],
                        "batch_size": self.config["training"]["per_device_train_batch_size"],
                        "learning_rate": self.config["training"]["learning_rate"],
                    },
                    file_path=str(best_model_path),
                    experiment_id=experiment_id,
                )
                logger.info(f"Model registered with ID: {model_id}")

            # ê²°ê³¼ ì €ì¥
            self._save_results(training_result)
            
            # ì‹¤í—˜ ì„±ê³µ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            from utils.experiment_continuity import get_continuity_manager
            continuity_manager = get_continuity_manager()
            continuity_manager.finish_experiment(success=True, final_metrics=eval_results)
            logger.info("ğŸ“‹ ì‹¤í—˜ ì„±ê³µ ì²´í¬í¬ì¸íŠ¸ ì €ì¥")
            
            return training_result

        except Exception as e:
            logger.error(f"Training failed with error: {type(e).__name__}: {str(e)}")
            logger.error(f"Current config: {self.config.get('model', {}).get('checkpoint', 'Unknown')}")
            logger.error(f"Device: {self.device}")
            import traceback

            logger.debug(f"Full traceback: {traceback.format_exc()}")
            logger.error(f"Training failed: {str(e)}")
            
            # ì‹¤í—˜ ì‹¤íŒ¨ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            from utils.experiment_continuity import get_continuity_manager
            continuity_manager = get_continuity_manager()
            continuity_manager.finish_experiment(success=False, final_metrics={'error': str(e)})
            logger.info("ğŸ“‹ ì‹¤í—˜ ì‹¤íŒ¨ ì²´í¬í¬ì¸íŠ¸ ì €ì¥")
            
            if self.experiment_tracker and experiment_id:
                self.experiment_tracker.end_experiment(experiment_id=experiment_id, status="failed", notes=str(e))
            raise

    def evaluate(self, dataset: Dataset, metric_key_prefix: str = "eval") -> Dict[str, float]:
        """
        ëª¨ë¸ í‰ê°€

        Args:
            dataset: í‰ê°€ ë°ì´í„°ì…‹
            metric_key_prefix: ë©”íŠ¸ë¦­ í‚¤ ì ‘ë‘ì‚¬

        Returns:
            í‰ê°€ ê²°ê³¼
        """
        if not self.trainer:
            raise ValueError("Trainer not initialized. Call train() first.")

        results = self.trainer.evaluate(eval_dataset=dataset, metric_key_prefix=metric_key_prefix)

        return results

    def generate_predictions(self, dataset: Dataset, max_samples: Optional[int] = None) -> List[Dict[str, str]]:
        """
        ì˜ˆì¸¡ ìƒì„±

        Args:
            dataset: ì…ë ¥ ë°ì´í„°ì…‹
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)

        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        self.model.eval()
        predictions = []

        # ìƒ˜í”Œë§
        if max_samples:
            indices = np.random.choice(len(dataset), min(max_samples, len(dataset)), replace=False)
            dataset = dataset.select(indices)

        # ìƒì„± ì„¤ì •
        gen_config = self.config["generation"]

        with torch.no_grad():
            for example in tqdm(dataset, desc="Generating predictions"):
                # í† í°í™”
                inputs = self.tokenizer(
                    example["input"],
                    max_length=self.config["tokenizer"]["encoder_max_len"],
                    truncation=True,
                    padding=True,
                    return_tensors="pt",
                ).to(self.device)

                # ìƒì„±
                outputs = self.model.generate(
                    **inputs,
                    max_length=gen_config["max_length"],
                    num_beams=gen_config["num_beams"],
                    length_penalty=gen_config.get("length_penalty", 1.0),
                    no_repeat_ngram_size=gen_config.get("no_repeat_ngram_size", 2),
                    early_stopping=gen_config.get("early_stopping", True),
                    do_sample=gen_config.get("do_sample", False),
                    temperature=gen_config.get("temperature", 1.0) if gen_config.get("do_sample") else None,
                    top_k=gen_config.get("top_k", 50) if gen_config.get("do_sample") else None,
                    top_p=gen_config.get("top_p", 0.95) if gen_config.get("do_sample") else None,
                )

                # ë””ì½”ë”©
                prediction = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

                predictions.append(
                    {"input": example["input"], "prediction": prediction, "reference": example.get("target", "")}
                )

        return predictions

    def _setup_device(self) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        from utils.device_utils import get_robust_optimal_device, setup_device_config
        
        device_config = self.config["general"].get("device", "auto")
        
        if device_config == "auto":
            # ê²¬ê³ í•œ ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ (ì»´í…Œì´ë„ˆ í™˜ê²½ ëŒ€ì‘)
            device, device_info = get_robust_optimal_device()

            # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            model_size = self.config.get("model", {}).get("size", "base")
            optimization_config = setup_device_config(device_info, model_size)

            # ìµœì í™” ì„¤ì •ì„ configì— ë³‘í•©
            if "training" not in self.config:
                self.config["training"] = {}

            # ê¸°ì¡´ ì„¤ì •ê³¼ ë³‘í•© (ê¸°ì¡´ ì„¤ì • ìš°ì„ )
            opt_dict = optimization_config.to_dict()
            for key, value in opt_dict.items():
                if key not in self.config["training"]:
                    self.config["training"][key] = value

            logger.info(f"ìë™ ê°ì§€ëœ ë””ë°”ì´ìŠ¤: {device_info}")
            logger.info(
                f"ìµœì í™” ì„¤ì • ì ìš©ë¨: batch_size={optimization_config.batch_size}, "
                f"mixed_precision={optimization_config.mixed_precision}, "
                f"num_workers={optimization_config.num_workers}"
            )
        else:
            # ìˆ˜ë™ ì„¤ì •
            device = torch.device(device_config)
            logger.info(f"ìˆ˜ë™ ì„¤ì •ëœ ë””ë°”ì´ìŠ¤: {device}")

        logger.info(f"Using device: {device}")
        return device

    def _setup_logging(self) -> None:
        """ë¡œê¹… ì„¤ì •"""
        log_level = self.config.get("logging", {}).get("level", "INFO")

        # ê¸°ë³¸ ë¡œê¹… ì„¤ì • (ì½˜ì†”ë§Œ)
        logging.basicConfig(
            level=getattr(logging, log_level),
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[logging.StreamHandler(sys.stdout)],
        )

        # output_dirì´ ì„¤ì •ëœ í›„ì— íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        if hasattr(self, "output_dir") and self.output_dir:
            self._add_file_handler()

    def _add_file_handler(self) -> None:
        """ë¡œê¹…ì— íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (output_dir ì„¤ì • í›„ í˜¸ì¶œ)"""
        if hasattr(self, "output_dir") and self.output_dir:
            # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ê°€ì ¸ì˜¤ê¸°
            root_logger = logging.getLogger()

            # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
            file_handler = logging.FileHandler(self.output_dir / "training.log")
            file_handler.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
            root_logger.addHandler(file_handler)
            logger.info(f"ë¡œê¹… íŒŒì¼ ìƒì„±: {self.output_dir / 'training.log'}")

    def _load_tokenizer(self) -> None:
        """í† í¬ë‚˜ì´ì € ë¡œë”© ê°œì„ """
        # model ì„¹ì…˜ì´ ì—†ìœ¼ë©´ generalì—ì„œ model_name ì‚¬ìš©
        if "model" in self.config:
            model_checkpoint = self.config.get("model", {}).get(
                "checkpoint", self.config.get("general", {}).get("model_name", "")
            )
        else:
            model_checkpoint = self.config.get("general", {}).get("model_name")
            if not model_checkpoint:
                raise ValueError(
                    "Model checkpoint not found in config. Please specify 'model.checkpoint' or 'general.model_name'"
                )

        logger.info(f"Loading tokenizer: {model_checkpoint}")

        try:
            # ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì € ì„¤ì •
            tokenizer_kwargs = {"trust_remote_code": True, "use_fast": True}

            # T5/mT5 ëª¨ë¸ íŠ¹ë³„ ì²˜ë¦¬
            if "mt5" in model_checkpoint.lower() or "t5" in model_checkpoint.lower():
                tokenizer_kwargs["use_fast"] = False  # T5/mT5ëŠ” SentencePieceë¡œ use_fast=False ì‚¬ìš©
                tokenizer_kwargs["legacy"] = False  # T5 legacy ëª¨ë“œ ë¹„í™œì„±í™”

            # ì•ˆì „í•œ í† í¬ë‚˜ì´ì € ë¡œë”© (ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨ ì‹œ ìºì‹œ í™œìš©)
            from utils.model_loading_utils import safe_load_tokenizer
            
            self.tokenizer = safe_load_tokenizer(model_checkpoint, **tokenizer_kwargs)
            
            # None ë°˜í™˜ ì‹œ ì§ì ‘ ë¡œë”© ì‹œë„ (íŠ¹ì • ëª¨ë¸ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
            if self.tokenizer is None:
                logger.warning(f"safe_load_tokenizer ì‹¤íŒ¨, ì§ì ‘ ë¡œë”© ì‹œë„: {model_checkpoint}")
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, **tokenizer_kwargs)
                except Exception as direct_error:
                    logger.error(f"ì§ì ‘ ë¡œë”©ë„ ì‹¤íŒ¨: {direct_error}")
                    # ëŒ€ì²´ ëª¨ë¸ë¡œ í´ë°± (KoBART ê¸°ë³¸)
                    logger.warning("ê¸°ë³¸ KoBART í† í¬ë‚˜ì´ì €ë¡œ í´ë°±")
                    self.tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

            # íŠ¹ìˆ˜ í† í° ì„¤ì • (í•„ìš”ì‹œ)
            model_architecture = self.config.get("model", {}).get("architecture", "")
            if model_architecture in ["kogpt2", "gpt2"]:
                # GPT ê³„ì—´ì€ pad_tokenì´ ì—†ì„ ìˆ˜ ìˆìŒ
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token

            logger.info(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ: {type(self.tokenizer).__name__}")

        except Exception as e:
            logger.error(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def _get_architecture_from_model_name(self) -> str:
        """ëª¨ë¸ ì´ë¦„ìœ¼ë¡œë¶€í„° ì•„í‚¤í…ì²˜ ì¶”ë¡ """
        model_name = self.config.get("general", {}).get("model_name", "").lower()

        if "bart" in model_name or "kobart" in model_name:
            return "bart"
        elif "mt5" in model_name:
            return "mt5"
        elif "t5" in model_name:
            return "t5"
        elif "gpt" in model_name:
            return "gpt2"
        else:
            return "bart"  # ê¸°ë³¸ê°’

    def _load_model(self) -> None:
        """ëª¨ë¸ ë¡œë”© (unsloth ë° QLoRA ì§€ì›)"""
        model_checkpoint = self.config.get("model", {}).get(
            "checkpoint", self.config.get("general", {}).get("model_name", "")
        )
        architecture = self.config.get("model", {}).get("architecture", "bart")

        # QLoRA ì„¤ì • í™•ì¸
        qlora_config = self.config.get("qlora", {})
        use_unsloth = qlora_config.get("use_unsloth", False) and UNSLOTH_AVAILABLE
        use_qlora = qlora_config.get("use_qlora", False)

        logger.info(f"Loading model: {model_checkpoint} ({architecture})")
        logger.info(f"QLoRA enabled: {use_qlora}, unsloth enabled: {use_unsloth}")

        if use_unsloth and architecture in ["kobart", "bart", "t5", "mt5"]:
            # unslothë¡œ ëª¨ë¸ ë¡œë”© (ìµœëŒ€ 75% ë©”ëª¨ë¦¬ ê°ì†Œ)
            self._load_model_with_unsloth(model_checkpoint, qlora_config)

        elif use_qlora:
            # ì¼ë°˜ QLoRA ëª¨ë¸ ë¡œë”©
            self._load_model_with_qlora(model_checkpoint, architecture, qlora_config)

        else:
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ë°©ì‹
            self._load_model_standard(model_checkpoint, architecture)

        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (QLoRA ëª¨ë¸ì€ ì´ë¯¸ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ì— ìˆìŒ)
        if not (use_unsloth or use_qlora):
            self.model = self.model.to(self.device)

        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (ë©”ëª¨ë¦¬ ìµœì í™”)
        if self.config["training"].get("gradient_checkpointing", False):
            if hasattr(self.model, "gradient_checkpointing_enable"):
                self.model.gradient_checkpointing_enable()
            else:
                logger.warning("ëª¨ë¸ì´ gradient_checkpointingì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ë¡œë”© ì™„ë£Œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        from utils.experiment_continuity import save_experiment_checkpoint
        model_info = {
            'architecture': architecture,
            'checkpoint': model_checkpoint,
            'model_loaded': True,
            'use_qlora': use_qlora,
            'use_unsloth': use_unsloth,
            'parameters': sum(p.numel() for p in self.model.parameters()) if hasattr(self, 'model') else 0
        }
        save_experiment_checkpoint('model_loaded', model_info=model_info)
        logger.info("ğŸ“‹ ëª¨ë¸ ë¡œë”© ì™„ë£Œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥")

    def _load_model_with_unsloth(self, model_checkpoint: str, qlora_config: Dict[str, Any]) -> None:
        """
        unslothë¥¼ ì‚¬ìš©í•œ ê³ íš¨ìœ¨ ëª¨ë¸ ë¡œë”© (ë©”ëª¨ë¦¬ 75% ê°ì†Œ)

        Args:
            model_checkpoint: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            qlora_config: QLoRA ì„¤ì •
        """
        logger.info("ğŸš€ unslothë¡œ ê³ íš¨ìœ¨ ëª¨ë¸ ë¡œë”© ì¤‘...")

        try:
            # unsloth FastLanguageModelë¡œ ëª¨ë¸ ë¡œë”©
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_checkpoint,
                max_seq_length=self.config["tokenizer"].get("encoder_max_len", 512)
                + self.config["tokenizer"].get("decoder_max_len", 200),
                dtype=torch.float16 if self.config["training"].get("fp16") else torch.float32,
                load_in_4bit=qlora_config.get("load_in_4bit", True),
            )

            # LoRA ì„¤ì • ì¶”ê°€
            model = FastLanguageModel.get_peft_model(
                model,
                r=qlora_config.get("lora_rank", 16),
                target_modules=qlora_config.get(
                    "target_modules", ["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2"]
                ),
                lora_alpha=qlora_config.get("lora_alpha", 32),
                lora_dropout=qlora_config.get("lora_dropout", 0.1),
                bias="none",
                use_gradient_checkpointing="unsloth",  # unsloth ìµœì í™”
                random_state=42,
            )

            self.model = model
            logger.info("âœ… unsloth ëª¨ë¸ ë¡œë”© ì„±ê³µ! ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 75% ê°ì†Œ ì˜ˆìƒ")

        except Exception as e:
            logger.error(f"âŒ unsloth ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.info("í´ë°± ëª¨ë“œ: ì¼ë°˜ QLoRAë¡œ ëŒ€ì²´")
            self._load_model_with_qlora(model_checkpoint, "kobart", qlora_config)

    def _load_model_with_qlora(self, model_checkpoint: str, architecture: str, qlora_config: Dict[str, Any]) -> None:
        """
        ì¼ë°˜ QLoRAë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ë¡œë”©

        Args:
            model_checkpoint: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            architecture: ëª¨ë¸ ì•„í‚¤í…ì²˜
            qlora_config: QLoRA ì„¤ì •
        """
        logger.info("ğŸ”‹ QLoRAë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ë¡œë”© ì¤‘...")

        try:
            # 4-bit ì–‘ìí™” ì„¤ì •
            from transformers import BitsAndBytesConfig

            bnb_config = BitsAndBytesConfig(
                load_in_4bit=qlora_config.get("load_in_4bit", True),
                bnb_4bit_compute_dtype=getattr(torch, qlora_config.get("bnb_4bit_compute_dtype", "bfloat16")),
                bnb_4bit_quant_type=qlora_config.get("bnb_4bit_quant_type", "nf4"),
                bnb_4bit_use_double_quant=qlora_config.get("bnb_4bit_use_double_quant", True),
            )
            
            # ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© (ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨ ì‹œ ìºì‹œ í™œìš©)
            from utils.model_loading_utils import safe_load_model
            
            if architecture in ["kobart", "bart", "t5", "mt5"]:
                model = safe_load_model(
                    AutoModelForSeq2SeqLM,
                    model_checkpoint,
                    quantization_config=bnb_config,
                    device_map="auto",
                    torch_dtype=torch.bfloat16,
                    trust_remote_code=True,
                )
            else:
                model = safe_load_model(
                    AutoModelForCausalLM,
                    model_checkpoint,
                    quantization_config=bnb_config,
                    device_map="auto",
                    torch_dtype=torch.bfloat16,
                    trust_remote_code=True,
                )

            # LoRA ì„¤ì • - PEFT ì–´ëŒ‘í„° ë°˜ë“œì‹œ ì ìš©
            if LoraConfig is not None:
                lora_config = LoraConfig(
                    r=qlora_config.get("lora_rank", 64),
                    lora_alpha=qlora_config.get("lora_alpha", 128),
                    target_modules=qlora_config.get("target_modules", ["q_proj", "k_proj", "v_proj", "out_proj"]),
                    lora_dropout=qlora_config.get("lora_dropout", 0.05),
                    bias="none",
                    task_type=(
                        TaskType.SEQ_2_SEQ_LM if architecture in ["kobart", "bart", "t5", "mt5"] else TaskType.CAUSAL_LM
                    ),
                )

                # PEFT ëª¨ë¸ ìƒì„± - ì´ ë¶€ë¶„ì´ í•µì‹¬
                model = get_peft_model(model, lora_config)
                model.print_trainable_parameters()  # í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° í™•ì¸
                logger.info("âœ… QLoRA + PEFT ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
            else:
                raise ImportError("PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            self.model = model

        except ImportError:
            logger.error("âŒ bitsandbytes ë˜ëŠ” peft ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            logger.info("í´ë°± ëª¨ë“œ: í‘œì¤€ ëª¨ë¸ ë¡œë”©")
            self._load_model_standard(model_checkpoint, architecture)
        except Exception as e:
            logger.error(f"âŒ QLoRA ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.info("í´ë°± ëª¨ë“œ: í‘œì¤€ ëª¨ë¸ ë¡œë”©")
            self._load_model_standard(model_checkpoint, architecture)

    def _load_model_standard(self, model_checkpoint: str, architecture: str) -> None:
        """
        í‘œì¤€ ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ ë°©ì‹)
    
        Args:
            model_checkpoint: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            architecture: ëª¨ë¸ ì•„í‚¤í…ì²˜
        """
        logger.info("ğŸ“š í‘œì¤€ ëª¨ë¸ ë¡œë”© ì¤‘...")
    
        # ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© (ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨ ì‹œ ìºì‹œ í™œìš©)
        from utils.model_loading_utils import safe_load_model
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ë”°ë¥¸ ë¡œë”©
        if architecture in ["kobart", "bart", "t5", "mt5"]:
            # ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ëª¨ë¸
            self.model = safe_load_model(
                AutoModelForSeq2SeqLM,
                model_checkpoint, 
                torch_dtype=torch.float16 if self.config["training"].get("fp16") else torch.float32
            )
        elif architecture in ["kogpt2", "gpt2", "gpt-neo"]:
            # ì¸ê³¼ ì–¸ì–´ ëª¨ë¸
            self.model = safe_load_model(
                AutoModelForCausalLM,
                model_checkpoint, 
                torch_dtype=torch.float16 if self.config["training"].get("fp16") else torch.float32
            )
        else:
            raise ValueError(f"Unsupported architecture: {architecture}")
    
            logger.info("âœ… í‘œì¤€ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
    

    def _get_model_specific_config(self, architecture: str, checkpoint: str) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì • ë°˜í™˜"""
        config = {}

        # T5 ê³„ì—´
        if architecture in ["t5", "mt5", "flan-t5"]:
            config["prefix"] = "summarize: "  # T5ëŠ” task prefix í•„ìš”

        # GPT ê³„ì—´
        elif architecture in ["gpt2", "kogpt2", "gpt-neo"]:
            config["max_length"] = (
                self.config["tokenizer"]["encoder_max_len"] + self.config["tokenizer"]["decoder_max_len"]
            )
            config["pad_token_id"] = self.tokenizer.pad_token_id

        return config

    def _preprocess_for_model(self, examples: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ë°ì´í„° ì „ì²˜ë¦¬"""
        architecture = self.config.get("model", {}).get("architecture", self._get_architecture_from_model_name())

        if architecture in ["t5", "mt5", "flan-t5"]:
            # T5ëŠ” prefix ì¶”ê°€
            examples["input"] = ["summarize: " + inp for inp in examples["input"]]

        elif architecture in ["gpt2", "kogpt2", "gpt-neo"]:
            # GPTëŠ” ì…ë ¥ê³¼ íƒ€ê²Ÿì„ ì—°ê²°
            examples["input"] = [f"{inp} TL;DR: {tgt}" for inp, tgt in zip(examples["input"], examples["target"])]

        return examples

    def _get_training_arguments(self) -> Seq2SeqTrainingArguments:
        """í•™ìŠµ ì¸ì ìƒì„±"""
        train_config = self.config["training"]

        # ê¸°ë³¸ ì¸ì
        args_dict = {
            "output_dir": str(self.output_dir / "checkpoints"),
            "overwrite_output_dir": True,
            "do_train": True,
            "do_eval": True,
            "eval_strategy": train_config.get("eval_strategy", "steps"),
            "eval_steps": train_config.get("eval_steps", 500),
            "save_strategy": train_config.get("save_strategy", "steps"),
            "save_steps": train_config.get("save_steps", 500),
            "save_total_limit": train_config.get("save_total_limit", 3),
            "per_device_train_batch_size": train_config["per_device_train_batch_size"],
            "per_device_eval_batch_size": train_config.get(
                "per_device_eval_batch_size", train_config["per_device_train_batch_size"]
            ),
            "gradient_accumulation_steps": train_config.get("gradient_accumulation_steps", 1),
            "learning_rate": train_config["learning_rate"],
            "weight_decay": train_config.get("weight_decay", 0.01),
            "adam_beta1": train_config.get("adam_beta1", 0.9),
            "adam_beta2": train_config.get("adam_beta2", 0.999),
            "adam_epsilon": train_config.get("adam_epsilon", 1e-8),
            "max_grad_norm": train_config.get("max_grad_norm", 1.0),
            "num_train_epochs": train_config["num_train_epochs"],
            "lr_scheduler_type": train_config.get("lr_scheduler_type", "linear"),
            "warmup_ratio": train_config.get("warmup_ratio", 0.1),
            "warmup_steps": train_config.get("warmup_steps", 0),
            "logging_dir": str(self.output_dir / "logs"),
            "logging_steps": train_config.get("logging_steps", 50),
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_rouge_combined_f1",
            "greater_is_better": True,
            "fp16": train_config.get("fp16", False),
            "fp16_opt_level": train_config.get("fp16_opt_level", "O1"),
            "dataloader_num_workers": train_config.get("dataloader_num_workers", 4),
            "remove_unused_columns": False,
            "seed": self.config["general"].get("seed", 42),
            "report_to": ["wandb"] if wandb.run else ["none"],
            "run_name": self.experiment_name if wandb.run else None,
            "push_to_hub": False,
            "predict_with_generate": True,
            "generation_max_length": self.config["generation"]["max_length"],
            "generation_num_beams": self.config["generation"]["num_beams"],
        }
        
        # save_stepsì™€ eval_steps ë™ê¸°í™” (load_best_model_at_end ì‚¬ìš© ì‹œ í•„ìˆ˜)
        if args_dict.get("load_best_model_at_end"):
            eval_steps = args_dict.get("eval_steps", 500)
            save_steps = args_dict.get("save_steps", 500)
            
            # save_stepsê°€ eval_stepsì˜ ë°°ìˆ˜ê°€ ì•„ë‹ˆë©´ ë™ê¸°í™”
            if save_steps % eval_steps != 0:
                args_dict["save_steps"] = eval_steps
                logger.warning(f"save_steps({save_steps})ê°€ eval_steps({eval_steps})ì˜ ë°°ìˆ˜ê°€ ì•„ë‹ˆì–´ì„œ {eval_steps}ë¡œ ì¡°ì •")
        
        # ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ íŠ¹í™” ì¸ì
        seq2seq_args = Seq2SeqTrainingArguments(**args_dict)

        return seq2seq_args

    def _save_results(self, result: TrainingResult) -> None:
        """ê²°ê³¼ ì €ì¥"""
        # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        results_dict = {
            "experiment_name": self.experiment_name,
            "model_architecture": self.config.get("model", {}).get("architecture", "unknown"),
            "model_checkpoint": self.config.get("model", {}).get(
                "checkpoint", self.config.get("general", {}).get("model_name", "")
            ),
            "best_metrics": result.best_metrics,
            "final_metrics": result.final_metrics,
            "model_path": result.model_path,
            "wandb_run_id": result.wandb_run_id,
            "experiment_id": result.experiment_id,
            "config": result.config_used,
            "timestamp": str(Path(result.model_path).parent.parent.name) if result.model_path else "unknown",
        }

        # JSON ì €ì¥
        results_file = self.results_dir / "training_results.json"
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)

        # ìš”ì•½ í…ìŠ¤íŠ¸ ì €ì¥
        summary_file = self.results_dir / "summary.txt"
        with open(summary_file, "w", encoding="utf-8") as f:
            f.write(f"Training Summary for {self.experiment_name}\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Model: {self.config.get('model', {}).get('architecture', 'unknown')} ({self.config.get('model', {}).get('checkpoint', 'unknown')})\n")
            f.write(f"Training Epochs: {self.config.get('training', {}).get('num_train_epochs', 'unknown')}\n")
            f.write(f"Batch Size: {self.config.get('training', {}).get('per_device_train_batch_size', 'unknown')}\n")
            f.write(f"Learning Rate: {self.config.get('training', {}).get('learning_rate', 'unknown')}\n\n")
            f.write("Best Metrics:\n")
            for metric, value in result.best_metrics.items():
                f.write(f"  {metric}: {value:.4f}\n")
            f.write("\nModel saved to: " + result.model_path + "\n")
            if result.wandb_run_id:
                f.write(f"WandB Run ID: {result.wandb_run_id}\n")

        logger.info(f"Results saved to {self.results_dir}")


def create_trainer(config: Union[str, Dict[str, Any]], sweep_mode: bool = False, one_epoch_mode: bool = False) -> DialogueSummarizationTrainer:
    """
    íŠ¸ë ˆì´ë„ˆ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Args:
        config: ì„¤ì • íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì„¤ì • ë”•ì…”ë„ˆë¦¬
        sweep_mode: WandB Sweep ëª¨ë“œ ì—¬ë¶€
        one_epoch_mode: 1ì—í¬í¬ ëª¨ë“œ ì—¬ë¶€ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
    
    Returns:
        ì´ˆê¸°í™”ëœ íŠ¸ë ˆì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤
    """
    # ì„¤ì • ë¡œë”©
    if isinstance(config, str):
        config_dict = load_config(config)
    else:
        config_dict = config
    
    # 1ì—í¬í¬ ëª¨ë“œ ì ìš©
    if one_epoch_mode:
        original_epochs = config_dict["training"].get("num_train_epochs", 3)
        config_dict["training"]["num_train_epochs"] = 1
        logger.info(f"ğŸš€ 1ì—í¬í¬ ëª¨ë“œ í™œì„±í™”: {original_epochs}ì—í¬í¬ â†’ 1ì—í¬í¬ë¡œ ë‹¨ì¶•")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = DialogueSummarizationTrainer(config=config_dict, sweep_mode=sweep_mode)

    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    trainer.initialize_components()

    return trainer


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš© ë©”ì¸ í•¨ìˆ˜
    import argparse

    parser = argparse.ArgumentParser(description="Train dialogue summarization model")
    parser.add_argument("--config", type=str, required=True, help="Config file path")
    parser.add_argument("--train-data", type=str, help="Train data path")
    parser.add_argument("--val-data", type=str, help="Validation data path")
    parser.add_argument("--test-data", type=str, help="Test data path")
    parser.add_argument("--sweep", action="store_true", help="Run in sweep mode")
    parser.add_argument("--one-epoch", action="store_true", help="Run only one epoch for quick testing")

    args = parser.parse_args()

    # WandB ì´ˆê¸°í™” (ë¹„ Sweep ëª¨ë“œ)
    if not args.sweep:
        # ê¸°ì¡´ ì„¸ì…˜ ì •ë¦¬
        if wandb.run is not None:
            wandb.finish()
    
        wandb.init(
            project="nlp-dialogue-summarization",
            name="manual_training",
            config={"manual_run": True},
            reinit=True,
            resume="never",
        )
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
    trainer = create_trainer(args.config, sweep_mode=args.sweep, one_epoch_mode=args.one_epoch)

    # ë°ì´í„° ì¤€ë¹„
    datasets = trainer.prepare_data(train_path=args.train_data, val_path=args.val_data, test_path=args.test_data)

    # í•™ìŠµ ì‹¤í–‰
    result = trainer.train(datasets)

    print(f"Training completed! Best ROUGE combined F1: {result.best_metrics.get('rouge_combined_f1', 0):.4f}")
    
