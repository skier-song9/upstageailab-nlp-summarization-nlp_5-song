# mT5 XL-Sum RTX 3090 μµμ ν™” μ‹¤ν— v2
experiment_name: mt5_xlsum_optimized_v2_rtx3090
description: "mT5 XL-Sum RTX 3090 24GB μµμ ν™” - 1λ‹¨κ³„ μ„±λ¥ λ³µμ›"

general:
  model_name: csebuetnlp/mT5_multilingual_XLSum
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: mt5_xlsum_optimized_v2_rtx3090

model:
  architecture: mt5
  checkpoint: csebuetnlp/mT5_multilingual_XLSum

# β… ν•κµ­μ–΄ νΉν™” prefix
input_prefix: "μ”μ•½: "

tokenizer:
  bos_token: <pad>
  eos_token: </s>
  encoder_max_len: 512
  decoder_max_len: 128
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'

# π€ RTX 3090 24GB μµμ ν™” ν•™μµ μ„¤μ •
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 200
  
  # π”¥ RTX 3090 μµμ  λ°°μΉ μ„¤μ •
  per_device_train_batch_size: 6       # RTX 3090 μµμ ν™”
  per_device_eval_batch_size: 12       # ν‰κ°€ μ‹ λ” ν° λ°°μΉ
  gradient_accumulation_steps: 3       # μ ν¨ λ°°μΉ 18
  
  # π― 1λ‹¨κ³„ ν•™μµ νλΌλ―Έν„°
  num_train_epochs: 4
  learning_rate: 6.0e-05               # RTX 3090 μ•μ •μ  ν•™μµλ¥ 
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.03
  weight_decay: 0.005
  
  # β΅ RTX 3090 μµμ ν™”
  fp16: true                           # RTX 3090 fp16 μ„±λ¥ μ°μ
  gradient_checkpointing: false
  dataloader_num_workers: 6
  dataloader_pin_memory: true
  group_by_length: true
  
  # π“ λ¨λ‹ν„°λ§
  logging_steps: 50
  save_strategy: steps
  save_steps: 200
  save_total_limit: 5
  load_best_model_at_end: true
  early_stopping_patience: 5
  
  # π― μƒμ„± μ„¤μ •
  predict_with_generate: true
  generation_num_beams: 6
  generation_max_length: 128
  generation_min_length: 10
  
  report_to: wandb
  seed: 42

# π€ RTX 3090 μµμ ν™” QLoRA (Unsloth ν™μ„±ν™”)
qlora:
  use_unsloth: true
  use_qlora: true
  lora_rank: 48                        # RTX 3090 λ©”λ¨λ¦¬ ν™μ©
  lora_alpha: 96                       # rankμ 2λ°°
  lora_dropout: 0.05
  target_modules: ["q", "k", "v", "o", "wi", "wo"]
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"   # RTX 3090 μ•μ •μ„±
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: mt5_xlsum_optimized_v2_rtx3090
  tags: [mT5, XL-Sum, RTX3090, optimized_v2, stage1]
  notes: "mT5 XL-Sum RTX 3090 μµμ ν™” 1λ‹¨κ³„ - μ„±λ¥ λ³µμ›"

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
