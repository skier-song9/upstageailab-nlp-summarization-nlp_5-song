# ê³ ì„±ëŠ¥ í•™ìŠµë¥  RTX 3090 24GB ìµœì í™” ì‹¤í—˜
experiment_name: high_learning_rate_rtx3090_optimized
description: "ê³ ì„±ëŠ¥ í•™ìŠµë¥  RTX 3090 ê·¹í•œ ìµœì í™” - ì„¸ê³„ ëŒ€íšŒ ìˆ˜ì¤€ ë„ì „"

general:
  model_name: digit82/kobart-summarization
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: high_learning_rate_rtx3090_optimized

model:
  architecture: bart
  checkpoint: digit82/kobart-summarization

# ğŸ¯ ê³ ì„±ëŠ¥ prefix
input_prefix: ""

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 1024              # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸
  decoder_max_len: 200               # ê¸´ ìš”ì•½ ì§€ì›
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'

# ğŸš€ RTX 3090 ê·¹í•œ ê³ ì„±ëŠ¥ í•™ìŠµë¥  ì‹¤í—˜
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 150
  
  # ğŸ”¥ ê³ ì„±ëŠ¥ ë°°ì¹˜ ì„¤ì •
  per_device_train_batch_size: 6      # ê¸´ ì‹œí€€ìŠ¤ ê³ ë ¤
  per_device_eval_batch_size: 12
  gradient_accumulation_steps: 4      # ìœ íš¨ ë°°ì¹˜ 24
  
  # âš¡ ê·¹í•œ ê³ ì„±ëŠ¥ í•™ìŠµë¥ 
  num_train_epochs: 5                 # ì¶©ë¶„í•œ í•™ìŠµ
  learning_rate: 8.0e-05              # ê·¹í•œ ê³ ì„±ëŠ¥ í•™ìŠµë¥ 
  lr_scheduler_type: cosine_with_restarts  # ê³ ì„±ëŠ¥ ìŠ¤ì¼€ì¤„ëŸ¬
  warmup_ratio: 0.02                  # ì§§ì€ ì›œì—…
  weight_decay: 0.005                 # ë‚®ì€ weight decay
  
  # ğŸ¯ RTX 3090 ê·¹í•œ ìµœì í™”
  fp16: false
  bf16: true                          # ë” ì•ˆì •ì ì¸ ì •ë°€ë„
  gradient_checkpointing: true
  dataloader_num_workers: 12          # ìµœëŒ€ ì›Œì»¤
  dataloader_pin_memory: true
  group_by_length: true
  
  # ğŸ“Š ê·¹ì„¸ë°€ ëª¨ë‹ˆí„°ë§
  logging_steps: 25
  save_strategy: steps
  save_steps: 150
  save_total_limit: 8
  load_best_model_at_end: true
  early_stopping_patience: 8
  
  # ğŸ† ê³ ì„±ëŠ¥ ìƒì„± ì„¤ì •
  predict_with_generate: true
  generation_num_beams: 10            # ê·¹í•œ ë¹” ì„œì¹˜
  generation_max_length: 200
  generation_min_length: 25
  generation_length_penalty: 0.6
  generation_no_repeat_ngram_size: 4
  generation_do_sample: false
  
  report_to: wandb
  seed: 42

# ğŸš€ ê·¹í•œ QLoRA ì„¤ì • (Unsloth í™œì„±í™”)
qlora:
  use_unsloth: true
  use_qlora: true
  lora_rank: 96                       # ê·¹í•œ í‘œí˜„ë ¥
  lora_alpha: 192
  lora_dropout: 0.05                  # ë‚®ì€ ë“œë¡­ì•„ì›ƒ
  target_modules: ["q_proj", "k_proj", "v_proj", "out_proj", "fc1", "fc2"]
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: high_learning_rate_rtx3090_extreme
  tags: [KoBART, RTX3090, extreme_lr, high_performance, korean]
  notes: "RTX 3090 ê·¹í•œ ê³ ì„±ëŠ¥ í•™ìŠµë¥  ì‹¤í—˜ - ì„¸ê³„ ëŒ€íšŒ ìˆ˜ì¤€ ë„ì „"

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
