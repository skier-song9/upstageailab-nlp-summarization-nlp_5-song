experiment_name: "03_high_learning_rate_safe"
base_config: "config.yaml"

general:
  experiment_name: "dialogue_summarization"
  model_name: "digit82/kobart-summarization"
  seed: 42
  device: "auto"

data:
  train_path: "data/train.csv"
  val_path: "data/dev.csv"
  test_path: "data/test.csv"
  # 특수 토큰 설정 - 안전 모드
  special_tokens:
    - "#PhoneNumber#"
    - "#Address#"
    - "#PassportNumber#"

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 128

training:
  output_dir: "outputs/"
  num_train_epochs: 3
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  warmup_ratio: 0.1
  learning_rate: 5.0e-05  # 높은 학습률
  weight_decay: 0.01
  gradient_checkpointing: true
  fp16: true
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "rouge_combined_f1"
  logging_steps: 100

generation:
  max_length: 200
  min_length: 30
  num_beams: 4
  length_penalty: 1.0
  no_repeat_ngram_size: 3

evaluation:
  rouge_types: ["rouge1", "rouge2", "rougeL", "rougeLsum"]
  rouge_tokenize_korean: true

callbacks:
  early_stopping:
    enabled: true
    patience: 3

wandb:
  enabled: true
  entity: "lyjune37-juneictlab"
  project: "nlp-5"
  name: "high_lr_kobart_safe"
  tags: ["high_lr", "kobart", "safe_mode"]

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
