experiment_name: backtranslation_augmentation
description: 백트랜슬레이션을 통한 고급 데이터 증강 실험

# 모델 구성
model:
  name: digit82/kobart-summarization
  architecture: kobart

# 토크나이저 구성  
tokenizer:
  encoder_max_len: 512
  decoder_max_len: 200
  bos_token: "<s>"
  eos_token: "</s>"
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#DateOfBirth#'
    - '#PassportNumber#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'

# 학습 구성
training:
  num_train_epochs: 20
  learning_rate: 2.0e-05
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 4
  warmup_steps: 500
  weight_decay: 0.01
  seed: 42
  evaluation_strategy: epoch
  save_strategy: epoch
  save_total_limit: 3
  logging_steps: 50
  load_best_model_at_end: true
  metric_for_best_model: "eval_rouge_combined_f1"
  greater_is_better: true
  fp16: true
  gradient_checkpointing: true
  
  # 조기 종료
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  
  # 데이터로더 설정
  dataloader_num_workers: 4
  dataloader_drop_last: false
  group_by_length: true
  predict_with_generate: true
  remove_unused_columns: true

# 백트랜슬레이션 데이터 증강
data_augmentation:
  enabled: true
  augmentation_type: "backtranslation"
  
  # 백트랜슬레이션 설정
  backtranslation:
    # 증강기 타입: "single" 또는 "multilingual"
    augmenter_type: "single"
    
    # 번역 방법: "google", "marian", "both"
    method: "google"
    
    # 언어 설정
    source_lang: "ko"
    intermediate_lang: "en"
    
    # 다중 언어 백트랜슬레이션 (augmenter_type이 "multilingual"일 때)
    intermediate_langs: ["en", "ja", "zh-cn", "es"]
    
    # 품질 설정
    quality_threshold: 0.3      # 최소 차이 (너무 다르면 제외)
    max_similarity: 0.9         # 최대 유사도 (너무 비슷하면 제외)
    
    # 증강 비율
    augmentation_ratio: 0.5     # 원본 데이터의 50% 증강
    num_augmentations_per_sample: 2  # 각 샘플당 생성할 증강 수
    
    # 캐싱 및 성능
    cache_dir: "./cache/backtranslation"
    batch_size: 32
    rate_limit_delay: 0.1       # API 호출 간 지연 (초)
    
    # 필터링
    filter_settings:
      min_length_ratio: 0.5     # 원본 대비 최소 길이
      max_length_ratio: 2.0     # 원본 대비 최대 길이
      preserve_special_tokens: true
      check_token_consistency: true
  
  # 기존 간단한 증강과 조합 (선택적)
  combine_with_simple: true
  simple_augmentation:
    - type: "SynonymReplacement"
      params:
        replace_ratio: 0.1
        preserve_special_tokens: true
    - type: "SentenceReorder"
      params:
        reorder_ratio: 0.1
        preserve_speaker_order: true

# 데이터 전처리
preprocessing:
  # 백트랜슬레이션 전 정규화
  normalize_before_translation: true
  normalization:
    normalize_spacing: true
    normalize_punctuation: true
    expand_abbreviations: false  # 번역 시 혼란 방지
    reduce_repetition: true

# 생성 구성
generation:
  max_length: 200
  min_length: 30
  num_beams: 4
  no_repeat_ngram_size: 3
  early_stopping: true
  length_penalty: 1.2
  temperature: 1.0
  do_sample: false

# 추론 구성
inference:
  batch_size: 16
  generate_max_length: 200
  remove_tokens:
    - '<usr>'
    - '<s>'
    - '</s>'
    - '<pad>'

# 평가 메트릭
evaluation:
  metrics:
    - rouge
    - diversity_metrics  # 증강 데이터의 다양성 측정
    - semantic_similarity  # 원본과의 의미 유사도
  
  augmentation_quality:
    sample_evaluation: true
    num_samples_to_evaluate: 100
    human_eval_required: false

# WandB 구성
wandb:
  name: "09_backtranslation_augmentation"
  notes: "백트랜슬레이션을 통한 고급 데이터 증강 - 한국어-영어-한국어"
  tags: ["data_augmentation", "backtranslation", "advanced", "kobart"]
  
  # 추가 로깅
  log_augmentation_samples: true
  log_translation_quality: true
  log_diversity_metrics: true

# 데이터 구성
data:
  train_file: "train.csv"
  dev_file: "dev.csv"
  test_file: "test.csv"
  
  # 증강된 데이터 저장
  save_augmented_data: true
  augmented_train_file: "train_backtranslated.csv"

# 모니터링 설정
monitoring:
  log_memory_usage: true
  log_training_speed: true
  log_augmentation_stats: true
  
  # 백트랜슬레이션 모니터링
  track_translation_quality: true
  track_api_usage: true
  track_cache_efficiency: true

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
