# eenzeenee T5 RTX 3090 24GB ìµœì í™” ì‹¤í—˜
experiment_name: eenzeenee_t5_rtx3090_optimized
description: "eenzeenee T5 í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸ RTX 3090 ìµœê³  ì„±ëŠ¥ ìµœì í™”"

general:
  model_name: eenzeenee/t5-base-korean-summarization
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: eenzeenee_t5_rtx3090_optimized

model:
  architecture: t5
  checkpoint: eenzeenee/t5-base-korean-summarization

# ğŸ¯ T5 í•œêµ­ì–´ íŠ¹í™” prefix
input_prefix: "summarize: "

tokenizer:
  bos_token: <pad>
  eos_token: </s>
  encoder_max_len: 768               # ê¸´ ëŒ€í™” ì²˜ë¦¬ ê°€ëŠ¥
  decoder_max_len: 150               # ì¶©ë¶„í•œ ìš”ì•½ ê¸¸ì´
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'

# ğŸš€ RTX 3090 ìµœì í™” í•™ìŠµ ì„¤ì •
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 200
  
  # ğŸ”¥ RTX 3090 T5-base ìµœì  ë°°ì¹˜
  per_device_train_batch_size: 12     # T5-baseëŠ” ë” í° ë°°ì¹˜ ê°€ëŠ¥
  per_device_eval_batch_size: 24      # í‰ê°€ì‹œ ë” í° ë°°ì¹˜
  gradient_accumulation_steps: 2      # ìœ íš¨ ë°°ì¹˜ 24
  
  # ğŸ¯ T5 í•œêµ­ì–´ ìµœì í™” í•™ìŠµë¥ 
  num_train_epochs: 4                 # ì¶©ë¶„í•œ í•™ìŠµ
  learning_rate: 5.0e-05              # T5-base ìµœì  í•™ìŠµë¥ 
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.05
  weight_decay: 0.01
  
  # âš¡ RTX 3090 ê°€ì† ìµœì í™”
  fp16: true                          # RTX 3090 fp16 ìš°ìˆ˜
  gradient_checkpointing: false       # T5-baseëŠ” ë©”ëª¨ë¦¬ ì—¬ìœ 
  dataloader_num_workers: 8           # RTX 3090 ìµœì  ì›Œì»¤
  dataloader_pin_memory: true
  group_by_length: true
  
  # ğŸ“Š ëª¨ë‹ˆí„°ë§
  logging_steps: 50
  save_strategy: steps
  save_steps: 200
  save_total_limit: 5
  load_best_model_at_end: true
  early_stopping_patience: 5
  
  # ğŸ¯ T5 í•œêµ­ì–´ ìµœì  ìƒì„± ì„¤ì •
  predict_with_generate: true
  generation_num_beams: 6             # HuggingFace ê¶Œì¥ë³´ë‹¤ ë†’ê²Œ
  generation_max_length: 150
  generation_min_length: 20
  generation_length_penalty: 0.8
  generation_no_repeat_ngram_size: 3
  generation_do_sample: false         # ê²°ì •ì  ìƒì„±
  
  report_to: wandb
  seed: 42

# ğŸš€ T5 ê³ ì„±ëŠ¥ QLoRA (Unsloth í™œì„±í™”)
qlora:
  use_unsloth: true
  use_qlora: true                    # T5-baseëŠ” Full Fine-tuning ê°€ëŠ¥
  # í•„ìš”ì‹œ QLoRA ì„¤ì •
  lora_rank: 64
  lora_alpha: 128
  lora_dropout: 0.1
  target_modules: ["q", "k", "v", "o"]
  load_in_4bit: false
  load_in_8bit: false

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: eenzeenee_t5_rtx3090_optimized
  tags: [eenzeenee, T5-base, RTX3090, optimized, korean, full_finetune]
  notes: "eenzeenee T5 RTX 3090 ìµœê³  ì„±ëŠ¥ ìµœì í™” - Full Fine-tuning"

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
