# KoBART RTX 3090 24GB ìµœì í™” ì‹¤í—˜
experiment_name: kobart_baseline_rtx3090_optimized
description: "KoBART ë² ì´ìŠ¤ë¼ì¸ RTX 3090 ìµœê³  ì„±ëŠ¥ ìµœì í™”"

general:
  model_name: digit82/kobart-summarization
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: kobart_baseline_rtx3090_optimized

model:
  architecture: bart
  checkpoint: digit82/kobart-summarization

# ğŸ¯ KoBARTìš© prefix (ë¹ˆ ë¬¸ìì—´)
input_prefix: ""

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 768               # ê¸´ ëŒ€í™” ì²˜ë¦¬
  decoder_max_len: 150               # ì¶©ë¶„í•œ ìš”ì•½ ê¸¸ì´
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'

# ğŸš€ RTX 3090 KoBART ìµœì í™”
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 200
  
  # ğŸ”¥ RTX 3090 KoBART ìµœì  ë°°ì¹˜ (BARTëŠ” ë©”ëª¨ë¦¬ ë” ë§ì´ ì‚¬ìš©)
  per_device_train_batch_size: 8      # BART ìµœì  ë°°ì¹˜
  per_device_eval_batch_size: 16      # í‰ê°€ì‹œ ë” í° ë°°ì¹˜
  gradient_accumulation_steps: 3      # ìœ íš¨ ë°°ì¹˜ 24
  
  # ğŸ¯ KoBART ìµœì í™” í•™ìŠµë¥ 
  num_train_epochs: 4
  learning_rate: 3.0e-05              # KoBART ì•ˆì •ì  í•™ìŠµë¥ 
  lr_scheduler_type: linear
  warmup_ratio: 0.1                   # BARTëŠ” ê¸´ ì›œì—… ì„ í˜¸
  weight_decay: 0.01
  
  # âš¡ RTX 3090 ìµœì í™”
  fp16: true
  gradient_checkpointing: true        # BART ë©”ëª¨ë¦¬ ì ˆì•½
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  group_by_length: true
  
  # ğŸ“Š ëª¨ë‹ˆí„°ë§
  logging_steps: 50
  save_strategy: steps
  save_steps: 200
  save_total_limit: 5
  load_best_model_at_end: true
  early_stopping_patience: 5
  
  # ğŸ¯ KoBART ìµœì  ìƒì„± ì„¤ì •
  predict_with_generate: true
  generation_num_beams: 8             # KoBART ê³ í’ˆì§ˆ ìƒì„±
  generation_max_length: 150
  generation_min_length: 20
  generation_length_penalty: 1.0
  generation_no_repeat_ngram_size: 3
  generation_do_sample: false
  
  report_to: wandb
  seed: 42

# ğŸš€ KoBART QLoRA ìµœì í™” (Unsloth í™œì„±í™”)
qlora:
  use_unsloth: true
  use_qlora: true                     # BARTëŠ” QLoRA íš¨ê³¼ì 
  lora_rank: 64                       # ë†’ì€ í‘œí˜„ë ¥
  lora_alpha: 128
  lora_dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "out_proj"]  # BART ëª¨ë“ˆ
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: kobart_baseline_rtx3090_optimized
  tags: [KoBART, RTX3090, optimized, korean, qlora]
  notes: "KoBART RTX 3090 ìµœê³  ì„±ëŠ¥ ìµœì í™” - QLoRA"

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
