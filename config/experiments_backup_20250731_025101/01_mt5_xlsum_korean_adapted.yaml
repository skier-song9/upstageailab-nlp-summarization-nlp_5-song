# mT5 XL-Sum RTX 3090 í•œêµ­ì–´ ë„ë©”ì¸ ì ì‘ ì‹¤í—˜
experiment_name: mt5_xlsum_korean_adapted_rtx3090
description: "mT5 XL-Sum RTX 3090 í•œêµ­ì–´ ëŒ€í™” íŠ¹í™” ë„ë©”ì¸ ì ì‘ - 2ë‹¨ê³„"

general:
  model_name: csebuetnlp/mT5_multilingual_XLSum
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: mt5_xlsum_korean_adapted_rtx3090

model:
  architecture: mt5
  checkpoint: csebuetnlp/mT5_multilingual_XLSum

# ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë„ë©”ì¸ íŠ¹í™” prefix
input_prefix: "í•œêµ­ì–´ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”: "

tokenizer:
  bos_token: <pad>
  eos_token: </s>
  encoder_max_len: 768               # ê¸´ ëŒ€í™” ì²˜ë¦¬
  decoder_max_len: 150
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'
    - '<ëŒ€í™”ì‹œì‘>'
    - '<ëŒ€í™”ì¢…ë£Œ>'
    - '<ì¤‘ìš”>'
    - '<ê²°ë¡ >'

# ğŸš€ RTX 3090 ë„ë©”ì¸ ì ì‘ ìµœì í™”
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 150
  
  # ğŸ“ˆ RTX 3090 ê³ ì„±ëŠ¥ ë°°ì¹˜ ì„¤ì •
  per_device_train_batch_size: 5      # ê¸´ ì‹œí€€ìŠ¤ ê³ ë ¤ ì¡°ì •
  per_device_eval_batch_size: 10
  gradient_accumulation_steps: 4      # ìœ íš¨ ë°°ì¹˜ 20
  
  # ğŸ¯ ë„ë©”ì¸ ì ì‘ íŠ¹í™” í•™ìŠµë¥ 
  num_train_epochs: 6                 # ì¶©ë¶„í•œ ë„ë©”ì¸ ì ì‘
  learning_rate: 9.0e-05              # 2ë‹¨ê³„ ë” ë†’ì€ í•™ìŠµë¥ 
  lr_scheduler_type: polynomial
  warmup_ratio: 0.05
  weight_decay: 0.001
  
  # âš¡ RTX 3090 í˜¼í•© ì •ë°€ë„ ìµœì í™”
  fp16: false
  bf16: true                          # ë” ì•ˆì •ì 
  gradient_checkpointing: false
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  group_by_length: true
  
  # ğŸ“Š ì„¸ë°€í•œ ëª¨ë‹ˆí„°ë§
  logging_steps: 25
  save_strategy: steps
  save_steps: 150
  save_total_limit: 8
  load_best_model_at_end: true
  early_stopping_patience: 8
  
  # ğŸ¯ í•œêµ­ì–´ íŠ¹í™” ìƒì„±
  predict_with_generate: true
  generation_num_beams: 8
  generation_max_length: 150
  generation_min_length: 15
  generation_length_penalty: 0.8
  generation_no_repeat_ngram_size: 3
  
  report_to: wandb
  seed: 42

# ğŸš€ RTX 3090 ê³ ê¸‰ QLoRA
qlora:
  use_qlora: true
  lora_rank: 64                       # ë” ë†’ì€ í‘œí˜„ë ¥
  lora_alpha: 128
  lora_dropout: 0.03
  target_modules: ["q", "k", "v", "o", "wi", "wo", "lm_head"]
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: mt5_xlsum_korean_adapted_rtx3090
  tags: [mT5, XL-Sum, RTX3090, korean_adapted, stage2]
  notes: "mT5 XL-Sum RTX 3090 í•œêµ­ì–´ ë„ë©”ì¸ ì ì‘ 2ë‹¨ê³„"

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
