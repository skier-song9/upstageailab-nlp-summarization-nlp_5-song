experiment_name: baseline_mini_test
description: Mini baseline test for quick validation (3 epochs)

# Model configuration
model:
  name: digit82/kobart-summarization
  architecture: kobart

# Tokenizer configuration  
tokenizer:
  encoder_max_len: 512
  decoder_max_len: 200
  bos_token: "<s>"
  eos_token: "</s>"
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#DateOfBirth#'
    - '#PassportNumber#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'

# Training configuration - Mini version for testing
training:
  num_train_epochs: 3  # Reduced for quick test
  learning_rate: 1.0e-05
  per_device_train_batch_size: 8   # Small batch for testing
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  fp16: true
  warmup_steps: 10
  weight_decay: 0.01
  seed: 42
  
  # Evaluation settings
  evaluation_strategy: epoch  # Changed to epoch for quick feedback
  save_strategy: epoch
  save_total_limit: 1
  logging_steps: 50
  
  # Early stopping disabled for mini test
  load_best_model_at_end: true
  
  # Other training settings
  dataloader_num_workers: 2
  dataloader_drop_last: false
  group_by_length: true
  predict_with_generate: true
  remove_unused_columns: true

# Generation configuration
generation:
  max_length: 100  # Reduced for speed
  num_beams: 2     # Reduced for speed
  no_repeat_ngram_size: 2
  early_stopping: true

# Inference configuration
inference:
  batch_size: 8
  generate_max_length: 100
  remove_tokens:
    - '<usr>'
    - '<s>'
    - '</s>'
    - '<pad>'

# WandB configuration
wandb:
  name: "00_baseline_mini_test"
  notes: "Quick 3-epoch test to validate pipeline"
  tags: ["baseline", "test", "mini", "kobart"]
  
# Data configuration override
data:
  train_file: "train.csv"
  dev_file: "dev.csv"
  test_file: "test.csv"

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
