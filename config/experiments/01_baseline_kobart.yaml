# 베이스라인 KoBART 실험
experiment_name: baseline_kobart
description: "KoBART 베이스라인 재현 실험"

general:
  model_name: digit82/kobart-summarization
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  name: baseline_kobart

model:
  architecture: bart
  checkpoint: digit82/kobart-summarization

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 512
  decoder_max_len: 100

training:
  do_eval: true
  do_train: true
  evaluation_strategy: epoch
  num_train_epochs: 5
  per_device_train_batch_size: 20     # RTX 3090 24GB KoBART 최적화
  per_device_eval_batch_size: 40      # 작은 모델이므로 대용량 가능
  learning_rate: 2.0e-05              # 조금 더 높은 학습률
  lr_scheduler_type: cosine            # 코사인 스케줄러
  warmup_ratio: 0.1                    # 웜업 비율
  weight_decay: 0.01                   # 가중치 감소
  
  # RTX 3090 최적화 설정
  gradient_checkpointing: false        # 메모리 여유로우므로 비활성화
  dataloader_num_workers: 12           # 48코어 활용
  dataloader_pin_memory: true          # 251GB RAM 활용
  group_by_length: true                # 효율성 증대
  fp16: true
  logging_steps: 100
  save_strategy: epoch
  save_total_limit: 3
  load_best_model_at_end: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: baseline_kobart_rtx3090_lyj
  tags: [baseline, kobart, RTX3090, optimized, lyj]
  notes: "KoBART 베이스라인 RTX 3090 최적화"

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
