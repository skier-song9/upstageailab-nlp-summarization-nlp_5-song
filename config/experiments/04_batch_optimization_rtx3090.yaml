# ë°°ì¹˜ ìµœì í™” RTX 3090 ê·¹í•œ ìµœì í™” ì‹¤í—˜
experiment_name: batch_optimization_extreme_rtx3090
description: "ë°°ì¹˜ ìµœì í™” RTX 3090 + Unsloth ê·¹í•œ ìµœì í™” - ë°°ì¹˜ 16 + ìœ íš¨ë°°ì¹˜ 64"

general:
  model_name: digit82/kobart-summarization
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: batch_optimization_extreme_rtx3090

model:
  architecture: bart
  checkpoint: digit82/kobart-summarization

# ğŸ”¥ ë°°ì¹˜ ìµœì í™” íŠ¹í™” prefix
input_prefix: ""

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 1200              # ë°°ì¹˜ ìµœì í™” 1024â†’1200
  decoder_max_len: 220               # 200â†’220
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'
    - '<summary>'
    - '</summary>'
    - '<dialogue>'
    - '</dialogue>'

# ğŸ”¥ RTX 3090 + Unsloth ë°°ì¹˜ ê·¹í•œ ìµœì í™”
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 120
  
  # ğŸ”¥ RTX 3090 ë°°ì¹˜ ê·¹í•œ ìµœì í™” (ìœ íš¨ ë°°ì¹˜ 64 ë‹¬ì„±)
  per_device_train_batch_size: 16     # ê·¹í•œ ìµœì í™” 4â†’16
  per_device_eval_batch_size: 24      # í‰ê°€ì‹œ ë” í° ë°°ì¹˜
  gradient_accumulation_steps: 4      # ìœ íš¨ ë°°ì¹˜ 64 ë‹¬ì„±
  
  # ğŸ¯ ë°°ì¹˜ ìµœì í™” íŠ¹í™” í•™ìŠµ íŒŒë¼ë¯¸í„°
  num_train_epochs: 5
  learning_rate: 7.0e-05              # ë°°ì¹˜ ìµœì í™”ì— ë§ëŠ” í•™ìŠµë¥ 
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.02
  weight_decay: 0.001
  
  # ğŸ”¥ RTX 3090 + CUDA 12.2 + Unsloth ê·¹í•œ ìµœì í™”
  fp16: false
  bf16: true
  tf32: true
  gradient_checkpointing: false
  dataloader_num_workers: 36          # ë°°ì¹˜ ìµœì í™”ì— ì¤‘ìš”í•œ ì›Œì»¤ ìˆ˜
  dataloader_pin_memory: true
  dataloader_persistent_workers: true
  group_by_length: true               # ë°°ì¹˜ ìµœì í™”ì— ì¤‘ìš”
  remove_unused_columns: false
  
  # ğŸ”¥ Unsloth íŠ¹í™” ì˜µí‹°ë§ˆì´ì €
  optim: adamw_torch
  adam_beta1: 0.9
  adam_beta2: 0.95
  max_grad_norm: 0.3
  
  # ğŸ“Š ë°°ì¹˜ ìµœì í™” ëª¨ë‹ˆí„°ë§
  logging_steps: 30
  save_strategy: steps
  save_steps: 120
  save_total_limit: 10
  load_best_model_at_end: true
  early_stopping_patience: 12
  
  # ğŸ† ë°°ì¹˜ ìµœì í™” ìƒì„±
  predict_with_generate: true
  generation_num_beams: 10
  generation_max_length: 220
  generation_min_length: 28
  generation_length_penalty: 0.75
  generation_no_repeat_ngram_size: 3
  generation_do_sample: false
  generation_early_stopping: true
  
  report_to: wandb
  seed: 42

# ğŸ”¥ ë°°ì¹˜ ìµœì í™” + Unsloth QLoRA
qlora:
  use_unsloth: true
  use_qlora: true
  lora_rank: 144
  lora_alpha: 288
  lora_dropout: 0.02
  target_modules: ["q", "k", "v", "o", "fc1", "fc2", "lm_head", "embed_tokens"]
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: batch_optimization_extreme_rtx3090
  tags: [KoBART, RTX3090, extreme, batch_opt, Unsloth, batch16, effective64]
  notes: "ë°°ì¹˜ ìµœì í™” RTX 3090 + Unsloth ê·¹í•œ ìµœì í™” - ë°°ì¹˜ 16, ìœ íš¨ë°°ì¹˜ 64, LoRA 144"

# ğŸ† ë°°ì¹˜ ìµœì í™” ìƒì„± ì„¤ì •
generation:
  max_length: 220
  min_length: 28
  num_beams: 10
  length_penalty: 0.75
  no_repeat_ngram_size: 3
  early_stopping: true
  do_sample: false
