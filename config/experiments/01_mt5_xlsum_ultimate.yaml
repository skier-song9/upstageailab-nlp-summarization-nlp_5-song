# mT5 XL-Sum RTX 3090 ê·¹í•œ ìµœì í™” ì‹¤í—˜ 3ë‹¨ê³„ (ì„¸ê³„ ëŒ€íšŒ ë„ì „)
experiment_name: mt5_xlsum_extreme_stage3_ultimate
description: "mT5 XL-Sum RTX 3090 + Unsloth ê·¹í•œ ìµœì í™” 3ë‹¨ê³„ - ë°°ì¹˜ 12 + ì‹œí€€ìŠ¤ 1536 + LoRA 256 ì„¸ê³„ëŒ€íšŒ ë„ì „"

general:
  model_name: csebuetnlp/mT5_multilingual_XLSum
  data_path: data/
  train_path: data/train.csv
  val_path: data/dev.csv
  model_type: seq2seq
  name: mt5_xlsum_extreme_stage3_ultimate

model:
  architecture: mt5
  checkpoint: csebuetnlp/mT5_multilingual_XLSum

# ğŸ† ì„¸ê³„ ëŒ€íšŒ ìˆ˜ì¤€ prefix
input_prefix: "dialogue summarization in korean: "

tokenizer:
  bos_token: <pad>
  eos_token: </s>
  encoder_max_len: 1024              # RTX 3090 ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸
  decoder_max_len: 200
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'
    - '#DateOfBirth#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'
    - '<summary>'
    - '</summary>'
    - '<dialogue>'
    - '</dialogue>'

# ğŸš€ RTX 3090 ê·¹í•œ ìµœì í™” (ì„¸ê³„ ëŒ€íšŒ ìˆ˜ì¤€)
training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 100
  
  # ğŸ”¥ RTX 3090 ê·¹í•œ ìµœì í™” 3ë‹¨ê³„ ë°°ì¹˜ ì„¤ì •
  per_device_train_batch_size: 12     # ê·¹í•œ ìµœì í™” ìµœëŒ€ ë°°ì¹˜
  per_device_eval_batch_size: 24      # í‰ê°€ì‹œ ìµœëŒ€ ë°°ì¹˜
  gradient_accumulation_steps: 4      # ìœ íš¨ ë°°ì¹˜ 48
  
  # âš¡ ê·¹í•œ í•™ìŠµë¥  (ì„¸ê³„ ëŒ€íšŒ ìˆ˜ì¤€)
  num_train_epochs: 8                 # ì¶©ë¶„í•œ í•™ìŠµ
  learning_rate: 1.2e-04              # ê·¹í•œ í•™ìŠµë¥  (RTX 3090 ì•ˆì •ì„ )
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.02
  weight_decay: 0.0001
  
  # ğŸ¯ RTX 3090 ìµœê³  ì •ë°€ë„
  fp16: false
  bf16: true                          # ìµœê³  ì•ˆì •ì„±
  gradient_checkpointing: false
  dataloader_num_workers: 36          # 48ì½”ì–´ì˜ 75% ê·¹í•œ í™œìš©
  dataloader_pin_memory: true
  group_by_length: true
  
  # ğŸ“Š ê·¹ì„¸ë°€ ëª¨ë‹ˆí„°ë§
  logging_steps: 10
  save_strategy: steps
  save_steps: 100
  save_total_limit: 12
  load_best_model_at_end: true
  early_stopping_patience: 12
  
  # ğŸ† ì„¸ê³„ ëŒ€íšŒ ìˆ˜ì¤€ ìƒì„±
  predict_with_generate: true
  generation_num_beams: 16            # RTX 3090 ê·¹í•œ ìµœì í™”ë¡œ 16ê°œ ë¹” ì‚¬ìš©
  generation_max_length: 200
  generation_min_length: 20
  generation_length_penalty: 0.6
  generation_no_repeat_ngram_size: 4
  generation_do_sample: false
  
  report_to: wandb
  seed: 42

# ğŸ”¥ RTX 3090 + Unsloth ê·¹í•œ ìµœì í™” 3ë‹¨ê³„ QLoRA (ìµœëŒ€ í‘œí˜„ë ¥)
qlora:
  use_unsloth: true
  use_qlora: true
  lora_rank: 256                      # RTX 3090 ê·¹í•œ ìµœì í™” ìµœëŒ€ rank
  lora_alpha: 512                     # rankì˜ 2ë°°
  lora_dropout: 0.01                  # ìµœì†Œ ë“œë¡­ì•„ì›ƒ
  target_modules: ["q", "k", "v", "o", "wi", "wo", "lm_head", "embed_tokens"]
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: mt5_xlsum_ultimate_rtx3090_challenge
  tags: [mT5, XL-Sum, RTX3090, ultimate, world_competition_level, stage3]
  notes: "mT5 XL-Sum RTX 3090 ì„¸ê³„ ëŒ€íšŒ ìˆ˜ì¤€ ë„ì „ - ê·¹í•œ ìµœì í™”"

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
