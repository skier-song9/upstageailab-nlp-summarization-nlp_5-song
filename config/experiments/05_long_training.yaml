# 긴 학습 실험 (고성능 추구)
experiment_name: long_training
description: "긴 학습으로 최고 성능 추구 실험"

general:
  model_name: digit82/kobart-summarization
  data_path: ../data/
  train_path: data/train.csv
  val_path: data/dev.csv
  name: long_training_experiment

tokenizer:
  bos_token: <s>
  eos_token: </s>
  encoder_max_len: 512
  decoder_max_len: 100

training:
  do_eval: true
  do_train: true
  evaluation_strategy: steps
  eval_steps: 300
  num_train_epochs: 10    # 긴 학습
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  learning_rate: 8.0e-06  # 낮은 학습률로 안정적 학습
  fp16: true
  lr_scheduler_type: cosine  # 코사인 스케줄러
  warmup_ratio: 0.1
  weight_decay: 0.01
  logging_steps: 100
  save_strategy: steps
  save_steps: 300
  save_total_limit: 5
  load_best_model_at_end: true
  early_stopping_patience: 5  # 긴 patience

wandb:
  entity: lyjune37-juneictlab
  project: nlp-5
  name: long_training_lyj
  tags: [long_training, high_performance, cosine_scheduler, lyj]

# Generation settings
generation:
  max_length: 256
  min_length: 5
  num_beams: 4
  no_repeat_ngram_size: 2
\ntraining:\n  eval_strategy: no
