general:
  data_path: ../data/
  model_name: digit82/kobart-summarization
  name: eenzeenee_korean_summarization

# eenzeenee T5 한국어 요약 모델 설정
eenzeenee:
  general:
    model_name: eenzeenee/t5-base-korean-summarization
    data_path: ../data/
    output_dir: ./eenzeenee_outputs/
    model_type: seq2seq  # AutoModelForSeq2SeqLM 자동 선택
    name: eenzeenee_korean_summarization
  input_prefix: "summarize: "  # T5 모델용 prefix
  # T5 기반 토크나이저 설정
  tokenizer:
    bos_token: <pad>  # T5 기본 BOS 토큰
    eos_token: </s>   # T5 기본 EOS 토큰
    encoder_max_len: 512   # 입력 최대 길이
    decoder_max_len: 64   # 한국어 요약 최적 길이 (모델 카드 기준)
    special_tokens:
      - '#Person1#'
      - '#Person2#'
      - '#Person3#'
  
  # eenzeenee 모델 최적화 추론 설정
  inference:
    batch_size: 8  # T5-base 크기에 적합한 배치 사이즈
    generate_max_length: 64  # 한국어 요약 최적 길이 (모델 카드 권장)
    tags: [eenzeenee, T5-base, Korean]
    no_repeat_ngram_size: 2  # 반복 방지
    early_stopping: true
    length_penalty: 1.0      # 길이 패널티
    do_sample: false         # 결정론적 생성
    remove_tokens:
      - <pad>
      - <unk>
      - </s>
    result_path: ./eenzeenee_predictions/
  
  # eenzeenee용 QLoRA 설정 (T5 아키텍처에 최적화)
  qlora:
    use_unsloth: true  # AIStages RTX 3090 환경에서 활성화
    use_qlora: true
    
    # LoRA 설정 (T5에 최적화)
    name: xlsum_mt5_korean_summarization
    lora_rank: 16
    lora_alpha: 32
    lora_dropout: 0.1
    # T5 아키텍처용 타겟 모듈
    target_modules:
      - "q"      # T5는 단순한 모듈명 사용
      - "k"
      - "v" 
      - "o"      # output projection
    
    # 4-bit 양자화 설정
    load_in_4bit: true
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true
  
  # eenzeenee 최적화 학습 설정
  training:
    do_eval: true
    do_train: true
    
    # 평가 전략
    eval_strategy: steps
    eval_steps: 400  # 적당한 평가 빈도
    
    # Early Stopping
    early_stopping_patience: 3  # 한국어 요약에 적합한 patience
    early_stopping_threshold: 0.001
    
    # 메모리 최적화
    gradient_checkpointing: true
    gradient_checkpointing_kwargs:
      use_reentrant: false
    torch_empty_cache_steps: 10  # 메모리 정리
    
    # 성능 최적화
    group_by_length: true
    dataloader_num_workers: 4
    
    # 학습 파라미터
    fp16: true
    generation_max_length: 200  # 한국어 요약 길이
    gradient_accumulation_steps: 1
    learning_rate: 3.0e-05  # T5-base에 적합한 학습률
    load_best_model_at_end: true
    
    # 로깅 설정
    logging_dir: ./eenzeenee_logs
    logging_strategy: steps
    logging_steps: 100
    
    # 옵티마이저
    lr_scheduler_type: cosine
    optim: adamw_torch
    warmup_ratio: 0.1
    weight_decay: 0.01
    
    # 모델 저장 설정
    num_train_epochs: 5  # 한국어 요약에 적합한 epoch 수
    overwrite_output_dir: true
    save_strategy: steps
    save_steps: 400
    save_total_limit: 3
    
    # 배치 사이즈
    per_device_eval_batch_size: 4
    tags: [mT5, XL-Sum, Korean, Large-Model]
    
    # 생성 설정
    predict_with_generate: true
    generation_num_beams: 4
    
    # 기타 설정
    report_to: wandb
    seed: 42
  
  # eenzeenee용 WandB 설정 (팀 공유 계정)
  wandb:
    entity: lyjune37-juneictlab  # 접근 가능한 팀 entity
    project: nlp-5     # 프로젝트명 (nlp-sum-song과 동일)
    name: eenzeenee_korean_summarization_lyj  # 개별 실험명 (구분용)
    tags:
      - eenzeenee
      - T5-base
      - Korean
      - Summarization
      - lyj  # 개인 구분용 태그
    notes: "eenzeenee T5 한국어 요약 모델 실험 (lyj)"

# mT5 multilingual XL-Sum 모델 설정
xlsum_mt5:
  general:
    model_name: csebuetnlp/mT5_multilingual_XLSum
    data_path: ../data/
    output_dir: ./xlsum_mt5_outputs/
    model_type: seq2seq
    input_prefix: "summarize: "  # mT5도 prefix 사용 (빈 요약 방지)
  
  # mT5 토크나이저 설정
  tokenizer:
    bos_token: <pad>  # mT5 기본 BOS 토큰
    eos_token: </s>   # mT5 기본 EOS 토큰
    encoder_max_len: 512   # Hugging Face 공식 권장
    decoder_max_len: 84    # mT5 XL-Sum 최적값
    special_tokens:
      - '#Person1#'
      - '#Person2#'
      - '#Person3#'
      - '#PhoneNumber#'
      - '#Address#'
      - '#PassportNumber#'
      - '#DateOfBirth#'
      - '#SSN#'
      - '#CardNumber#'
      - '#CarNumber#'
      - '#Email#'
  
  # mT5 모델 최적화 추론 설정 (GPU 메모리 고려)
  inference:
    batch_size: 2  # 대형 모델(1.2B)에 안전한 배치 크기
    generate_max_length: 84  # mT5 XL-Sum 최적 길이
    num_beams: 4             # Hugging Face 공식 권장
    no_repeat_ngram_size: 2  # 반복 방지
    early_stopping: true
    length_penalty: 1.0      # 길이 패널티
    do_sample: false         # 결정론적 생성
    remove_tokens:
      - <pad>
      - <unk>
      - </s>
    result_path: ./xlsum_mt5_predictions/
  
  # mT5용 QLoRA 설정 (대형 모델 메모리 최적화)
  qlora:
    use_unsloth: true  # AIStages RTX 3090 환경에서 활성화
    r: 16              # LoRA rank (대형 모델에 적합)
    lora_alpha: 32     # LoRA alpha
    lora_dropout: 0.1  # LoRA dropout
    bias: "none"       # 편향 업데이트 없음
    task_type: "SEQ_2_SEQ_LM"
    
    # mT5 아키텍처에 최적화된 타겟 모듈
    target_modules:
      - "q"      # Query projection
      - "k"      # Key projection  
      - "v"      # Value projection
      - "o"      # Output projection
    
    # BitsAndBytes 설정 (메모리 절약)
    load_in_4bit: true
    bnb_4bit_compute_dtype: float16
    bnb_4bit_quant_type: nf4
    bnb_4bit_use_double_quant: true
  
  # mT5 대형 모델 최적화 학습 설정
  training:
    do_eval: true
    do_train: true
    early_stopping_patience: 3
    early_stopping_threshold: 0.001
    evaluation_strategy: epoch
    fp16: true  # 메모리 50% 절약
    generation_max_length: 84
    
    # 안전한 배치 크기 (GPU 메모리 고려)
    per_device_train_batch_size: 1    # V100 16GB 기준 안전
    per_device_eval_batch_size: 2     # 추론은 약간 여유
    gradient_accumulation_steps: 4    # 효과적 배치 크기 = 4
    
    learning_rate: 5e-5  # 대형 모델에 적합한 LR
    load_best_model_at_end: true
    logging_dir: ./xlsum_mt5_logs
    logging_strategy: steps
    logging_steps: 100
    lr_scheduler_type: cosine
    num_train_epochs: 3  # 대형 모델은 적은 epoch로 충분
    optim: adamw_torch
    overwrite_output_dir: true
    predict_with_generate: true
    report_to: wandb
    save_strategy: epoch
    save_total_limit: 3  # 디스크 공간 절약
    seed: 42
    warmup_ratio: 0.1
    weight_decay: 0.01
    
    # 로깅 설정
    logging_dir: ./xlsum_mt5_logs
    logging_strategy: steps
    logging_steps: 50  # 대형 모델은 자주 로깅
    
    # 메모리 최적화
    gradient_checkpointing: false       # mT5에서 grad_fn 에러 방지
    dataloader_num_workers: 2
    dataloader_pin_memory: false        # 안정성 우선
    remove_unused_columns: false
    # mT5용 WandB 설정 (팀 공유 계정)
    wandb:
      entity: lyjune37-juneictlab  # 접근 가능한 팀 entity
      project: nlp-5     # 프로젝트명
      name: xlsum_mt5_korean_summarization_lyj  # 개별 실험명
      tags:
        - mT5
        - XL-Sum
        - Korean
        - Multilingual
        - Large-Model
        - lyj  # 개인 구분용 태그
      notes: "mT5 XL-Sum 한국어 요약 모델 실험 (lyj)"


inference:
  batch_size: 8  # 기본 모델에 안전한 크기 (32에서 조정)
  ckt_path: model ckt path
  early_stopping: true
  generate_max_length: 100
  no_repeat_ngram_size: 2
  num_beams: 4
  remove_tokens:
  - <usr>
  - <s>
  - </s>
  - <pad>
  result_path: ./prediction/
tokenizer:
  bos_token: <s>
  decoder_max_len: 100
  encoder_max_len: 512
  eos_token: </s>
  special_tokens:
  - '#Person1#'
  - '#Person2#'
  - '#Person3#'
  - '#PhoneNumber#'
  - '#Address#'
  - '#PassportNumber#'
training:
  do_eval: true
  do_train: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  evaluation_strategy: epoch
  fp16: true
  generation_max_length: 100
  gradient_accumulation_steps: 1
  learning_rate: 1.0e-05
  load_best_model_at_end: true
  logging_dir: ./logs
  logging_strategy: epoch
  lr_scheduler_type: cosine
  num_train_epochs: 20
  optim: adamw_torch
  overwrite_output_dir: true
  per_device_eval_batch_size: 8   # 기본 모델에 안전한 크기 (32에서 조정)
  per_device_train_batch_size: 8  # GPU 메모리 고려 조정 (50에서 조정)
  predict_with_generate: true
  report_to: wandb
  save_strategy: epoch
  save_total_limit: 5
  seed: 42
  warmup_ratio: 0.1
  weight_decay: 0.01
wandb:
  wandb:
    entity: lyjune37-juneictlab  # 접근 가능한 팀 entity
    project: nlp-5     # 프로젝트명
    name: baseline_lyj  # 개별 실험명
    tags:
      - baseline
      - lyj
    notes: "기본 설정 실험 (lyj)"

# inference 설정 추가 (baseline.ipynb와 동일)
inference:
  batch_size: 32
  no_repeat_ngram_size: 2
  early_stopping: true
  generate_max_length: 100
  num_beams: 4
  remove_tokens:
    - '<usr>'
    - '<s>'
    - '</s>'
    - '<pad>'