# Baseline.py vs Run_main_5_experiments.sh ìƒì„¸ ë¹„êµ ë¶„ì„

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#1-ê°œìš”)
2. [ì „ì²´ ì•„í‚¤í…ì²˜ ë¹„êµ](#2-ì „ì²´-ì•„í‚¤í…ì²˜-ë¹„êµ)
3. [Baseline.py ìƒì„¸ ë¶„ì„](#3-baselinepy-ìƒì„¸-ë¶„ì„)
4. [Run_main_5_experiments.sh íŒŒì´í”„ë¼ì¸ ë¶„ì„](#4-run_main_5_experimentssh-íŒŒì´í”„ë¼ì¸-ë¶„ì„)
5. [ë°ì´í„° ì²˜ë¦¬ íë¦„ ë¹„êµ](#5-ë°ì´í„°-ì²˜ë¦¬-íë¦„-ë¹„êµ)
6. [ëª¨ë¸ í•™ìŠµ ê³¼ì • ë¹„êµ](#6-ëª¨ë¸-í•™ìŠµ-ê³¼ì •-ë¹„êµ)
7. [ì¶”ë¡  ë° ê²°ê³¼ ìƒì„± ë¹„êµ](#7-ì¶”ë¡ -ë°-ê²°ê³¼-ìƒì„±-ë¹„êµ)
8. [ì£¼ìš” ì°¨ì´ì  ë° í†µí•© ë°©ì•ˆ](#8-ì£¼ìš”-ì°¨ì´ì -ë°-í†µí•©-ë°©ì•ˆ)

---

## 1. ê°œìš”

### 1.1 Baseline.py
- **ëª©ì **: ëŒ€í™” ìš”ì•½ ëª¨ë¸ì˜ ê¸°ë³¸ êµ¬í˜„ì²´
- **í˜•íƒœ**: Jupyter Notebookì—ì„œ ë³€í™˜ëœ ë‹¨ì¼ Python ìŠ¤í¬ë¦½íŠ¸
- **íŠ¹ì§•**: 
  - ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ êµ¬ì¡°
  - KoBART ëª¨ë¸ ê¸°ë°˜
  - ë‹¨ì¼ ì‹¤í—˜ìš© ì„¤ê³„
  - ìˆ˜ë™ ì„¤ì • ë°©ì‹

### 1.2 Run_main_5_experiments.sh
- **ëª©ì **: ì—¬ëŸ¬ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì‹¤í—˜í•˜ëŠ” ê³ ê¸‰ íŒŒì´í”„ë¼ì¸
- **í˜•íƒœ**: Bash ìŠ¤í¬ë¦½íŠ¸ + Python ëª¨ë“ˆ ì‹œìŠ¤í…œ
- **íŠ¹ì§•**:
  - ìë™í™”ëœ ë‹¤ì¤‘ ì‹¤í—˜ ì‹œìŠ¤í…œ
  - GPU ë©”ëª¨ë¦¬ ìµœì í™”
  - ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì› (mT5, T5, KoBART ë“±)
  - ëª¨ë“ˆí™”ëœ êµ¬ì¡°

---

## 2. ì „ì²´ ì•„í‚¤í…ì²˜ ë¹„êµ

### 2.1 Baseline.py ì•„í‚¤í…ì²˜
```
baseline.py
â”œâ”€â”€ ì„¤ì • ìƒì„± (YAML)
â”œâ”€â”€ ë°ì´í„° ë¡œë“œ
â”œâ”€â”€ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (Preprocess)
â”œâ”€â”€ Dataset í´ë˜ìŠ¤ë“¤
â”œâ”€â”€ ëª¨ë¸ ë¡œë“œ
â”œâ”€â”€ Trainer ì„¤ì •
â”œâ”€â”€ í•™ìŠµ ì‹¤í–‰
â””â”€â”€ ì¶”ë¡  ë° ê²°ê³¼ ì €ì¥
```

### 2.2 Run_main_5_experiments.sh ì•„í‚¤í…ì²˜
```
run_main_5_experiments.sh
â”œâ”€â”€ GPU ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜ë“¤
â”œâ”€â”€ ì‹¤í—˜ ëª©ë¡ ì •ì˜
â”œâ”€â”€ ê° ì‹¤í—˜ë³„ ë£¨í”„
â”‚   â”œâ”€â”€ auto_experiment_runner.py í˜¸ì¶œ
â”‚   â”‚   â”œâ”€â”€ ì„¤ì • ë¡œë“œ (YAML)
â”‚   â”‚   â”œâ”€â”€ trainer.py í˜¸ì¶œ
â”‚   â”‚   â”‚   â”œâ”€â”€ DataProcessor (utils/data_utils.py)
â”‚   â”‚   â”‚   â”œâ”€â”€ ëª¨ë¸ ë¡œë“œ (ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›)
â”‚   â”‚   â”‚   â”œâ”€â”€ í•™ìŠµ ì‹¤í–‰
â”‚   â”‚   â”‚   â””â”€â”€ ë©”íŠ¸ë¦­ ê³„ì‚°
â”‚   â”‚   â””â”€â”€ post_training_inference.py
â”‚   â””â”€â”€ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
â””â”€â”€ ê²°ê³¼ ìš”ì•½
```

---

## 3. Baseline.py ìƒì„¸ ë¶„ì„

### 3.1 ë°ì´í„° ì²˜ë¦¬ ê³¼ì •

#### 3.1.1 ë°ì´í„° ë¡œë“œ
```python
# baseline.py ë¼ì¸ 160-165
train_df = pd.read_csv(os.path.join(data_path,'train.csv'))
val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))
```

#### 3.1.2 ì „ì²˜ë¦¬ í´ë˜ìŠ¤
```python
# baseline.py ë¼ì¸ 174-203
class Preprocess:
    def __init__(self, bos_token: str, eos_token: str):
        self.bos_token = bos_token
        self.eos_token = eos_token
    
    def make_input(self, dataset, is_test=False):
        if is_test:
            # í…ŒìŠ¤íŠ¸ìš©: ëŒ€í™”ë§Œ ì¸ì½”ë” ì…ë ¥ìœ¼ë¡œ
            encoder_input = dataset['dialogue']
            decoder_input = [self.bos_token] * len(dataset['dialogue'])
        else:
            # í•™ìŠµìš©: ëŒ€í™”ëŠ” ì¸ì½”ë”, ìš”ì•½ì€ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ
            encoder_input = dataset['dialogue']
            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))
            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)
```

**ê°œë… ì„¤ëª…**:
- **BOS (Beginning of Sequence)**: ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ í† í°
- **EOS (End of Sequence)**: ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ í† í°
- **ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°**: 
  - ì¸ì½”ë”: ì…ë ¥(ëŒ€í™”)ì„ ì´í•´í•˜ëŠ” ë¶€ë¶„
  - ë””ì½”ë”: ì¶œë ¥(ìš”ì•½)ì„ ìƒì„±í•˜ëŠ” ë¶€ë¶„

### 3.2 ëª¨ë¸ êµ¬ì„±

#### 3.2.1 ëª¨ë¸ ë¡œë“œ
```python
# baseline.py ë¼ì¸ 384-395
def load_tokenizer_and_model_for_train(config, device):
    model_name = config['general']['model_name']  # "digit82/kobart-summarization"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    generate_model = BartForConditionalGeneration.from_pretrained(model_name)
    
    # íŠ¹ìˆ˜ í† í° ì¶”ê°€
    special_tokens_dict = {
        'additional_special_tokens': ['#Person1#', '#Person2#', '#Person3#', 
                                     '#PhoneNumber#', '#Address#', '#PassportNumber#']
    }
    tokenizer.add_special_tokens(special_tokens_dict)
    generate_model.resize_token_embeddings(len(tokenizer))
```

**ê°œë… ì„¤ëª…**:
- **BART**: Bidirectional and Auto-Regressive Transformers
- **íŠ¹ìˆ˜ í† í°**: ê°œì¸ì •ë³´ë¥¼ ë§ˆìŠ¤í‚¹í•˜ëŠ” í† í°ë“¤
- **í† í° ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ**: ìƒˆ í† í° ì¶”ê°€ ì‹œ ëª¨ë¸ í¬ê¸° ì¡°ì •

### 3.3 í•™ìŠµ ì„¤ì •

```python
# baseline.py ë¼ì¸ 321-350
training_args = Seq2SeqTrainingArguments(
    output_dir=config['general']['output_dir'],
    num_train_epochs=20,
    learning_rate=1e-5,
    per_device_train_batch_size=50,
    per_device_eval_batch_size=32,
    warmup_ratio=0.1,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    fp16=True,  # 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    predict_with_generate=True,  # ìƒì„± ì‘ì—…ìš©
    generation_max_length=100
)
```

---

## 4. Run_main_5_experiments.sh íŒŒì´í”„ë¼ì¸ ë¶„ì„

### 4.1 ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œ

#### 4.1.1 GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
```bash
# run_main_5_experiments.sh ë¼ì¸ 42-80
enhanced_gpu_monitor() {
    local gpu_data=$(nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu,temperature.gpu \
                     --format=csv,noheader,nounits)
    # GPU ìƒíƒœ ë¶„ì„ ë° ê²½ê³ 
    if [ "$memory_used" -gt 22000 ]; then
        echo "âš ï¸ ê²½ê³ : GPU ë©”ëª¨ë¦¬ ì„ê³„ ìƒíƒœ (22GB ì´ˆê³¼)"
    fi
}
```

#### 4.1.2 ìŠ¤ë§ˆíŠ¸ ëŒ€ê¸° ì‹œìŠ¤í…œ
```bash
# run_main_5_experiments.sh ë¼ì¸ 83-112
smart_wait() {
    local target_memory=${1:-5000}  # ëª©í‘œ: 5GB ì´í•˜
    # GPU ë©”ëª¨ë¦¬ê°€ ëª©í‘œì¹˜ ì´í•˜ê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    while [ "$current_memory" -gt "$target_memory" ]; do
        sleep 10
    done
}
```

### 4.2 ì‹¤í—˜ ì‹¤í–‰ íë¦„

#### 4.2.1 ì‹¤í—˜ ëª©ë¡ ì •ì˜
```bash
# run_main_5_experiments.sh ë¼ì¸ 290-299
declare -a experiments=(
    01_mt5_xlsum_ultimate_korean_qlora.yaml:ğŸš€_mT5_í•œêµ­ì–´_QLoRA_ê·¹í•œìµœì í™”:60ë¶„
    02_eenzeenee_t5_rtx3090.yaml:ğŸ’ª_eenzeenee_T5_RTX3090_ê·¹í•œìµœì í™”:40ë¶„
    01_baseline_kobart_rtx3090.yaml:ğŸ’ª_KoBART_RTX3090_ê·¹í•œìµœì í™”:45ë¶„
    # ...
)
```

#### 4.2.2 ê° ì‹¤í—˜ ì‹¤í–‰
```bash
# run_main_5_experiments.sh ë¼ì¸ 382-440
for i in "${!experiments[@]}"; do
    # 1. GPU ìƒíƒœ í™•ì¸
    enhanced_gpu_monitor "ì‹¤í—˜ $EXPERIMENT_NUM ì‹œì‘ ì „"
    
    # 2. auto_experiment_runner.py ì‹¤í–‰
    EXPERIMENT_CMD="python code/auto_experiment_runner.py --config config/experiments/${config_file}"
    
    # 3. ì‹¤í—˜ ì„±ê³µ/ì‹¤íŒ¨ ì²˜ë¦¬
    if eval "$EXPERIMENT_CMD > ${LOG_FILE} 2>&1"; then
        echo "âœ… ì‹¤í—˜ ${EXPERIMENT_NUM} ì™„ë£Œ!"
    else
        handle_experiment_error "$exp_name" "$LOG_FILE" "$EXPERIMENT_NUM"
    fi
    
    # 4. GPU ì •ë¦¬ ë° ëŒ€ê¸°
    cleanup_gpu
    smart_wait 5000 240
done
```

### 4.3 Auto Experiment Runner ë¶„ì„

#### 4.3.1 ì„¤ì • ë³‘í•© ì‹œìŠ¤í…œ
```python
# auto_experiment_runner.py ë¼ì¸ 186-208
def _load_and_merge_config(self, config_path: str) -> Dict[str, Any]:
    # 1. ê¸°ë³¸ ì„¤ì • ë¡œë“œ
    base_config = load_config(self.base_config_path)
    
    # 2. ì‹¤í—˜ë³„ ì„¤ì • ë¡œë“œ
    exp_config = load_config(exp_config_path)
    
    # 3. ë”¥ ë¨¸ì§€ (ì‹¤í—˜ ì„¤ì •ì´ ìš°ì„ )
    merged = self._deep_merge(base_config, exp_config)
    return merged
```

#### 4.3.2 ë””ë°”ì´ìŠ¤ ìµœì í™”
```python
# auto_experiment_runner.py ë¼ì¸ 251-275
def _apply_device_config(self, config: Dict[str, Any]) -> None:
    # ëª¨ë¸ í¬ê¸° ì¶”ì •
    if 'large' in model_name or 'xl' in model_name:
        model_size = 'large'
    
    # ìµœì í™” ì„¤ì • ìƒì„±
    opt_config = setup_device_config(self.device_info, model_size)
    
    # ë””ë°”ì´ìŠ¤ë³„ ìµœì  ë°°ì¹˜ í¬ê¸°, gradient accumulation ë“± ì„¤ì •
```

---

## 5. ë°ì´í„° ì²˜ë¦¬ íë¦„ ë¹„êµ

### 5.1 Baseline.py ë°ì´í„° ì²˜ë¦¬
```
train.csv â†’ Pandas DataFrame â†’ Preprocess í´ë˜ìŠ¤ â†’ Tokenizer â†’ Dataset í´ë˜ìŠ¤ â†’ DataLoader
```

### 5.2 Pipeline ë°ì´í„° ì²˜ë¦¬
```
train.csv â†’ DataProcessor (utils/data_utils.py) â†’ 
â”œâ”€â”€ ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ì˜µì…˜ (ë…¸ì´ì¦ˆ ì œê±°, ì •ê·œí™” ë“±)
â”œâ”€â”€ ë°ì´í„° ì¦ê°• ì˜µì…˜
â”œâ”€â”€ ë™ì  í† í°í™” (ëª¨ë¸ë³„ ìµœì í™”)
â””â”€â”€ HuggingFace Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜
```

### 5.3 ì£¼ìš” ì°¨ì´ì 

#### Baseline.py:
```python
# ê°„ë‹¨í•œ ì „ì²˜ë¦¬
encoder_input = dataset['dialogue']
decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))
```

#### Pipeline (DataProcessor):
```python
# utils/data_utils.pyì˜ ê³ ê¸‰ ì „ì²˜ë¦¬
def preprocess_dialogue(self, text: str) -> str:
    # 1. HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    
    # 2. íŠ¹ìˆ˜ ë¬¸ì ì •ê·œí™”
    text = self._normalize_whitespace(text)
    
    # 3. ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
    text = self._mask_personal_info(text)
    
    # 4. ëŒ€í™” êµ¬ì¡° íŒŒì‹±
    text = self._parse_dialogue_structure(text)
    
    return text
```

---

## 6. ëª¨ë¸ í•™ìŠµ ê³¼ì • ë¹„êµ

### 6.1 Baseline.py í•™ìŠµ

#### 6.1.1 ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ
```python
# baseline.py ë¼ì¸ 361-373
trainer = Seq2SeqTrainer(
    model=generate_model,
    args=training_args,
    train_dataset=train_inputs_dataset,
    eval_dataset=val_inputs_dataset,
    compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),
    callbacks=[EarlyStoppingCallback()]
)
trainer.train()
```

### 6.2 Pipeline í•™ìŠµ

#### 6.2.1 ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› (trainer.py)
```python
# trainer.pyì˜ ëª¨ë¸ ë¡œë“œ ë¡œì§
def load_model(self, config: Dict[str, Any]):
    model_name = config['general']['model_name']
    
    if 'mt5' in model_name.lower():
        # mT5 ëª¨ë¸ ë¡œë“œ
        model = self._load_mt5_model(config)
    elif 't5' in model_name.lower():
        # T5 ëª¨ë¸ ë¡œë“œ
        model = self._load_t5_model(config)
    elif 'bart' in model_name.lower():
        # BART ëª¨ë¸ ë¡œë“œ
        model = self._load_bart_model(config)
    elif 'solar' in model_name.lower():
        # Solar ëª¨ë¸ ë¡œë“œ (Causal LM)
        model = self._load_solar_model(config)
    
    # QLoRA ì ìš© (ì„ íƒì )
    if config.get('use_qlora', False):
        model = self._apply_qlora(model, config)
    
    return model
```

#### 6.2.2 ê³ ê¸‰ í•™ìŠµ ê¸°ëŠ¥
```python
# trainer.pyì˜ í•™ìŠµ ìµœì í™”
class DialogueSummarizationTrainer:
    def train(self):
        # 1. Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
        if self.config.get('gradient_checkpointing', False):
            self.model.gradient_checkpointing_enable()
        
        # 2. Mixed Precision Training (ì†ë„ í–¥ìƒ)
        if self.config.get('fp16', False) or self.config.get('bf16', False):
            self._setup_mixed_precision()
        
        # 3. DeepSpeed í†µí•© (ëŒ€ê·œëª¨ ëª¨ë¸)
        if self.config.get('deepspeed'):
            self._setup_deepspeed()
        
        # 4. ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if self.config.get('auto_find_batch_size', False):
            self._find_optimal_batch_size()
```

---

## 7. ì¶”ë¡  ë° ê²°ê³¼ ìƒì„± ë¹„êµ

### 7.1 Baseline.py ì¶”ë¡ 

```python
# baseline.py ë¼ì¸ 499-523
def inference(config):
    # 1. ëª¨ë¸ ë¡œë“œ
    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)
    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=32)
    
    # 3. ì¶”ë¡  ì‹¤í–‰
    summary = []
    with torch.no_grad():
        for item in tqdm(dataloader):
            generated_ids = generate_model.generate(
                input_ids=item['input_ids'].to('cuda:0'),
                no_repeat_ngram_size=2,
                early_stopping=True,
                max_length=100,
                num_beams=4
            )
            for ids in generated_ids:
                result = tokenizer.decode(ids)
                summary.append(result)
    
    # 4. ê²°ê³¼ ì €ì¥
    output = pd.DataFrame({
        "fname": test_data['fname'],
        "summary": preprocessed_summary
    })
    output.to_csv(os.path.join(result_path, "output.csv"), index=False)
```

### 7.2 Pipeline ì¶”ë¡ 

#### 7.2.1 ìë™ ì¶”ë¡  ì‹œìŠ¤í…œ (auto_experiment_runner.py)
```python
# auto_experiment_runner.py ë¼ì¸ 420-470
# í•™ìŠµ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ test.csv ì¶”ë¡  ìˆ˜í–‰
try:
    from post_training_inference import generate_submission_after_training
    
    submission_path = generate_submission_after_training(
        experiment_name=experiment_id,
        model_path=str(best_checkpoint),
        config_dict=config
    )
except:
    # ëŒ€ì•ˆ: run_inference.py ì§ì ‘ ì‚¬ìš©
    inference_cmd = [
        sys.executable,
        "code/run_inference.py",
        "--model_path", str(best_checkpoint),
        "--input_file", "data/test.csv",
        "--output_file", f"outputs/{experiment_id}_submission.csv"
    ]
```

#### 7.2.2 ê³ ê¸‰ ìƒì„± ì „ëµ (run_inference.py)
```python
# run_inference.pyì˜ ìƒì„± ì „ëµ
def generate_summary(self, batch_texts: List[str]) -> List[str]:
    # 1. ë‹¤ì–‘í•œ ìƒì„± ì „ëµ ì§€ì›
    if self.generation_strategy == 'beam_search':
        outputs = self._beam_search_generate(batch_texts)
    elif self.generation_strategy == 'sampling':
        outputs = self._sampling_generate(batch_texts)
    elif self.generation_strategy == 'contrastive':
        outputs = self._contrastive_generate(batch_texts)
    
    # 2. í›„ì²˜ë¦¬
    summaries = self._postprocess_summaries(outputs)
    
    return summaries
```

---

## 8. ì£¼ìš” ì°¨ì´ì  ë° í†µí•© ë°©ì•ˆ

### 8.1 ì£¼ìš” ì°¨ì´ì  ìš”ì•½

| í•­ëª© | Baseline.py | Pipeline |
|------|------------|----------|
| **êµ¬ì¡°** | ë‹¨ì¼ ìŠ¤í¬ë¦½íŠ¸ | ëª¨ë“ˆí™”ëœ ì‹œìŠ¤í…œ |
| **ëª¨ë¸** | KoBARTë§Œ ì§€ì› | ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì› |
| **ì‹¤í—˜** | ìˆ˜ë™ ì‹¤í–‰ | ìë™í™”ëœ ë‹¤ì¤‘ ì‹¤í—˜ |
| **GPU ê´€ë¦¬** | ê¸°ë³¸ì  | ê³ ê¸‰ ë©”ëª¨ë¦¬ ìµœì í™” |
| **ë°ì´í„° ì²˜ë¦¬** | ê°„ë‹¨í•œ ì „ì²˜ë¦¬ | ê³ ê¸‰ ì „ì²˜ë¦¬ ë° ì¦ê°• |
| **ì„¤ì • ê´€ë¦¬** | í•˜ë“œì½”ë”© | YAML ê¸°ë°˜ ìœ ì—°í•œ ì„¤ì • |
| **ì¶”ë¡ ** | ìˆ˜ë™ ì‹¤í–‰ í•„ìš” | í•™ìŠµ í›„ ìë™ ì‹¤í–‰ |
| **ê²°ê³¼ ì¶”ì ** | ê¸°ë³¸ì  | WandB, CSV, JSON ë“± ë‹¤ì–‘í•œ í˜•ì‹ |

### 8.2 í†µí•© ë°©ì•ˆ

#### 8.2.1 Baseline ë¡œì§ì„ Pipelineì— í†µí•©
```python
# config/experiments/00_baseline_exact.yaml
general:
  model_name: "digit82/kobart-summarization"
  experiment_name: "baseline_exact_reproduction"

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 100
  special_tokens: ['#Person1#', '#Person2#', '#Person3#', 
                   '#PhoneNumber#', '#Address#', '#PassportNumber#']

training:
  num_train_epochs: 20
  learning_rate: 1e-5
  per_device_train_batch_size: 50
  per_device_eval_batch_size: 32
  warmup_ratio: 0.1
  weight_decay: 0.01
  lr_scheduler_type: 'cosine'
  fp16: true
  evaluation_strategy: 'epoch'
  save_strategy: 'epoch'
  early_stopping_patience: 3

inference:
  no_repeat_ngram_size: 2
  early_stopping: true
  max_length: 100
  num_beams: 4
  batch_size: 32
```

#### 8.2.2 ë°ì´í„° ì²˜ë¦¬ í†µí•©
```python
# utils/data_utils.pyì— baseline ëª¨ë“œ ì¶”ê°€
class DataProcessor:
    def __init__(self, config: Dict[str, Any], baseline_mode: bool = False):
        self.baseline_mode = baseline_mode
        
    def preprocess_dialogue(self, text: str) -> str:
        if self.baseline_mode:
            # Baselineê³¼ ë™ì¼í•œ ê°„ë‹¨í•œ ì²˜ë¦¬
            return text
        else:
            # ê³ ê¸‰ ì „ì²˜ë¦¬
            return self._advanced_preprocess(text)
```

### 8.3 ì‹¤í–‰ ëª…ë ¹ì–´ ë¹„êµ

#### Baseline.py ì‹¤í–‰:
```bash
python code/baseline.py
```

#### Pipeline ì‹¤í–‰:
```bash
# ì „ì²´ ì‹¤í—˜ ì‹¤í–‰
bash run_main_5_experiments.sh

# íŠ¹ì • ì‹¤í—˜ë§Œ ì‹¤í–‰
python code/auto_experiment_runner.py --config config/experiments/01_baseline.yaml

# 1ì—í¬í¬ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
bash run_main_5_experiments.sh -1
```

### 8.4 ê²°ê³¼ íŒŒì¼ êµ¬ì¡°

#### Baseline.py:
```
outputs/
â””â”€â”€ prediction/
    â””â”€â”€ output.csv
```

#### Pipeline:
```
outputs/
â”œâ”€â”€ auto_experiments/
â”‚   â”œâ”€â”€ experiments/          # ì‹¤í—˜ë³„ ìƒì„¸ ê²°ê³¼
â”‚   â”œâ”€â”€ models/              # ì €ì¥ëœ ëª¨ë¸ë“¤
â”‚   â”œâ”€â”€ csv_results/         # CSV í˜•ì‹ ê²°ê³¼
â”‚   â””â”€â”€ experiment_summary.json
â”œâ”€â”€ checkpoints/             # í•™ìŠµ ì²´í¬í¬ì¸íŠ¸
â””â”€â”€ submissions/            # ì œì¶œìš© CSV íŒŒì¼
```

---

## ê²°ë¡ 

1. **Baseline.py**ëŠ” ë‹¨ìˆœí•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ êµ¬ì¡°ë¡œ, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì— ì í•©í•©ë‹ˆë‹¤.

2. **Pipeline**ì€ ëŒ€ê·œëª¨ ì‹¤í—˜ê³¼ í”„ë¡œë•ì…˜ í™˜ê²½ì— ì í•©í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

3. ë‘ ì‹œìŠ¤í…œì€ **ë™ì¼í•œ ë°ì´í„° íë¦„**ì„ ë”°ë¥´ë¯€ë¡œ, Pipelineì—ì„œ Baselineê³¼ ì™„ì „íˆ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

4. **í†µí•© ê¶Œì¥ì‚¬í•­**:
   - ì´ˆê¸° ê°œë°œ: Baseline.pyë¡œ ë¹ ë¥¸ ê²€ì¦
   - ë³¸ê²© ì‹¤í—˜: Pipelineìœ¼ë¡œ ë‹¤ì–‘í•œ ëª¨ë¸/ì„¤ì • í…ŒìŠ¤íŠ¸
   - ìµœì¢… ì œì¶œ: Pipelineì˜ best ëª¨ë¸ ì‚¬ìš©

5. **test.csv ì²˜ë¦¬**ëŠ” ë‘ ì‹œìŠ¤í…œ ëª¨ë‘ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ìˆ˜í–‰ë˜ë©°, Pipelineì€ í•™ìŠµ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ì¶”ë¡ ê¹Œì§€ ì‹¤í–‰í•©ë‹ˆë‹¤.
