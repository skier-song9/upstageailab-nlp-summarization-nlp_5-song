# Test.csv ì²˜ë¦¬ ë° ê²°ê³¼ ìƒì„± ìƒì„¸ ë¶„ì„

## ğŸ“‹ ëª©ì°¨
1. [Test ë°ì´í„° êµ¬ì¡° ë¶„ì„](#1-test-ë°ì´í„°-êµ¬ì¡°-ë¶„ì„)
2. [Baseline.pyì˜ Test ì²˜ë¦¬ íë¦„](#2-baselinepyì˜-test-ì²˜ë¦¬-íë¦„)
3. [Pipelineì˜ Test ì²˜ë¦¬ íë¦„](#3-pipelineì˜-test-ì²˜ë¦¬-íë¦„)
4. [ê²°ê³¼ CSV ìƒì„± ë¹„êµ](#4-ê²°ê³¼-csv-ìƒì„±-ë¹„êµ)
5. [ì‹¤ì œ êµ¬í˜„ ë§¤í•‘](#5-ì‹¤ì œ-êµ¬í˜„-ë§¤í•‘)

---

## 1. Test ë°ì´í„° êµ¬ì¡° ë¶„ì„

### 1.1 ë°ì´í„° íŒŒì¼ ìœ„ì¹˜
- **Train ë°ì´í„°**: `/data/train.csv`
- **Dev ë°ì´í„°**: `/data/dev.csv`  
- **Test ë°ì´í„°**: `/data/test.csv`

### 1.2 ë°ì´í„° êµ¬ì¡°
```python
# Train/Dev ë°ì´í„° êµ¬ì¡°
train_df = pd.DataFrame({
    'fname': ['ëŒ€í™”ID_1', 'ëŒ€í™”ID_2', ...],      # íŒŒì¼ëª…/ID
    'dialogue': ['ëŒ€í™” ë‚´ìš© 1', 'ëŒ€í™” ë‚´ìš© 2', ...],  # ì…ë ¥ ëŒ€í™”
    'summary': ['ìš”ì•½ 1', 'ìš”ì•½ 2', ...]         # ì •ë‹µ ìš”ì•½ (í•™ìŠµìš©)
})

# Test ë°ì´í„° êµ¬ì¡° 
test_df = pd.DataFrame({
    'fname': ['í…ŒìŠ¤íŠ¸ID_1', 'í…ŒìŠ¤íŠ¸ID_2', ...],   # íŒŒì¼ëª…/ID
    'dialogue': ['í…ŒìŠ¤íŠ¸ ëŒ€í™” 1', 'í…ŒìŠ¤íŠ¸ ëŒ€í™” 2', ...]  # ì…ë ¥ ëŒ€í™”ë§Œ ì¡´ì¬
    # summary ì»¬ëŸ¼ ì—†ìŒ - ëª¨ë¸ì´ ìƒì„±í•´ì•¼ í•¨
})
```

---

## 2. Baseline.pyì˜ Test ì²˜ë¦¬ íë¦„

### 2.1 ì „ì²´ ì²˜ë¦¬ ê³¼ì •
```
1. ëª¨ë¸ í•™ìŠµ ì™„ë£Œ
2. ìˆ˜ë™ìœ¼ë¡œ inference() í•¨ìˆ˜ ì‹¤í–‰
3. test.csv ë¡œë“œ
4. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìš”ì•½ ìƒì„±  
5. output.csv ì €ì¥
```

### 2.2 ìƒì„¸ ì½”ë“œ ë¶„ì„

#### 2.2.1 Test ë°ì´í„° ì¤€ë¹„ (baseline.py ë¼ì¸ 455-490)
```python
def prepare_test_dataset(config, preprocessor, tokenizer):
    # 1. test.csv íŒŒì¼ ì½ê¸°
    test_file_path = os.path.join(config['general']['data_path'], 'test.csv')
    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)
    test_id = test_data['fname']  # ê²°ê³¼ ì €ì¥ìš© ID
    
    # 2. ì „ì²˜ë¦¬ - BOS í† í°ë§Œ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì„¤ì •
    encoder_input_test, decoder_input_test = preprocessor.make_input(
        test_data, 
        is_test=True  # test ëª¨ë“œ í™œì„±í™”
    )
    
    # 3. í† í°í™”
    test_tokenized_encoder_inputs = tokenizer(
        encoder_input_test,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )
    
    # 4. Dataset ê°ì²´ ìƒì„±
    test_encoder_inputs_dataset = DatasetForInference(
        test_tokenized_encoder_inputs, 
        test_id, 
        len(encoder_input_test)
    )
    
    return test_data, test_encoder_inputs_dataset
```

#### 2.2.2 ì¶”ë¡  ì‹¤í–‰ (baseline.py ë¼ì¸ 493-542)
```python
def inference(config):
    # 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    # 2. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)
    
    # 3. ë°ì´í„° ì¤€ë¹„
    preprocessor = Preprocess(
        config['tokenizer']['bos_token'], 
        config['tokenizer']['eos_token']
    )
    test_data, test_encoder_inputs_dataset = prepare_test_dataset(
        config, preprocessor, tokenizer
    )
    
    # 4. DataLoader ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ìš©)
    dataloader = DataLoader(
        test_encoder_inputs_dataset, 
        batch_size=config['inference']['batch_size']  # 32
    )
    
    # 5. ë°°ì¹˜ ë‹¨ìœ„ ì¶”ë¡ 
    summary = []
    text_ids = []
    
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        for item in tqdm(dataloader):
            # ID ìˆ˜ì§‘
            text_ids.extend(item['ID'])
            
            # ìš”ì•½ ìƒì„±
            generated_ids = generate_model.generate(
                input_ids=item['input_ids'].to(device),
                no_repeat_ngram_size=2,      # 2-gram ë°˜ë³µ ë°©ì§€
                early_stopping=True,          # EOS í† í° ì‹œ ì¡°ê¸° ì¢…ë£Œ
                max_length=100,              # ìµœëŒ€ ìƒì„± ê¸¸ì´
                num_beams=4                  # ë¹” ì„œì¹˜ í¬ê¸°
            )
            
            # ë””ì½”ë”©
            for ids in generated_ids:
                result = tokenizer.decode(ids)
                summary.append(result)
```

#### 2.2.3 ê²°ê³¼ ì €ì¥ (baseline.py ë¼ì¸ 544-560)
```python
    # 6. í›„ì²˜ë¦¬ - íŠ¹ìˆ˜ í† í° ì œê±°
    remove_tokens = config['inference']['remove_tokens']
    # ['<usr>', '<s>', '</s>', '<pad>']
    
    preprocessed_summary = summary.copy()
    for token in remove_tokens:
        preprocessed_summary = [
            sentence.replace(token, " ") for sentence in preprocessed_summary
        ]
    
    # 7. DataFrame ìƒì„± ë° CSV ì €ì¥
    output = pd.DataFrame({
        "fname": test_data['fname'],      # ì›ë³¸ íŒŒì¼ëª…/ID
        "summary": preprocessed_summary    # ìƒì„±ëœ ìš”ì•½
    })
    
    # 8. ê²°ê³¼ íŒŒì¼ ì €ì¥
    result_path = config['inference']['result_path']  # "./prediction/"
    if not os.path.exists(result_path):
        os.makedirs(result_path)
    
    output.to_csv(
        os.path.join(result_path, "output.csv"), 
        index=False  # ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì œì™¸
    )
```

---

## 3. Pipelineì˜ Test ì²˜ë¦¬ íë¦„

### 3.1 ì „ì²´ ì²˜ë¦¬ ê³¼ì •
```
1. ëª¨ë¸ í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ best checkpoint ì €ì¥
2. í•™ìŠµ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ test ì¶”ë¡  ì‹œì‘
3. post_training_inference.py ë˜ëŠ” run_inference.py ì‹¤í–‰
4. ë‹¤ì–‘í•œ ìƒì„± ì „ëµ ì ìš© ê°€ëŠ¥
5. ì‹¤í—˜ë³„ë¡œ ê³ ìœ í•œ submission íŒŒì¼ ìƒì„±
```

### 3.2 ìë™ ì¶”ë¡  ì‹œìŠ¤í…œ

#### 3.2.1 Auto Experiment Runnerì˜ ì¶”ë¡  íŠ¸ë¦¬ê±° (auto_experiment_runner.py ë¼ì¸ 418-473)
```python
def _run_single_experiment(self, config, config_path, one_epoch=False):
    # ... í•™ìŠµ ì‹¤í–‰ ...
    
    if process.returncode == 0:  # í•™ìŠµ ì„±ê³µ ì‹œ
        # 1. ë² ìŠ¤íŠ¸ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        print(f"\nğŸ“Š Test ì¶”ë¡  ì‹œì‘: {experiment_id}")
        
        output_dir = Path(config.get('training', {}).get('output_dir', 'outputs'))
        checkpoint_dirs = list(output_dir.glob('checkpoint-*'))
        
        if checkpoint_dirs:
            # ê°€ì¥ ìµœê·¼(ë² ìŠ¤íŠ¸) ì²´í¬í¬ì¸íŠ¸ ì„ íƒ
            best_checkpoint = max(checkpoint_dirs, key=lambda p: p.stat().st_mtime)
            print(f"ğŸ¯ ë² ìŠ¤íŠ¸ ì²´í¬í¬ì¸íŠ¸: {best_checkpoint}")
            
            # 2. post_training_inference ëª¨ë“ˆ ì‚¬ìš©
            try:
                from post_training_inference import generate_submission_after_training
                
                submission_path = generate_submission_after_training(
                    experiment_name=experiment_id,
                    model_path=str(best_checkpoint),
                    config_dict=config
                )
                
                print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_path}")
                
            except ImportError:
                # 3. ëŒ€ì•ˆ: run_inference.py ì§ì ‘ ì‹¤í–‰
                self._run_inference_fallback(best_checkpoint, experiment_id)
```

#### 3.2.2 Post Training Inference êµ¬í˜„
```python
# post_training_inference.py
def generate_submission_after_training(
    experiment_name: str,
    model_path: str,
    config_dict: Dict[str, Any]
) -> str:
    """í•™ìŠµ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ test.csvì— ëŒ€í•œ ì¶”ë¡  ì‹¤í–‰"""
    
    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_trained_model(model_path, config_dict)
    
    # 2. Test ë°ì´í„° ë¡œë“œ
    test_df = pd.read_csv('data/test.csv')
    
    # 3. ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰
    batch_size = config_dict.get('inference', {}).get('batch_size', 16)
    summaries = []
    
    for i in range(0, len(test_df), batch_size):
        batch = test_df.iloc[i:i+batch_size]
        batch_summaries = generate_batch_summaries(
            model, tokenizer, batch['dialogue'].tolist(), config_dict
        )
        summaries.extend(batch_summaries)
    
    # 4. ê²°ê³¼ ì €ì¥
    submission_df = pd.DataFrame({
        'fname': test_df['fname'],
        'summary': summaries
    })
    
    # 5. ì‹¤í—˜ë³„ ê³ ìœ  íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_path = f'outputs/submissions/{experiment_name}_{timestamp}.csv'
    
    submission_df.to_csv(output_path, index=False)
    return output_path
```

### 3.3 ê³ ê¸‰ ì¶”ë¡  ê¸°ëŠ¥

#### 3.3.1 Run Inferenceì˜ ìƒì„± ì „ëµ (run_inference.py)
```python
class InferenceRunner:
    def __init__(self, model_path: str, generation_config: Dict[str, Any]):
        self.model, self.tokenizer = self.load_model(model_path)
        self.generation_config = generation_config
        
    def generate_summaries(self, dialogues: List[str]) -> List[str]:
        """ë‹¤ì–‘í•œ ìƒì„± ì „ëµì„ ì§€ì›í•˜ëŠ” ì¶”ë¡  í•¨ìˆ˜"""
        
        strategy = self.generation_config.get('strategy', 'beam_search')
        
        if strategy == 'beam_search':
            return self._beam_search_generation(dialogues)
        elif strategy == 'sampling':
            return self._sampling_generation(dialogues)
        elif strategy == 'diverse_beam_search':
            return self._diverse_beam_generation(dialogues)
        elif strategy == 'contrastive_search':
            return self._contrastive_generation(dialogues)
            
    def _beam_search_generation(self, dialogues: List[str]) -> List[str]:
        """ë¹” ì„œì¹˜ ê¸°ë°˜ ìƒì„± (Baselineê³¼ ë™ì¼)"""
        inputs = self.tokenizer(
            dialogues,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        outputs = self.model.generate(
            **inputs,
            max_length=100,
            num_beams=4,
            no_repeat_ngram_size=2,
            early_stopping=True,
            length_penalty=1.0,
            repetition_penalty=1.2
        )
        
        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

#### 3.3.2 í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```python
def postprocess_summaries(summaries: List[str], config: Dict[str, Any]) -> List[str]:
    """ìƒì„±ëœ ìš”ì•½ë¬¸ í›„ì²˜ë¦¬"""
    
    processed = []
    for summary in summaries:
        # 1. íŠ¹ìˆ˜ í† í° ì œê±°
        for token in config.get('remove_tokens', []):
            summary = summary.replace(token, ' ')
        
        # 2. ê³µë°± ì •ê·œí™”
        summary = ' '.join(summary.split())
        
        # 3. ë¬¸ì¥ ë ì²˜ë¦¬
        if not summary.endswith('.'):
            summary += '.'
            
        # 4. ê¸¸ì´ ì œí•œ
        max_length = config.get('max_summary_length', 150)
        if len(summary.split()) > max_length:
            words = summary.split()[:max_length]
            summary = ' '.join(words) + '.'
            
        processed.append(summary)
    
    return processed
```

---

## 4. ê²°ê³¼ CSV ìƒì„± ë¹„êµ

### 4.1 Baseline ê²°ê³¼ í˜•ì‹
```csv
fname,summary
ëŒ€í™”_001,íšŒì˜ì—ì„œ í”„ë¡œì íŠ¸ ì¼ì •ê³¼ ì˜ˆì‚°ì— ëŒ€í•´ ë…¼ì˜í–ˆìŠµë‹ˆë‹¤.
ëŒ€í™”_002,ê³ ê°ì´ ì œí’ˆ í™˜ë¶ˆì„ ìš”ì²­í–ˆê³  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
...
```

### 4.2 Pipeline ê²°ê³¼ í˜•ì‹
```csv
fname,summary
ëŒ€í™”_001,íšŒì˜ì—ì„œ í”„ë¡œì íŠ¸ ì¼ì •ê³¼ ì˜ˆì‚°ì— ëŒ€í•´ ë…¼ì˜í–ˆìŠµë‹ˆë‹¤.
ëŒ€í™”_002,ê³ ê°ì´ ì œí’ˆ í™˜ë¶ˆì„ ìš”ì²­í–ˆê³  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
...
```

**ë™ì¼í•œ í˜•ì‹**ì´ì§€ë§Œ, Pipelineì€ ì¶”ê°€ ë©”íƒ€ë°ì´í„°ë„ ì €ì¥:
- ì‹¤í—˜ IDì™€ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ íŒŒì¼ëª…
- ì‹¤í—˜ë³„ ì„¤ì • ì •ë³´ (JSON)
- ìƒì„± ë©”íŠ¸ë¦­ (ìƒì„± ì‹œê°„, í† í° ìˆ˜ ë“±)

---

## 5. ì‹¤ì œ êµ¬í˜„ ë§¤í•‘

### 5.1 Pipelineì—ì„œ Baseline ë¡œì§ ì¬í˜„

#### 5.1.1 ì„¤ì • íŒŒì¼ (config/experiments/baseline_exact.yaml)
```yaml
# Baselineê³¼ ì™„ì „íˆ ë™ì¼í•œ ì„¤ì •
general:
  model_name: "digit82/kobart-summarization"
  
tokenizer:
  encoder_max_len: 512
  decoder_max_len: 100
  special_tokens: 
    - '#Person1#'
    - '#Person2#' 
    - '#Person3#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#PassportNumber#'

training:
  output_dir: "./outputs"
  num_train_epochs: 20
  learning_rate: 1e-5
  per_device_train_batch_size: 50
  per_device_eval_batch_size: 32
  warmup_ratio: 0.1
  weight_decay: 0.01
  lr_scheduler_type: 'cosine'
  optim: 'adamw_torch'
  fp16: true
  evaluation_strategy: 'epoch'
  save_strategy: 'epoch'
  load_best_model_at_end: true
  metric_for_best_model: 'eval_loss'
  early_stopping_patience: 3

inference:
  batch_size: 32
  max_length: 100
  num_beams: 4
  no_repeat_ngram_size: 2
  early_stopping: true
  remove_tokens: ['<usr>', '<s>', '</s>', '<pad>']
```

#### 5.1.2 ì‹¤í–‰ ëª…ë ¹ì–´
```bash
# Baseline ë°©ì‹
python code/baseline.py

# Pipelineìœ¼ë¡œ ë™ì¼í•œ ê²°ê³¼ ì¬í˜„
python code/auto_experiment_runner.py \
    --config config/experiments/baseline_exact.yaml
```

### 5.2 ì£¼ìš” êµ¬í˜„ íŒŒì¼ ìœ„ì¹˜

| ê¸°ëŠ¥ | Baseline.py | Pipeline |
|------|------------|----------|
| **ë°ì´í„° ë¡œë“œ** | baseline.py:160-165 | utils/data_utils.py:load_dataset() |
| **ì „ì²˜ë¦¬** | baseline.py:174-203 | utils/data_utils.py:DataProcessor |
| **ëª¨ë¸ ë¡œë“œ** | baseline.py:384-395 | trainer.py:load_model() |
| **í•™ìŠµ** | baseline.py:321-373 | trainer.py:train() |
| **ì¶”ë¡ ** | baseline.py:493-542 | run_inference.py, post_training_inference.py |
| **ê²°ê³¼ ì €ì¥** | baseline.py:544-560 | utils/csv_results_saver.py |

### 5.3 ë°ì´í„° íë¦„ ê²€ì¦

```python
# Pipelineì—ì„œ Baselineê³¼ ë™ì¼í•œ ë°ì´í„° íë¦„ ë³´ì¥
assert train_df.shape == (12457, 3)  # train.csv
assert val_df.shape == (499, 3)      # dev.csv  
assert test_df.shape == (250, 2)     # test.csv (fname, dialogueë§Œ)

# ìƒì„±ëœ ê²°ê³¼ë„ ë™ì¼í•œ í˜•ì‹
assert output_df.shape == (250, 2)   # (fname, summary)
assert list(output_df.columns) == ['fname', 'summary']
```

---

## ê²°ë¡ 

1. **ë°ì´í„° íë¦„ì€ ì™„ì „íˆ ë™ì¼**: ë‘ ì‹œìŠ¤í…œ ëª¨ë‘ train.csvë¡œ í•™ìŠµí•˜ê³  test.csvë¡œ ì¶”ë¡ í•˜ì—¬ ë™ì¼í•œ í˜•ì‹ì˜ CSVë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

2. **Pipelineì˜ ì¥ì **:
   - í•™ìŠµ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ test ì¶”ë¡  ì‹¤í–‰
   - ì‹¤í—˜ë³„ë¡œ êµ¬ë¶„ëœ ê²°ê³¼ íŒŒì¼ ìƒì„±
   - ë‹¤ì–‘í•œ ìƒì„± ì „ëµ ì„ íƒ ê°€ëŠ¥
   - ë©”íƒ€ë°ì´í„° ë° ì¶”ì  ì •ë³´ í¬í•¨

3. **í†µí•© ì‚¬ìš© ë°©ë²•**:
   - ê°œë°œ ë‹¨ê³„: Baseline.pyë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
   - ì‹¤í—˜ ë‹¨ê³„: Pipelineìœ¼ë¡œ ë‹¤ì–‘í•œ ì„¤ì • í…ŒìŠ¤íŠ¸
   - ìµœì¢… ì œì¶œ: Pipelineì˜ best ëª¨ë¸ë¡œ ìµœì¢… ê²°ê³¼ ìƒì„±

4. **ì™„ì „í•œ í˜¸í™˜ì„±**: Pipelineì—ì„œ Baselineê³¼ ë™ì¼í•œ ì„¤ì •ì„ ì‚¬ìš©í•˜ë©´ ì™„ì „íˆ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
