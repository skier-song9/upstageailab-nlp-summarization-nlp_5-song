# NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œ - í”„ë¡œë•ì…˜ ë°°í¬ ê°€ì´ë“œ

## ëª©ì°¨
1. [ë°°í¬ ê°œìš”](#ë°°í¬-ê°œìš”)
2. [ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­](#ì‹œìŠ¤í…œ-ìš”êµ¬ì‚¬í•­)
3. [ê¸°ë³¸ ë°°í¬ ì„¤ì •](#ê¸°ë³¸-ë°°í¬-ì„¤ì •)
4. [FastAPI ì„œë²„ êµ¬ì„±](#fastapi-ì„œë²„-êµ¬ì„±)
5. [ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§](#ë©”íŠ¸ë¦­-ëª¨ë‹ˆí„°ë§)
6. [ë¡œê¹… ì‹œìŠ¤í…œ](#ë¡œê¹…-ì‹œìŠ¤í…œ)
7. [í—¬ìŠ¤ì²´í¬ êµ¬ì„±](#í—¬ìŠ¤ì²´í¬-êµ¬ì„±)
8. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
9. [ë³´ì•ˆ ì„¤ì •](#ë³´ì•ˆ-ì„¤ì •)
10. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ë°°í¬ ê°œìš”

ì´ ê°€ì´ë“œëŠ” NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œì„ í”„ë¡œë•ì…˜ í™˜ê²½ì— ì•ˆì „í•˜ê³  ì•ˆì •ì ìœ¼ë¡œ ë°°í¬í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤. Docker ì»¨í…Œì´ë„ˆí™”, ëª¨ë‹ˆí„°ë§, ë¡œê¹…, ë³´ì•ˆ ì„¤ì •ì„ í¬í•¨í•œ ëª¨ë“  ë°°í¬ ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- ğŸš€ **FastAPI ê¸°ë°˜ ê³ ì„±ëŠ¥ REST API**
- ğŸ“Š **Prometheus ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§**
- ğŸ“ **êµ¬ì¡°í™”ëœ JSON ë¡œê¹…**
- ğŸ’š **ì¢…í•©ì ì¸ í—¬ìŠ¤ì²´í¬**
- ğŸ”’ **API í‚¤ ê¸°ë°˜ ì¸ì¦**
- âš¡ **ìë™ ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”**

---

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ìµœì†Œ ìš”êµ¬ì‚¬í•­
- **CPU**: 4 cores (Intel/AMD x64 ë˜ëŠ” Apple Silicon)
- **RAM**: 8GB (ê¶Œì¥: 16GB+)
- **Storage**: 20GB ì—¬ìœ  ê³µê°„
- **OS**: Ubuntu 20.04+, macOS 12+, Windows 10+

### ê¶Œì¥ ìš”êµ¬ì‚¬í•­
- **GPU**: NVIDIA RTX 3070+ (CUDA 11.8+) ë˜ëŠ” Apple M1/M2
- **RAM**: 32GB
- **Storage**: SSD 50GB+

### í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´
```bash
# Docker
docker --version  # Docker 20.0+

# Python
python --version  # Python 3.8+

# Git
git --version
```

---

## ê¸°ë³¸ ë°°í¬ ì„¤ì •

### 1. í”„ë¡œì íŠ¸ ì¤€ë¹„

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone <your-repo-url>
cd nlp-sum-lyj

# í™˜ê²½ ì„¤ì •
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
# API ì„¤ì •
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# ëª¨ë¸ ì„¤ì •
MODEL_PATH=outputs/best_model
MAX_BATCH_SIZE=16
DEVICE=auto

# ë³´ì•ˆ ì„¤ì •
API_KEYS=your-secret-key-1,your-secret-key-2
RATE_LIMIT_PER_MINUTE=100

# ëª¨ë‹ˆí„°ë§ ì„¤ì •
METRICS_PORT=9090
LOG_LEVEL=INFO
ENABLE_PROMETHEUS=true

# ë°ì´í„°ë² ì´ìŠ¤ (ì„ íƒì‚¬í•­)
DATABASE_URL=sqlite:///./app.db
EOF
```

### 3. Docker ì´ë¯¸ì§€ ë¹Œë“œ

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„±
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„±
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY . .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000 9090

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ì‹¤í–‰
CMD ["python", "deployment/main.py"]
```

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t nlp-summarization-api:latest .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
    --name nlp-api \
    -p 8000:8000 \
    -p 9090:9090 \
    --env-file .env \
    --restart unless-stopped \
    nlp-summarization-api:latest
```

---

## FastAPI ì„œë²„ êµ¬ì„±

### 1. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¡°

```python
# deployment/main.py
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
import uvicorn
import os
from contextlib import asynccontextmanager

from api.routes import router
from api.middleware import setup_middleware
from monitoring.metrics import setup_metrics
from core.inference import InferenceEngine
from utils.logging import setup_logging

# ì „ì—­ ìƒíƒœ ê´€ë¦¬
app_state = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    setup_logging()
    logger.info("ğŸš€ NLP ìš”ì•½ API ì‹œì‘")
    
    # ëª¨ë¸ ë¡œë”©
    model_path = os.getenv("MODEL_PATH", "outputs/best_model")
    app_state["inference_engine"] = InferenceEngine(model_path)
    logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_path}")
    
    # ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
    if os.getenv("ENABLE_PROMETHEUS", "true").lower() == "true":
        metrics_port = int(os.getenv("METRICS_PORT", "9090"))
        setup_metrics(metrics_port)
        logger.info(f"ğŸ“Š ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘: í¬íŠ¸ {metrics_port}")
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    logger.info("ğŸ›‘ NLP ìš”ì•½ API ì¢…ë£Œ")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="NLP ëŒ€í™” ìš”ì•½ API",
    description="ëŒ€í™” í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” REST API",
    version="1.0.0",
    lifespan=lifespan
)

# ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ í•„ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì»¤ìŠ¤í…€ ë¯¸ë“¤ì›¨ì–´
setup_middleware(app)

# ë¼ìš°í„° ë“±ë¡
app.include_router(router, prefix="/api/v1")

# ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {
        "service": "NLP ëŒ€í™” ìš”ì•½ API",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs"
    }

if __name__ == "__main__":
    host = os.getenv("API_HOST", "0.0.0.0")
    port = int(os.getenv("API_PORT", "8000"))
    workers = int(os.getenv("API_WORKERS", "1"))
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        workers=workers,
        reload=False,
        access_log=True
    )
```

### 2. API ë¼ìš°í„° êµ¬ì„±

```python
# deployment/api/routes.py
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from pydantic import BaseModel, Field
from typing import List, Optional
import time

from .auth import get_api_key, rate_limiter
from .models import SummarizeRequest, SummarizeResponse, BatchSummarizeRequest
from ..monitoring.metrics import REQUEST_COUNT, REQUEST_LATENCY

router = APIRouter()

class SummarizeRequest(BaseModel):
    dialogue: str = Field(..., min_length=10, max_length=10000, description="ìš”ì•½í•  ëŒ€í™” í…ìŠ¤íŠ¸")
    max_length: Optional[int] = Field(100, ge=10, le=512, description="ìš”ì•½ ìµœëŒ€ ê¸¸ì´")
    min_length: Optional[int] = Field(10, ge=1, le=100, description="ìš”ì•½ ìµœì†Œ ê¸¸ì´")
    num_beams: Optional[int] = Field(4, ge=1, le=10, description="ë¹” ì„œì¹˜ í¬ê¸°")

class SummarizeResponse(BaseModel):
    summary: str = Field(..., description="ìƒì„±ëœ ìš”ì•½ë¬¸")
    original_length: int = Field(..., description="ì›ë³¸ ëŒ€í™” ê¸¸ì´")
    summary_length: int = Field(..., description="ìš”ì•½ë¬¸ ê¸¸ì´")
    processing_time: float = Field(..., description="ì²˜ë¦¬ ì‹œê°„(ì´ˆ)")

@router.post("/summarize", response_model=SummarizeResponse)
async def summarize_dialogue(
    request: SummarizeRequest,
    background_tasks: BackgroundTasks,
    api_key: str = Depends(get_api_key)
):
    """ë‹¨ì¼ ëŒ€í™” ìš”ì•½"""
    
    # ì†ë„ ì œí•œ í™•ì¸
    await rate_limiter.check_rate_limit(api_key)
    
    start_time = time.time()
    
    try:
        # ì¶”ë¡  ì‹¤í–‰
        inference_engine = app_state["inference_engine"]
        summary = inference_engine.predict_single(
            request.dialogue,
            max_length=request.max_length,
            min_length=request.min_length,
            num_beams=request.num_beams
        )
        
        processing_time = time.time() - start_time
        
        # ë©”íŠ¸ë¦­ ê¸°ë¡
        REQUEST_COUNT.labels(method="POST", endpoint="summarize", status="success").inc()
        REQUEST_LATENCY.observe(processing_time)
        
        return SummarizeResponse(
            summary=summary,
            original_length=len(request.dialogue),
            summary_length=len(summary),
            processing_time=round(processing_time, 3)
        )
        
    except Exception as e:
        REQUEST_COUNT.labels(method="POST", endpoint="summarize", status="error").inc()
        raise HTTPException(status_code=500, detail=f"ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.post("/batch-summarize")
async def batch_summarize(
    request: BatchSummarizeRequest,
    api_key: str = Depends(get_api_key)
):
    """ë°°ì¹˜ ëŒ€í™” ìš”ì•½"""
    
    # ë°°ì¹˜ í¬ê¸° ì œí•œ
    if len(request.dialogues) > 50:
        raise HTTPException(status_code=400, detail="ë°°ì¹˜ í¬ê¸°ëŠ” ìµœëŒ€ 50ê°œê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    await rate_limiter.check_rate_limit(api_key, multiplier=len(request.dialogues))
    
    start_time = time.time()
    
    try:
        inference_engine = app_state["inference_engine"]
        summaries = inference_engine.predict_batch(
            request.dialogues,
            batch_size=request.batch_size or 8
        )
        
        processing_time = time.time() - start_time
        
        REQUEST_COUNT.labels(method="POST", endpoint="batch-summarize", status="success").inc()
        
        return {
            "summaries": summaries,
            "count": len(summaries),
            "processing_time": round(processing_time, 3)
        }
        
    except Exception as e:
        REQUEST_COUNT.labels(method="POST", endpoint="batch-summarize", status="error").inc()
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
```

---

## ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§

### 1. Prometheus ë©”íŠ¸ë¦­ ì„¤ì •

```python
# deployment/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import functools
import time

# ë©”íŠ¸ë¦­ ì •ì˜
REQUEST_COUNT = Counter(
    'nlp_api_requests_total',
    'Total API requests',
    ['method', 'endpoint', 'status']
)

REQUEST_LATENCY = Histogram(
    'nlp_api_request_duration_seconds',
    'Request duration in seconds',
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, float('inf')]
)

MODEL_MEMORY_USAGE = Gauge(
    'nlp_api_model_memory_bytes',
    'Model memory usage in bytes'
)

ACTIVE_CONNECTIONS = Gauge(
    'nlp_api_active_connections',
    'Number of active connections'
)

def monitor_performance(func):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        status = "success"
        
        try:
            result = await func(*args, **kwargs)
            return result
        except Exception as e:
            status = "error"
            raise
        finally:
            duration = time.time() - start_time
            REQUEST_LATENCY.observe(duration)
            REQUEST_COUNT.labels(
                method="POST", 
                endpoint=func.__name__, 
                status=status
            ).inc()
    
    return wrapper

def start_metrics_server(port=9090):
    """ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘"""
    start_http_server(port)
    print(f"ğŸ“Š ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘: í¬íŠ¸ {port}")

def setup_metrics(port=9090):
    """ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    start_metrics_server(port)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    import threading
    import psutil
    import torch
    
    def update_memory_metrics():
        while True:
            try:
                # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
                process = psutil.Process()
                memory_bytes = process.memory_info().rss
                MODEL_MEMORY_USAGE.set(memory_bytes)
                
                time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
            except:
                pass
    
    thread = threading.Thread(target=update_memory_metrics, daemon=True)
    thread.start()
```

### 2. Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •

```json
{
  "dashboard": {
    "title": "NLP ìš”ì•½ API ëª¨ë‹ˆí„°ë§",
    "panels": [
      {
        "title": "ì´ˆë‹¹ ìš”ì²­ ìˆ˜",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(nlp_api_requests_total[5m])",
            "legendFormat": "{{status}}"
          }
        ]
      },
      {
        "title": "ì‘ë‹µ ì‹œê°„ ë¶„í¬",
        "type": "graph", 
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(nlp_api_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(nlp_api_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰",
        "type": "graph",
        "targets": [
          {
            "expr": "nlp_api_model_memory_bytes / 1024 / 1024",
            "legendFormat": "Memory (MB)"
          }
        ]
      }
    ]
  }
}
```

---

## ë¡œê¹… ì‹œìŠ¤í…œ

### 1. êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
# deployment/utils/logging.py
import logging
import json
import time
from typing import Dict, Any
from datetime import datetime

class StructuredLogger:
    """êµ¬ì¡°í™”ëœ JSON ë¡œê¹… í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, level: str = "INFO"):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, level.upper()))
        
        # JSON í¬ë§·í„° ì„¤ì •
        formatter = logging.Formatter('%(message)s')
        
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    
    def _format_log(self, level: str, message: str, **kwargs) -> str:
        """ë¡œê·¸ ë©”ì‹œì§€ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": level,
            "message": message,
            "service": "nlp-summarization-api",
            **kwargs
        }
        return json.dumps(log_entry, ensure_ascii=False)
    
    def info(self, message: str, **kwargs):
        self.logger.info(self._format_log("INFO", message, **kwargs))
    
    def error(self, message: str, **kwargs):
        self.logger.error(self._format_log("ERROR", message, **kwargs))
    
    def warning(self, message: str, **kwargs):
        self.logger.warning(self._format_log("WARNING", message, **kwargs))

# ì „ì—­ ë¡œê±°
logger = StructuredLogger("api")

def setup_logging():
    """ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    import os
    
    log_level = os.getenv("LOG_LEVEL", "INFO")
    
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    file_handler = logging.FileHandler("logs/api.log")
    file_handler.setFormatter(logging.Formatter('%(message)s'))
    
    logger.logger.addHandler(file_handler)
    logger.info("ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ", log_level=log_level)

def log_request(request_id: str, endpoint: str, duration: float, status: str):
    """API ìš”ì²­ ë¡œê¹…"""
    logger.info(
        "API ìš”ì²­ ì™„ë£Œ",
        request_id=request_id,
        endpoint=endpoint,
        duration=duration,
        status=status
    )
```

### 2. ìš”ì²­ ì¶”ì  ë¯¸ë“¤ì›¨ì–´

```python
# deployment/api/middleware.py
import uuid
import time
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware

class RequestTrackingMiddleware(BaseHTTPMiddleware):
    """ìš”ì²­ ì¶”ì  ë¯¸ë“¤ì›¨ì–´"""
    
    async def dispatch(self, request: Request, call_next):
        # ìš”ì²­ ID ìƒì„±
        request_id = str(uuid.uuid4())[:8]
        request.state.request_id = request_id
        
        start_time = time.time()
        
        # ìš”ì²­ ë¡œê¹…
        logger.info(
            "API ìš”ì²­ ì‹œì‘",
            request_id=request_id,
            method=request.method,
            url=str(request.url),
            client_ip=request.client.host
        )
        
        # ì‘ë‹µ ì²˜ë¦¬
        try:
            response = await call_next(request)
            status = "success"
            status_code = response.status_code
        except Exception as e:
            status = "error"
            status_code = 500
            logger.error(
                "API ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜",
                request_id=request_id,
                error=str(e)
            )
            raise
        finally:
            duration = time.time() - start_time
            
            # ì‘ë‹µ ë¡œê¹…
            logger.info(
                "API ìš”ì²­ ì™„ë£Œ",
                request_id=request_id,
                status=status,
                status_code=status_code,
                duration=round(duration, 3)
            )
        
        # ì‘ë‹µ í—¤ë”ì— ìš”ì²­ ID ì¶”ê°€
        response.headers["X-Request-ID"] = request_id
        return response

def setup_middleware(app):
    """ë¯¸ë“¤ì›¨ì–´ ì„¤ì •"""
    app.add_middleware(RequestTrackingMiddleware)
```

---

## í—¬ìŠ¤ì²´í¬ êµ¬ì„±

### 1. ì¢…í•© í—¬ìŠ¤ì²´í¬

```python
# deployment/api/health.py
from fastapi import APIRouter
from typing import Dict, Any
import torch
import psutil
import time
from pathlib import Path

router = APIRouter()

class HealthChecker:
    """ì¢…í•©ì ì¸ í—¬ìŠ¤ì²´í¬ í´ë˜ìŠ¤"""
    
    def __init__(self, inference_engine=None):
        self.inference_engine = inference_engine
        self.startup_time = time.time()
    
    async def comprehensive_health_check(self) -> Dict[str, Any]:
        """ì¢…í•© í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰"""
        checks = {
            "system": await self._check_system_health(),
            "model": await self._check_model_health(),
            "dependencies": await self._check_dependencies(),
            "performance": await self._check_performance()
        }
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        overall_status = "healthy"
        for check_name, check_result in checks.items():
            if not check_result.get("healthy", True):
                overall_status = "unhealthy"
                break
        
        return {
            "status": overall_status,
            "timestamp": time.time(),
            "uptime": time.time() - self.startup_time,
            "checks": checks
        }
    
    async def _check_system_health(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬"""
        try:
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            memory_healthy = memory.percent < 90
            disk_healthy = disk.percent < 90
            
            return {
                "healthy": memory_healthy and disk_healthy,
                "memory_usage_percent": memory.percent,
                "disk_usage_percent": disk.percent,
                "available_memory_gb": round(memory.available / (1024**3), 2)
            }
        except Exception as e:
            return {"healthy": False, "error": str(e)}
    
    async def _check_model_health(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ì²´í¬"""
        try:
            if self.inference_engine is None:
                return {"healthy": False, "error": "Model not loaded"}
            
            # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
            test_dialogue = "í™”ì1: ì•ˆë…•í•˜ì„¸ìš”\ní™”ì2: ì•ˆë…•í•˜ì„¸ìš”"
            start_time = time.time()
            result = self.inference_engine.predict_single(test_dialogue)
            inference_time = time.time() - start_time
            
            # GPU ë©”ëª¨ë¦¬ ì²´í¬ (ì‚¬ìš© ì¤‘ì¸ ê²½ìš°)
            gpu_info = {}
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated(0)
                gpu_total = torch.cuda.get_device_properties(0).total_memory
                gpu_usage_percent = (gpu_memory / gpu_total) * 100
                
                gpu_info = {
                    "gpu_memory_usage_percent": round(gpu_usage_percent, 2),
                    "gpu_memory_used_gb": round(gpu_memory / (1024**3), 2),
                    "gpu_memory_total_gb": round(gpu_total / (1024**3), 2)
                }
            
            return {
                "healthy": True,
                "inference_time_seconds": round(inference_time, 3),
                "test_result_length": len(result) if result else 0,
                **gpu_info
            }
        except Exception as e:
            return {"healthy": False, "error": str(e)}
    
    async def _check_dependencies(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬"""
        try:
            import torch
            import transformers
            import fastapi
            
            return {
                "healthy": True,
                "torch_version": torch.__version__,
                "transformers_version": transformers.__version__,
                "fastapi_version": fastapi.__version__,
                "cuda_available": torch.cuda.is_available()
            }
        except ImportError as e:
            return {"healthy": False, "error": f"Missing dependency: {e}"}
    
    async def _check_performance(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì§€í‘œ ì²´í¬"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë©”íŠ¸ë¦­ ì €ì¥ì†Œì—ì„œ ë°ì´í„° ì¡°íšŒ
        return {
            "healthy": True,
            "note": "Performance metrics collection needed"
        }

# ì „ì—­ í—¬ìŠ¤ì²´ì»¤
health_checker = None

@router.get("/health")
async def basic_health_check():
    """ê¸°ë³¸ í—¬ìŠ¤ì²´í¬"""
    return {"status": "healthy", "timestamp": time.time()}

@router.get("/health/detailed")
async def detailed_health_check():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    global health_checker
    if health_checker is None:
        # app_stateì—ì„œ inference_engine ê°€ì ¸ì˜¤ê¸°
        from ..main import app_state
        health_checker = HealthChecker(app_state.get("inference_engine"))
    
    return await health_checker.comprehensive_health_check()

@router.get("/health/ready")
async def readiness_check():
    """ì¤€ë¹„ ìƒíƒœ ì²´í¬ (Kubernetesìš©)"""
    global health_checker
    if health_checker is None:
        from ..main import app_state
        health_checker = HealthChecker(app_state.get("inference_engine"))
    
    result = await health_checker.comprehensive_health_check()
    
    if result["status"] == "healthy":
        return {"status": "ready"}
    else:
        from fastapi import HTTPException
        raise HTTPException(status_code=503, detail="Service not ready")
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ëª¨ë¸ ìµœì í™”

```bash
# ëª¨ë¸ ì–‘ìí™” (ì˜µì…˜)
python scripts/quantize_model.py \
    --model-path outputs/best_model \
    --output-path outputs/quantized_model \
    --quantization-type int8

# ìµœì í™”ëœ ëª¨ë¸ë¡œ ë°°í¬
export MODEL_PATH=outputs/quantized_model
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
# deployment/optimization/batch_processor.py
import asyncio
from typing import List
from collections import deque
import time

class AdaptiveBatchProcessor:
    """ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì • í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, max_batch_size: int = 16, max_wait_time: float = 0.1):
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = deque()
        self.processing = False
    
    async def add_request(self, dialogue: str, response_future: asyncio.Future):
        """ìš”ì²­ì„ ë°°ì¹˜ì— ì¶”ê°€"""
        self.pending_requests.append((dialogue, response_future, time.time()))
        
        if not self.processing:
            asyncio.create_task(self._process_batch())
    
    async def _process_batch(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        if self.processing:
            return
        
        self.processing = True
        
        try:
            # ëŒ€ê¸° ì‹œê°„ ë˜ëŠ” ìµœëŒ€ í¬ê¸°ê¹Œì§€ ê¸°ë‹¤ë¦¼
            await asyncio.sleep(self.max_wait_time)
            
            # ë°°ì¹˜ êµ¬ì„±
            batch = []
            futures = []
            
            while self.pending_requests and len(batch) < self.max_batch_size:
                dialogue, future, timestamp = self.pending_requests.popleft()
                batch.append(dialogue)
                futures.append(future)
            
            if batch:
                # ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰
                from ..main import app_state
                inference_engine = app_state["inference_engine"]
                results = inference_engine.predict_batch(batch)
                
                # ê²°ê³¼ ì „ë‹¬
                for future, result in zip(futures, results):
                    if not future.cancelled():
                        future.set_result(result)
                        
        except Exception as e:
            # ì—ëŸ¬ë¥¼ ëª¨ë“  futureì— ì „íŒŒ
            for future in futures:
                if not future.cancelled():
                    future.set_exception(e)
        finally:
            self.processing = False
            
            # ë‚¨ì€ ìš”ì²­ì´ ìˆìœ¼ë©´ ë‹¤ìŒ ë°°ì¹˜ ì²˜ë¦¬
            if self.pending_requests:
                asyncio.create_task(self._process_batch())
```

### 3. ìºì‹± ì‹œìŠ¤í…œ

```python
# deployment/caching/cache_manager.py
import hashlib
import time
from typing import Optional
import redis

class CacheManager:
    """Redis ê¸°ë°˜ ìºì‹œ ë§¤ë‹ˆì €"""
    
    def __init__(self, redis_url: Optional[str] = None, ttl_seconds: int = 3600):
        self.ttl_seconds = ttl_seconds
        
        if redis_url:
            self.redis_client = redis.from_url(redis_url)
        else:
            self.redis_client = None
    
    def _generate_key(self, dialogue: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"summary:{hashlib.md5(dialogue.encode()).hexdigest()}"
    
    async def get(self, dialogue: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ì¡°íšŒ"""
        if not self.redis_client:
            return None
        
        key = self._generate_key(dialogue)
        result = self.redis_client.get(key)
        
        return result.decode('utf-8') if result else None
    
    async def set(self, dialogue: str, summary: str):
        """ìºì‹œì— ì €ì¥"""
        if not self.redis_client:
            return
        
        key = self._generate_key(dialogue)
        self.redis_client.setex(key, self.ttl_seconds, summary)
```

---

## ë³´ì•ˆ ì„¤ì •

### 1. API í‚¤ ì¸ì¦

```python
# deployment/api/auth.py
from fastapi import HTTPException, Security, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import time
import os

security = HTTPBearer()

def get_api_key(credentials: HTTPAuthorizationCredentials = Security(security)):
    """API í‚¤ ê²€ì¦"""
    valid_keys = os.getenv("API_KEYS", "").split(",")
    
    if credentials.credentials not in valid_keys:
        raise HTTPException(
            status_code=401,
            detail="Invalid API key"
        )
    return credentials.credentials

class RateLimiter:
    """ì†ë„ ì œí•œê¸°"""
    
    def __init__(self, requests_per_minute: int = 60):
        self.requests_per_minute = requests_per_minute
        self.requests = {}  # {api_key: [timestamp, ...]}
    
    async def check_rate_limit(self, api_key: str, multiplier: int = 1):
        """ì†ë„ ì œí•œ í™•ì¸"""
        current_time = time.time()
        minute_ago = current_time - 60
        
        # ì´ì „ ìš”ì²­ ê¸°ë¡ ì •ë¦¬
        if api_key in self.requests:
            self.requests[api_key] = [
                req_time for req_time in self.requests[api_key]
                if req_time > minute_ago
            ]
        else:
            self.requests[api_key] = []
        
        # í˜„ì¬ ìš”ì²­ ìˆ˜ í™•ì¸
        if len(self.requests[api_key]) >= self.requests_per_minute:
            raise HTTPException(
                status_code=429,
                detail="Rate limit exceeded"
            )
        
        # í˜„ì¬ ìš”ì²­ ê¸°ë¡
        for _ in range(multiplier):
            self.requests[api_key].append(current_time)

# ì „ì—­ ì†ë„ ì œí•œê¸°
rate_limit_per_minute = int(os.getenv("RATE_LIMIT_PER_MINUTE", "100"))
rate_limiter = RateLimiter(rate_limit_per_minute)
```

### 2. ì…ë ¥ ê²€ì¦

```python
# deployment/api/validation.py
import re
from fastapi import HTTPException

def validate_dialogue_input(dialogue: str) -> str:
    """ëŒ€í™” ì…ë ¥ ê²€ì¦ ë° ì •ì œ"""
    
    # ê¸¸ì´ ê²€ì¦
    if len(dialogue) > 10000:
        raise HTTPException(
            status_code=400,
            detail="ëŒ€í™” í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ (ìµœëŒ€ 10,000ì)"
        )
    
    if len(dialogue.strip()) < 10:
        raise HTTPException(
            status_code=400,
            detail="ëŒ€í™” í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 10ì)"
        )
    
    # ì•…ì„± íŒ¨í„´ ê²€ì‚¬
    malicious_patterns = [
        r'<script.*?>.*?</script>',
        r'javascript:',
        r'data:text/html',
        r'vbscript:',
    ]
    
    for pattern in malicious_patterns:
        if re.search(pattern, dialogue, re.IGNORECASE | re.DOTALL):
            raise HTTPException(
                status_code=400,
                detail="ì˜ëª»ëœ ì…ë ¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤"
            )
    
    # ê¸°ë³¸ ì •ë¦¬
    dialogue = re.sub(r'<[^>]+>', '', dialogue)  # HTML íƒœê·¸ ì œê±°
    dialogue = re.sub(r'\s+', ' ', dialogue).strip()  # ê³µë°± ì •ë¦¬
    
    return dialogue
```

---

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë°°í¬ ë¬¸ì œ

#### 1. ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹¤íŒ¨
```bash
# ë¡œê·¸ í™•ì¸
docker logs nlp-api

# ì¼ë°˜ì ì¸ ì›ì¸ê³¼ í•´ê²°ì±…:
# - ëª¨ë¸ íŒŒì¼ ì—†ìŒ: outputs/best_model ë””ë ‰í† ë¦¬ í™•ì¸
# - í¬íŠ¸ ì¶©ëŒ: ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
# - ë©”ëª¨ë¦¬ ë¶€ì¡±: ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
```

#### 2. ë©”ëª¨ë¦¬ ë¶€ì¡±
```yaml
# docker-compose.ymlì—ì„œ ë©”ëª¨ë¦¬ ì œí•œ
services:
  nlp-api:
    image: nlp-summarization-api:latest
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
```

#### 3. ì„±ëŠ¥ ì´ìŠˆ
```bash
# CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
docker stats nlp-api

# GPU ì‚¬ìš©ë¥  í™•ì¸ (NVIDIA GPU)
nvidia-smi
```

### ë¡œê·¸ ë¶„ì„

```bash
# API ë¡œê·¸ í™•ì¸
tail -f logs/api.log

# ì—ëŸ¬ ë¡œê·¸ë§Œ í•„í„°ë§
grep "ERROR" logs/api.log

# íŠ¹ì • ìš”ì²­ ID ì¶”ì 
grep "abc12345" logs/api.log
```

### ì„±ëŠ¥ íŠœë‹

1. **ë°°ì¹˜ í¬ê¸° ìµœì í™”**
   - GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ vs ì²˜ë¦¬ ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„

2. **ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì¡°ì •**
   - CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì„¤ì •
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤

3. **ìºì‹± í™œìš©**
   - ìì£¼ ìš”ì²­ë˜ëŠ” ë‚´ìš© ìºì‹±
   - Redis ë˜ëŠ” ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©

---

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ëª¨ë‹ˆí„°ë§ ì„¤ì •

```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana

volumes:
  grafana-storage:
```

### ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# scripts/deploy.sh

echo "ğŸš€ NLP ìš”ì•½ API ë°°í¬ ì‹œì‘"

# í™˜ê²½ í™•ì¸
if [ ! -f ".env" ]; then
    echo "âŒ .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"
    exit 1
fi

# ì´ë¯¸ì§€ ë¹Œë“œ
echo "ğŸ”¨ Docker ì´ë¯¸ì§€ ë¹Œë“œ..."
docker build -t nlp-summarization-api:latest .

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ì§€
echo "ğŸ›‘ ê¸°ì¡´ ì„œë¹„ìŠ¤ ì •ì§€..."
docker stop nlp-api 2>/dev/null || true
docker rm nlp-api 2>/dev/null || true

# ìƒˆ ì»¨í…Œì´ë„ˆ ì‹œì‘
echo "ğŸš€ ìƒˆ ì„œë¹„ìŠ¤ ì‹œì‘..."
docker run -d \
    --name nlp-api \
    -p 8000:8000 \
    -p 9090:9090 \
    --env-file .env \
    --restart unless-stopped \
    -v $(pwd)/outputs:/app/outputs \
    nlp-summarization-api:latest

# í—¬ìŠ¤ì²´í¬
echo "ğŸ’š í—¬ìŠ¤ì²´í¬ ëŒ€ê¸°..."
sleep 10

if curl -f http://localhost:8000/health > /dev/null 2>&1; then
    echo "âœ… ë°°í¬ ì„±ê³µ!"
    echo "ğŸ“Š API ë¬¸ì„œ: http://localhost:8000/docs"
    echo "ğŸ“ˆ ë©”íŠ¸ë¦­: http://localhost:9090"
else
    echo "âŒ ë°°í¬ ì‹¤íŒ¨ - ë¡œê·¸ í™•ì¸ í•„ìš”"
    docker logs nlp-api
    exit 1
fi
```

ì´ í”„ë¡œë•ì…˜ ë°°í¬ ê°€ì´ë“œë¥¼ í†µí•´ ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ NLP ìš”ì•½ ì„œë¹„ìŠ¤ë¥¼ ìš´ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ì„¹ì…˜ì˜ ì½”ë“œëŠ” ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
