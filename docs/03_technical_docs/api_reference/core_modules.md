# NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œ - í•µì‹¬ ëª¨ë“ˆ API ì°¸ì¡°

## ëª©ì°¨
1. [API ê°œìš”](#api-ê°œìš”)
2. [ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ](#ë°ì´í„°-ì²˜ë¦¬-ëª¨ë“ˆ)
3. [ë©”íŠ¸ë¦­ ê³„ì‚° ëª¨ë“ˆ](#ë©”íŠ¸ë¦­-ê³„ì‚°-ëª¨ë“ˆ)
4. [ì¶”ë¡  ì—”ì§„ ëª¨ë“ˆ](#ì¶”ë¡ -ì—”ì§„-ëª¨ë“ˆ)
5. [ê²½ë¡œ ê´€ë¦¬ ëª¨ë“ˆ](#ê²½ë¡œ-ê´€ë¦¬-ëª¨ë“ˆ)
6. [ì‹¤í—˜ ê´€ë¦¬ ëª¨ë“ˆ](#ì‹¤í—˜-ê´€ë¦¬-ëª¨ë“ˆ)
7. [ë””ë°”ì´ìŠ¤ ìµœì í™” ëª¨ë“ˆ](#ë””ë°”ì´ìŠ¤-ìµœì í™”-ëª¨ë“ˆ)
8. [ì‚¬ìš© ì˜ˆì œ](#ì‚¬ìš©-ì˜ˆì œ)
9. [ì—ëŸ¬ ì²˜ë¦¬](#ì—ëŸ¬-ì²˜ë¦¬)
10. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)

---

## API ê°œìš”

ì´ API ì°¸ì¡° ë¬¸ì„œëŠ” NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œì˜ í•µì‹¬ ëª¨ë“ˆë“¤ì— ëŒ€í•œ ì™„ì „í•œ ê¸°ìˆ  ë¬¸ì„œì…ë‹ˆë‹¤. ëª¨ë“  APIëŠ” **ìƒëŒ€ ê²½ë¡œ ê¸°ë°˜**ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, **í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„±**(macOS MPS, Ubuntu CUDA)ì„ ì§€ì›í•©ë‹ˆë‹¤.

### ì„¤ê³„ ì›ì¹™
- **ğŸ”— ëª¨ë“ˆí™”**: ê° ê¸°ëŠ¥ì´ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
- **ğŸ“‚ ìƒëŒ€ ê²½ë¡œ**: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ê°•ì œ
- **âš¡ ìë™ ìµœì í™”**: ë””ë°”ì´ìŠ¤ë³„ ìë™ ì„¤ì • ë° ìµœì í™”
- **ğŸ›¡ï¸ íƒ€ì… ì•ˆì „ì„±**: ì™„ì „í•œ íƒ€ì… íŒíŠ¸ ì§€ì›
- **ğŸ“Š ì‹¤í—˜ ì¶”ì **: ëª¨ë“  ì‘ì—…ì˜ ë©”íƒ€ë°ì´í„° ìë™ ê¸°ë¡

### ì£¼ìš” íŠ¹ì§•
- **Multi-reference ROUGE** ì™„ì „ ì§€ì›
- **MPS/CUDA ìë™ ê°ì§€** ë° ìµœì í™”
- **ë°°ì¹˜ ì²˜ë¦¬** ë° **ë©”ëª¨ë¦¬ ìµœì í™”**
- **ì‹¤í—˜ ì¶”ì ** ë° **ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬**
- **ëŒ€íšŒ ì œì¶œ í˜•ì‹** ì™„ë²½ ì§€ì›

---

## ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ

### `DataProcessor` í´ë˜ìŠ¤

ëŒ€í™” ìš”ì•½ ë°ì´í„°ì˜ ë¡œë”©, ì „ì²˜ë¦¬, ì €ì¥ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

#### í´ë˜ìŠ¤ ì •ì˜

```python
class DataProcessor:
    """ë‹¤ì¤‘ ì°¸ì¡° ìš”ì•½ ë°ì´í„° ì „ìš© í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, project_root: Optional[Union[str, Path]] = None):
        """
        DataProcessor ì´ˆê¸°í™”
        
        Args:
            project_root: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (Noneì‹œ ìë™ ê°ì§€)
        
        Raises:
            ValueError: ì ˆëŒ€ ê²½ë¡œ ì…ë ¥ ì‹œ
        """
```

#### ì£¼ìš” ë©”ì„œë“œ

##### `load_multi_reference_data()`

```python
def load_multi_reference_data(self, file_path: Union[str, Path]) -> pd.DataFrame:
    """
    ë‹¤ì¤‘ ì°¸ì¡° ìš”ì•½ ë°ì´í„° ë¡œë”©
    
    ì§€ì›í•˜ëŠ” ë°ì´í„° í˜•ì‹:
    1. ê°œë³„ ì»¬ëŸ¼: summary1, summary2, summary3
    2. êµ¬ë¶„ì ë¶„ë¦¬: summary ì»¬ëŸ¼ì— ||| êµ¬ë¶„ìë¡œ ë¶„ë¦¬
    3. JSON í˜•ì‹: summary ì»¬ëŸ¼ì— JSON ë°°ì—´
    
    Args:
        file_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ í•„ìˆ˜)
    
    Returns:
        pd.DataFrame: í‘œì¤€í™”ëœ ë°ì´í„°í”„ë ˆì„
                     í•„ìˆ˜ ì»¬ëŸ¼: fname, dialogue, summaries
    
    Raises:
        ValueError: ì ˆëŒ€ ê²½ë¡œ ì…ë ¥ ì‹œ
        FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ì‹œ
        pd.errors.EmptyDataError: ë¹ˆ íŒŒì¼ì¼ ì‹œ
    
    Example:
        >>> processor = DataProcessor()
        >>> df = processor.load_multi_reference_data("data/train.csv")
        >>> print(f"ë¡œë”©ëœ ìƒ˜í”Œ: {len(df)}")
        >>> print(f"ì»¬ëŸ¼: {df.columns.tolist()}")
    """
```

##### `export_submission_format()`

```python
def export_submission_format(self, 
                           predictions: List[str],
                           fnames: List[str],
                           output_path: Union[str, Path]) -> pd.DataFrame:
    """
    ëŒ€íšŒ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì €ì¥
    
    Args:
        predictions: ì˜ˆì¸¡ëœ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
        fnames: íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
    
    Returns:
        pd.DataFrame: ì œì¶œ í˜•ì‹ ë°ì´í„°í”„ë ˆì„ (fname, summary)
    
    Raises:
        ValueError: ì˜ˆì¸¡ê³¼ íŒŒì¼ëª… ê°œìˆ˜ ë¶ˆì¼ì¹˜ ì‹œ
        ValueError: ì ˆëŒ€ ê²½ë¡œ ì…ë ¥ ì‹œ
    
    Example:
        >>> predictions = ["ìš”ì•½ë¬¸1", "ìš”ì•½ë¬¸2"]
        >>> fnames = ["file1.txt", "file2.txt"]
        >>> result_df = processor.export_submission_format(
        ...     predictions, fnames, "outputs/submission.csv"
        ... )
        >>> print(f"ì œì¶œ íŒŒì¼ ìƒì„±: {len(result_df)} í•­ëª©")
    """
```

##### `validate_submission_format()`

```python
def validate_submission_format(self, file_path: Union[str, Path]) -> bool:
    """
    ì œì¶œ íŒŒì¼ í˜•ì‹ ê²€ì¦
    
    Args:
        file_path: ê²€ì¦í•  íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
    
    Returns:
        bool: ìœ íš¨í•œ í˜•ì‹ì¸ì§€ ì—¬ë¶€
    
    Example:
        >>> is_valid = processor.validate_submission_format("outputs/submission.csv")
        >>> print(f"ì œì¶œ íŒŒì¼ ìœ íš¨ì„±: {'PASS' if is_valid else 'FAIL'}")
    """
```

---

## ë©”íŠ¸ë¦­ ê³„ì‚° ëª¨ë“ˆ

### `RougeCalculator` í´ë˜ìŠ¤

Multi-reference ROUGE ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ì „ìš© í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

#### í´ë˜ìŠ¤ ì •ì˜

```python
class RougeCalculator:
    """ë‹¤ì¤‘ ì°¸ì¡° ROUGE ê³„ì‚°ê¸°"""
    
    def __init__(self, use_korean_tokenizer: bool = True):
        """
        RougeCalculator ì´ˆê¸°í™”
        
        Args:
            use_korean_tokenizer: í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš© ì—¬ë¶€
        """
```

#### ì£¼ìš” ë©”ì„œë“œ

##### `compute_multi_reference_rouge()`

```python
def compute_multi_reference_rouge(self, 
                                predictions: List[str],
                                references_list: List[List[str]]) -> Dict[str, Dict[str, float]]:
    """
    ë‹¤ì¤‘ ì°¸ì¡° ROUGE ì ìˆ˜ ê³„ì‚°
    
    Args:
        predictions: ì˜ˆì¸¡ëœ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
        references_list: ì°¸ì¡° ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
                        ì˜ˆ: [["ref1_1", "ref1_2", "ref1_3"], ["ref2_1", "ref2_2", "ref2_3"]]
    
    Returns:
        Dict[str, Dict[str, float]]: ROUGE ì ìˆ˜
        {
            "rouge1": {"precision": float, "recall": float, "f1": float},
            "rouge2": {"precision": float, "recall": float, "f1": float},
            "rougeL": {"precision": float, "recall": float, "f1": float},
            "rouge_combined_f1": float  # í‰ê·  F1 ì ìˆ˜
        }
    
    Example:
        >>> calculator = RougeCalculator()
        >>> predictions = ["ìƒì„±ëœ ìš”ì•½ë¬¸"]
        >>> references = [["ì°¸ì¡° ìš”ì•½ë¬¸1", "ì°¸ì¡° ìš”ì•½ë¬¸2", "ì°¸ì¡° ìš”ì•½ë¬¸3"]]
        >>> scores = calculator.compute_multi_reference_rouge(predictions, references)
        >>> print(f"ROUGE-1 F1: {scores['rouge1']['f1']:.4f}")
        >>> print(f"ROUGE-2 F1: {scores['rouge2']['f1']:.4f}")
        >>> print(f"ROUGE-L F1: {scores['rougeL']['f1']:.4f}")
        >>> print(f"ì¢…í•© F1: {scores['rouge_combined_f1']:.4f}")
    """
```

##### `compute_single_reference_rouge()`

```python
def compute_single_reference_rouge(self, 
                                 predictions: List[str],
                                 references: List[str]) -> Dict[str, Dict[str, float]]:
    """
    ë‹¨ì¼ ì°¸ì¡° ROUGE ì ìˆ˜ ê³„ì‚°
    
    Args:
        predictions: ì˜ˆì¸¡ëœ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
        references: ì°¸ì¡° ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        Dict[str, Dict[str, float]]: ROUGE ì ìˆ˜
    
    Example:
        >>> scores = calculator.compute_single_reference_rouge(
        ...     ["ì˜ˆì¸¡ ìš”ì•½"], ["ì°¸ì¡° ìš”ì•½"]
        ... )
        >>> print(f"ë‹¨ì¼ ì°¸ì¡° ROUGE-1 F1: {scores['rouge1']['f1']:.4f}")
    """
```

---

## ì¶”ë¡  ì—”ì§„ ëª¨ë“ˆ (InferenceEngine)

### ê°œìš”

`core/inference.py`ì˜ `InferenceEngine`ì€ ê³ ê¸‰ ì¶”ë¡  ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ ëª¨ë¸ ë˜í¼ë¥¼ ë„˜ì–´ì„œ **ë‹¤ì¤‘ ì…ë ¥ í˜•ì‹ ì§€ì›**, **ìë™ ë””ë°”ì´ìŠ¤ ìµœì í™”**, **ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”**, **ìºì‹œ ì‹œìŠ¤í…œ** ë“±ì˜ í”„ë¡œë•ì…˜ê¸‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•
- ğŸ”„ **ë‹¤ì¤‘ ì…ë ¥ í˜•ì‹**: string, list, DataFrame ìë™ ì²˜ë¦¬
- âš¡ **ìë™ ë””ë°”ì´ìŠ¤ ìµœì í™”**: CUDA, MPS, CPU ìë™ ê°ì§€ ë° ì„¤ì •
- ğŸ“¦ **ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”**: DataLoader ê¸°ë°˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
- ğŸ¯ **ëŒ€íšŒ ì œì¶œ í˜•ì‹**: CSV í˜•ì‹ ìë™ ë³€í™˜ ë° ê²€ì¦
- ğŸ§  **ìºì‹œ ì‹œìŠ¤í…œ**: ë°˜ë³µ ì¶”ë¡  ì„±ëŠ¥ í–¥ìƒ

### `InferenceConfig` í´ë˜ìŠ¤

ì¶”ë¡  ì—”ì§„ì˜ ëª¨ë“  ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” ë°ì´í„°í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
@dataclass
class InferenceConfig:
    """ì¶”ë¡  ì„¤ì •"""
    model_path: str
    batch_size: int = 8
    max_source_length: int = 1024
    max_target_length: int = 256
    num_beams: int = 5
    length_penalty: float = 1.0
    early_stopping: bool = True
    use_cache: bool = True
    device: Optional[str] = None  # Noneì‹œ ìë™ ê°ì§€
    fp16: bool = False
    num_workers: int = 0
```

**ì„¤ì • ë§¤ê°œë³€ìˆ˜ ìƒì„¸**:
- `model_path`: ëª¨ë¸ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ ë˜ëŠ” HuggingFace Hub ID)
- `batch_size`: ë°°ì¹˜ í¬ê¸° (ìë™ ìµœì í™” ì‹œ ë¬´ì‹œë¨)
- `max_source_length`: ì…ë ¥ í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´
- `max_target_length`: ì¶œë ¥ ìš”ì•½ ìµœëŒ€ ê¸¸ì´
- `num_beams`: ë¹” ì„œì¹˜ í¬ê¸° (1=íƒìš•ì  ë””ì½”ë”©)
- `length_penalty`: ê¸¸ì´ í˜ë„í‹° (1.0=ì¤‘ë¦½, >1.0=ê¸´ ìš”ì•½ ì„ í˜¸)
- `early_stopping`: ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€
- `use_cache`: KV ìºì‹œ ì‚¬ìš© ì—¬ë¶€
- `device`: ë””ë°”ì´ìŠ¤ ì§€ì • (Noneì‹œ ìë™ ê°ì§€)
- `fp16`: Mixed Precision ì‚¬ìš© ì—¬ë¶€
- `num_workers`: DataLoader ì›Œì»¤ ìˆ˜

### `InferenceEngine` í´ë˜ìŠ¤

#### í´ë˜ìŠ¤ ì •ì˜

```python
class InferenceEngine:
    """ë…ë¦½ ì¶”ë¡  ì—”ì§„
    
    ëª¨ë¸ ë¡œë“œ, ë‹¨ì¼/ë°°ì¹˜ ì˜ˆì¸¡, DataFrame ì²˜ë¦¬ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: Union[InferenceConfig, Dict[str, Any]]):
        """
        ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            config: InferenceConfig ê°ì²´ ë˜ëŠ” ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
        Raises:
            ValueError: ì˜ëª»ëœ ì„¤ì •ê°’
            FileNotFoundError: ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ì‹œ
            RuntimeError: ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ
        
        Example:
            >>> # ë”•ì…”ë„ˆë¦¬ ì„¤ì •
            >>> config = {
            ...     "model_path": "gogamza/kobart-base-v2",
            ...     "batch_size": 16,
            ...     "device": None  # ìë™ ê°ì§€
            ... }
            >>> engine = InferenceEngine(config)
            
            >>> # InferenceConfig ê°ì²´ ì‚¬ìš©
            >>> config = InferenceConfig(
            ...     model_path="outputs/best_model",
            ...     batch_size=8,
            ...     fp16=True
            ... )
            >>> engine = InferenceEngine(config)
        """
```

#### ìë™ ë””ë°”ì´ìŠ¤ ìµœì í™”

`InferenceEngine`ì€ ì‹¤í–‰ í™˜ê²½ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ìµœì í™”í•©ë‹ˆë‹¤:

```python
def _setup_device(self):
    """ë””ë°”ì´ìŠ¤ ì„¤ì • ë° ìµœì í™”"""
    if self.config.device:
        self.device = torch.device(self.config.device)
    else:
        # ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€
        self.device, device_info = get_optimal_device()
        
        # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ì ìš©
        opt_config = setup_device_config(device_info, 'base')
        
        # ì„¤ì • ìë™ ì¡°ì •
        if self.config.batch_size == 8:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°
            self.config.batch_size = opt_config.batch_size
        
        if opt_config.fp16 and not self.config.fp16:
            self.config.fp16 = opt_config.fp16
```

**ë””ë°”ì´ìŠ¤ë³„ ìë™ ìµœì í™”**:
- **CUDA**: í° ë°°ì¹˜ í¬ê¸° (16-32), FP16 í™œì„±í™”, GPU ë©”ëª¨ë¦¬ ìµœì í™”
- **MPS (Apple Silicon)**: ì¤‘ê°„ ë°°ì¹˜ í¬ê¸° (8-16), FP32 ìœ ì§€, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- **CPU**: ì‘ì€ ë°°ì¹˜ í¬ê¸° (4-8), FP32, ë‹¨ì¼ ì›Œì»¤

#### ì£¼ìš” ë©”ì„œë“œ

##### `predict_single()`

ë‹¨ì¼ ëŒ€í™”ì— ëŒ€í•œ ì¦‰ì‹œ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
def predict_single(self, dialogue: str) -> str:
    """
    ë‹¨ì¼ ëŒ€í™” ìš”ì•½ ìƒì„±
    
    Args:
        dialogue: ëŒ€í™” í…ìŠ¤íŠ¸
        
    Returns:
        str: ìƒì„±ëœ ìš”ì•½
        
    Example:
        >>> engine = InferenceEngine(config)
        >>> dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš”. #Person2#: ë„¤, ì•ˆë…•í•˜ì„¸ìš”."
        >>> summary = engine.predict_single(dialogue)
        >>> print(f"ìš”ì•½: {summary}")
    """
```

##### `predict_batch()`

ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
def predict_batch(self, 
                 dialogues: List[str], 
                 show_progress: bool = True) -> List[str]:
    """
    ë°°ì¹˜ ëŒ€í™” ìš”ì•½ ìƒì„±
    
    ë‚´ë¶€ì ìœ¼ë¡œ DataLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        dialogues: ëŒ€í™” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
        
    Returns:
        List[str]: ìƒì„±ëœ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        
    Features:
        - DataLoader ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬
        - ìë™ í† í°í™” ë° íŒ¨ë”©
        - GPU ë©”ëª¨ë¦¬ ìµœì í™”
        - ì§„í–‰ë¥  í‘œì‹œ (tqdm)
        
    Example:
        >>> dialogues = ["ëŒ€í™”1...", "ëŒ€í™”2...", "ëŒ€í™”3..."]
        >>> summaries = engine.predict_batch(
        ...     dialogues, 
        ...     show_progress=True
        ... )
        >>> print(f"ì²˜ë¦¬ ì™„ë£Œ: {len(summaries)} ìš”ì•½ë¬¸")
    """
```

**ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” íŠ¹ì§•**:
1. **DataLoader ê¸°ë°˜**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
2. **ìë™ íŒ¨ë”©**: ë°°ì¹˜ ë‚´ ìµœëŒ€ ê¸¸ì´ë¡œ ë™ì  íŒ¨ë”©
3. **ì§„í–‰ë¥  ì¶”ì **: tqdmì„ í†µí•œ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ
4. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ë°°ì¹˜ë³„ GPU ë©”ëª¨ë¦¬ ìë™ ì •ë¦¬

##### `predict_from_dataframe()`

DataFrameì—ì„œ ì§ì ‘ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ê³ ê¸‰ ê¸°ëŠ¥ì…ë‹ˆë‹¤.

```python
def predict_from_dataframe(self, 
                          df: pd.DataFrame, 
                          dialogue_column: str = 'dialogue',
                          output_column: str = 'summary',
                          show_progress: bool = True) -> pd.DataFrame:
    """
    DataFrameì—ì„œ ì§ì ‘ ì¶”ë¡  ìˆ˜í–‰
    
    Args:
        df: ì…ë ¥ DataFrame
        dialogue_column: ëŒ€í™”ê°€ í¬í•¨ëœ ì»¬ëŸ¼ëª…
        output_column: ìƒì„±ëœ ìš”ì•½ì„ ì €ì¥í•  ì»¬ëŸ¼ëª…
        show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
        
    Returns:
        pd.DataFrame: ìš”ì•½ì´ ì¶”ê°€ëœ DataFrame
        
    Raises:
        ValueError: ì§€ì •ëœ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ì‹œ
        
    Example:
        >>> import pandas as pd
        >>> df = pd.read_csv("data/test.csv")
        >>> result_df = engine.predict_from_dataframe(
        ...     df, 
        ...     dialogue_column='dialogue',
        ...     output_column='generated_summary'
        ... )
        >>> print(result_df[['fname', 'generated_summary']].head())
    """
```

##### `save_submission()`

ëŒ€íšŒ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

```python
def save_submission(self, 
                   df: pd.DataFrame, 
                   output_path: str,
                   fname_column: str = 'fname',
                   summary_column: str = 'summary'):
    """
    ëŒ€íšŒ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ì €ì¥
    
    Args:
        df: ê²°ê³¼ DataFrame
        output_path: ì €ì¥ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
        fname_column: íŒŒì¼ëª… ì»¬ëŸ¼
        summary_column: ìš”ì•½ ì»¬ëŸ¼
        
    Features:
        - ìë™ ê²½ë¡œ í•´ê²° ë° ë””ë ‰í† ë¦¬ ìƒì„±
        - CSV í˜•ì‹ ê²€ì¦
        - UTF-8 ì¸ì½”ë”© ë³´ì¥
        
    Example:
        >>> engine.save_submission(
        ...     result_df,
        ...     "outputs/submission.csv",
        ...     fname_column='fname',
        ...     summary_column='summary'
        ... )
    """
```

##### `__call__()` - í†µí•© ì¸í„°í˜ì´ìŠ¤

ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í†µí•© ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.

```python
def __call__(self, 
            dialogue: Union[str, List[str], pd.DataFrame], 
            **kwargs):
    """
    ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ ì§€ì›
    
    Args:
        dialogue: ëŒ€í™” í…ìŠ¤íŠ¸, ë¦¬ìŠ¤íŠ¸, ë˜ëŠ” DataFrame
        **kwargs: ì¶”ê°€ ì¸ì
        
    Returns:
        ìš”ì•½ ê²°ê³¼ (ì…ë ¥ íƒ€ì…ì— ë”°ë¼ str, List[str], ë˜ëŠ” DataFrame)
        
    Example:
        >>> # ë‹¨ì¼ í…ìŠ¤íŠ¸
        >>> summary = engine("ë‹¨ì¼ ëŒ€í™” í…ìŠ¤íŠ¸")
        
        >>> # ë¦¬ìŠ¤íŠ¸ ë°°ì¹˜
        >>> summaries = engine(["ëŒ€í™”1", "ëŒ€í™”2", "ëŒ€í™”3"])
        
        >>> # DataFrame
        >>> result_df = engine(dataframe)
    """
```

### ê³ ê¸‰ ê¸°ëŠ¥ ë° ìµœì í™”

#### 1. ìë™ ëª¨ë¸ íƒ€ì… ê°ì§€

```python
def _load_model_and_tokenizer(self):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ìë™ ë¡œë“œ"""
    try:
        # Seq2Seq ëª¨ë¸ ì‹œë„ (BART, T5 ë“±)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(
            str(model_path),
            torch_dtype=torch.float16 if self.config.fp16 else torch.float32
        )
        self.model_type = "seq2seq"
    except:
        # Causal LM ëª¨ë¸ ì‹œë„ (GPT ê³„ì—´)
        self.model = AutoModelForCausalLM.from_pretrained(
            str(model_path),
            torch_dtype=torch.float16 if self.config.fp16 else torch.float32
        )
        self.model_type = "causal"
```

#### 2. ë©”ëª¨ë¦¬ ìµœì í™”

- **ë™ì  ë°°ì¹˜ í¬ê¸°**: ë””ë°”ì´ìŠ¤ ë©”ëª¨ë¦¬ì— ë§ì¶° ìë™ ì¡°ì •
- **FP16 Mixed Precision**: GPUì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì ˆì•½
- **Gradient Checkpointing**: í•™ìŠµ ì‹œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- **KV Cache**: ë°˜ë³µ ìƒì„± ì‹œ ê³„ì‚° ìµœì í™”

#### 3. ê²½ë¡œ ê´€ë¦¬ í†µí•©

```python
# ìƒëŒ€ ê²½ë¡œ ìë™ í•´ê²°
model_path = path_manager.resolve_path(self.config.model_path)

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìë™ ìƒì„±
output_path = path_manager.resolve_path(output_path)
output_path.parent.mkdir(parents=True, exist_ok=True)
```

### í—¬í¼ í•¨ìˆ˜

#### `create_inference_engine()`

í¸ì˜ë¥¼ ìœ„í•œ íŒ©í† ë¦¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.

```python
def create_inference_engine(model_path: str, **kwargs) -> InferenceEngine:
    """
    ì¶”ë¡  ì—”ì§„ ìƒì„± í—¬í¼ í•¨ìˆ˜
    
    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        **kwargs: ì¶”ê°€ ì„¤ì •
        
    Returns:
        InferenceEngine: ì´ˆê¸°í™”ëœ ì¶”ë¡  ì—”ì§„
        
    Example:
        >>> # ê°„ë‹¨í•œ ìƒì„±
        >>> engine = create_inference_engine("gogamza/kobart-base-v2")
        
        >>> # ì‚¬ìš©ì ì •ì˜ ì„¤ì •
        >>> engine = create_inference_engine(
        ...     "outputs/best_model",
        ...     batch_size=16,
        ...     fp16=True,
        ...     num_beams=8
        ... )
    """
    config = InferenceConfig(model_path=model_path, **kwargs)
    return InferenceEngine(config)
```

### ì‚¬ìš© íŒ¨í„´ ë° ì˜ˆì œ

#### ê¸°ë³¸ ì‚¬ìš© íŒ¨í„´

```python
from core.inference import InferenceEngine, InferenceConfig

# 1. ì„¤ì • ìƒì„±
config = InferenceConfig(
    model_path="gogamza/kobart-base-v2",
    batch_size=None,  # ìë™ ìµœì í™”
    device=None,      # ìë™ ê°ì§€
    fp16=None         # ìë™ ì„¤ì •
)

# 2. ì—”ì§„ ì´ˆê¸°í™”
engine = InferenceEngine(config)

# 3. ì¶”ë¡  ì‹¤í–‰
summary = engine("ë‹¨ì¼ ëŒ€í™”")
summaries = engine(["ëŒ€í™”1", "ëŒ€í™”2"])
result_df = engine(dataframe)
```

#### ëŒ€íšŒ ì œì¶œ ì›Œí¬í”Œë¡œìš°

```python
import pandas as pd
from core.inference import create_inference_engine

# 1. ì¶”ë¡  ì—”ì§„ ìƒì„±
engine = create_inference_engine(
    "outputs/best_model",
    batch_size=16,
    num_beams=5
)

# 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv("data/test.csv")

# 3. ë°°ì¹˜ ì¶”ë¡ 
result_df = engine.predict_from_dataframe(
    test_df,
    dialogue_column='dialogue',
    output_column='summary'
)

# 4. ì œì¶œ íŒŒì¼ ì €ì¥
engine.save_submission(
    result_df,
    "outputs/submission.csv"
)

print("ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ!")
```

#### ì„±ëŠ¥ ìµœì í™” íŒ¨í„´

```python
# í’ˆì§ˆ ìš°ì„  ì„¤ì •
quality_config = InferenceConfig(
    model_path="outputs/best_model",
    num_beams=8,           # ë” ë§ì€ íƒìƒ‰
    length_penalty=1.2,    # ì ì ˆí•œ ê¸¸ì´
    max_target_length=512  # ë” ê¸´ ìš”ì•½
)

# ì†ë„ ìš°ì„  ì„¤ì •
speed_config = InferenceConfig(
    model_path="outputs/best_model",
    num_beams=1,          # íƒìš•ì  ë””ì½”ë”©
    batch_size=32,        # í° ë°°ì¹˜
    fp16=True            # Mixed Precision
)
```

### ì—ëŸ¬ ì²˜ë¦¬ ë° ë””ë²„ê¹…

#### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ì±…

```python
try:
    engine = InferenceEngine(config)
except FileNotFoundError:
    print("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
except torch.cuda.OutOfMemoryError:
    print("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ FP16ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    config.batch_size = 4
    config.fp16 = True
    engine = InferenceEngine(config)
except RuntimeError as e:
    if "MPS" in str(e):
        print("MPS í˜¸í™˜ì„± ë¬¸ì œ. CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        config.device = "cpu"
        engine = InferenceEngine(config)
```

#### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
import time

# ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
start_time = time.time()
summaries = engine.predict_batch(dialogues)
end_time = time.time()

processing_time = end_time - start_time
throughput = len(dialogues) / processing_time

print(f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
print(f"ì²˜ë¦¬ëŸ‰: {throughput:.2f} ëŒ€í™”/ì´ˆ")
print(f"í‰ê·  ëŒ€í™”ë‹¹: {processing_time/len(dialogues):.3f}ì´ˆ")
```

---
    
    Args:
        input_file: ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
        output_file: ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
        batch_size: ë°°ì¹˜ í¬ê¸°
        **kwargs: ì¶”ê°€ ìƒì„± íŒŒë¼ë¯¸í„°
    
    Returns:
        int: ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜
    
    Example:
        >>> processed_count = engine.predict_from_file(
        ...     "data/test.csv",
        ...     "outputs/submission.csv",
        ...     batch_size=8
        ... )
        >>> print(f"ì²˜ë¦¬ ì™„ë£Œ: {processed_count} ìƒ˜í”Œ")
    """
```

---

## ê²½ë¡œ ê´€ë¦¬ ëª¨ë“ˆ

### `PathManager` í´ë˜ìŠ¤

í”„ë¡œì íŠ¸ ë‚´ ìƒëŒ€ ê²½ë¡œ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

#### ì£¼ìš” ë©”ì„œë“œ

##### `resolve_path()`

```python
@staticmethod
def resolve_path(relative_path: Union[str, Path]) -> Path:
    """
    ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ í•´ê²°
    
    Args:
        relative_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
    
    Returns:
        Path: í•´ê²°ëœ ì ˆëŒ€ ê²½ë¡œ
    
    Raises:
        ValueError: ì ˆëŒ€ ê²½ë¡œ ì…ë ¥ ì‹œ
        FileNotFoundError: ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ì‹œ (ì˜µì…˜)
    
    Example:
        >>> abs_path = PathManager.resolve_path("data/train.csv")
        >>> print(f"ì ˆëŒ€ ê²½ë¡œ: {abs_path}")
    """
```

##### `ensure_dir()`

```python
@staticmethod
def ensure_dir(dir_path: Union[str, Path]) -> Path:
    """
    ë””ë ‰í† ë¦¬ ì¡´ì¬ ë³´ì¥ (ì—†ìœ¼ë©´ ìƒì„±)
    
    Args:
        dir_path: ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
    
    Returns:
        Path: ìƒì„±ëœ ë””ë ‰í† ë¦¬ ì ˆëŒ€ ê²½ë¡œ
    
    Example:
        >>> PathManager.ensure_dir("outputs/experiments")
        >>> print("ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ")
    """
```

##### `get_project_root()`

```python
@staticmethod
def get_project_root() -> Path:
    """
    í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì¡°íšŒ
    
    Returns:
        Path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì ˆëŒ€ ê²½ë¡œ
    
    Example:
        >>> root = PathManager.get_project_root()
        >>> print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {root}")
    """
```

---

## ì‹¤í—˜ ê´€ë¦¬ ëª¨ë“ˆ

### `ExperimentTracker` í´ë˜ìŠ¤

ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì¶”ì  ë° ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

#### í´ë˜ìŠ¤ ì •ì˜

```python
class ExperimentTracker:
    """ì‹¤í—˜ ì¶”ì  ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, experiments_dir: Union[str, Path] = "outputs/experiments"):
        """
        ExperimentTracker ì´ˆê¸°í™”
        
        Args:
            experiments_dir: ì‹¤í—˜ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œ)
        """
```

#### ì£¼ìš” ë©”ì„œë“œ

##### `start_experiment()`

```python
def start_experiment(self, 
                    name: str,
                    description: str,
                    config: Dict[str, Any]) -> str:
    """
    ìƒˆ ì‹¤í—˜ ì‹œì‘
    
    Args:
        name: ì‹¤í—˜ ì´ë¦„
        description: ì‹¤í—˜ ì„¤ëª…
        config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    
    Returns:
        str: ìƒì„±ëœ ì‹¤í—˜ ID (UUID)
    
    Example:
        >>> tracker = ExperimentTracker()
        >>> exp_id = tracker.start_experiment(
        ...     name="baseline_kobart",
        ...     description="KoBART ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜",
        ...     config={"model": "kobart", "lr": 0.001, "epochs": 5}
        ... )
        >>> print(f"ì‹¤í—˜ ì‹œì‘: {exp_id[:8]}")
    """
```

##### `log_metrics()`

```python
def log_metrics(self, 
               metrics: Dict[str, float],
               step: Optional[int] = None) -> None:
    """
    ì‹¤í—˜ ë©”íŠ¸ë¦­ ë¡œê¹…
    
    Args:
        metrics: ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        step: ìŠ¤í… ë²ˆí˜¸ (Noneì‹œ ìë™ ì¦ê°€)
    
    Example:
        >>> tracker.log_metrics({
        ...     "rouge1_f1": 0.45,
        ...     "rouge2_f1": 0.32,
        ...     "rougeL_f1": 0.38,
        ...     "loss": 1.5
        ... }, step=100)
    """
```

##### `end_experiment()`

```python
def end_experiment(self, 
                  final_metrics: Dict[str, Any],
                  status: str = "completed") -> None:
    """
    ì‹¤í—˜ ì¢…ë£Œ
    
    Args:
        final_metrics: ìµœì¢… ë©”íŠ¸ë¦­
        status: ì‹¤í—˜ ìƒíƒœ ("completed", "failed", "cancelled")
    
    Example:
        >>> tracker.end_experiment(
        ...     final_metrics={"best_rouge_combined_f1": 0.456},
        ...     status="completed"
        ... )
    """
```

##### `get_experiment_summary()`

```python
def get_experiment_summary(self) -> pd.DataFrame:
    """
    ëª¨ë“  ì‹¤í—˜ ìš”ì•½ ì¡°íšŒ
    
    Returns:
        pd.DataFrame: ì‹¤í—˜ ìš”ì•½ í…Œì´ë¸”
                     ì»¬ëŸ¼: id, name, status, device, start_time, best_rouge_combined_f1
    
    Example:
        >>> summary = tracker.get_experiment_summary()
        >>> print(summary.head())
        >>> best_exp = summary.loc[summary['best_rouge_combined_f1'].idxmax()]
        >>> print(f"ìµœê³  ì„±ëŠ¥ ì‹¤í—˜: {best_exp['name']}")
    """
```

### `ModelRegistry` í´ë˜ìŠ¤

í•™ìŠµëœ ëª¨ë¸ì˜ ë©”íƒ€ë°ì´í„° ë° ì„±ëŠ¥ ê´€ë¦¬ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

#### ì£¼ìš” ë©”ì„œë“œ

##### `register_model()`

```python
def register_model(self,
                  name: str,
                  architecture: str,
                  config: Dict[str, Any],
                  performance: Dict[str, float],
                  model_path: Optional[Union[str, Path]] = None,
                  experiment_id: Optional[str] = None) -> str:
    """
    ëª¨ë¸ ë“±ë¡
    
    Args:
        name: ëª¨ë¸ ì´ë¦„
        architecture: ëª¨ë¸ ì•„í‚¤í…ì²˜ ("kobart", "kt5", "mt5" ë“±)
        config: ëª¨ë¸ ì„¤ì •
        performance: ì„±ëŠ¥ ë©”íŠ¸ë¦­
        model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
        experiment_id: ì—°ê´€ëœ ì‹¤í—˜ ID
    
    Returns:
        str: ëª¨ë¸ ID
    
    Example:
        >>> registry = ModelRegistry()
        >>> model_id = registry.register_model(
        ...     name="kobart_baseline_v1",
        ...     architecture="kobart",
        ...     config={"lr": 0.001, "epochs": 5},
        ...     performance={"rouge_combined_f1": 0.456},
        ...     model_path="outputs/best_model"
        ... )
        >>> print(f"ëª¨ë¸ ë“±ë¡: {model_id[:8]}")
    """
```

##### `get_best_model()`

```python
def get_best_model(self, 
                  architecture: Optional[str] = None,
                  metric: str = "rouge_combined_f1") -> Optional[Dict[str, Any]]:
    """
    ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¡°íšŒ
    
    Args:
        architecture: íŠ¹ì • ì•„í‚¤í…ì²˜ë¡œ í•„í„°ë§ (ì„ íƒì‚¬í•­)
        metric: ë¹„êµ ê¸°ì¤€ ë©”íŠ¸ë¦­
    
    Returns:
        Optional[Dict[str, Any]]: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´
    
    Example:
        >>> best_model = registry.get_best_model("kobart")
        >>> if best_model:
        ...     print(f"ìµœê³  ì„±ëŠ¥ KoBART: {best_model['name']}")
        ...     print(f"ì„±ëŠ¥: {best_model['performance']['rouge_combined_f1']:.4f}")
    """
```

---

## ë””ë°”ì´ìŠ¤ ìµœì í™” ëª¨ë“ˆ

### ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ìµœì í™”

#### `get_optimal_device()`

```python
def get_optimal_device() -> str:
    """
    ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
    
    Returns:
        str: ê°ì§€ëœ ë””ë°”ì´ìŠ¤ ("mps", "cuda", "cpu")
    
    Example:
        >>> device = get_optimal_device()
        >>> print(f"ê°ì§€ëœ ë””ë°”ì´ìŠ¤: {device}")
    """
```

#### `get_device_config()`

```python
def get_device_config(device: str) -> Dict[str, Any]:
    """
    ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ì¡°íšŒ
    
    Args:
        device: ë””ë°”ì´ìŠ¤ íƒ€ì…
    
    Returns:
        Dict[str, Any]: ìµœì í™” ì„¤ì •
        {
            "batch_size": int,
            "fp16": bool,
            "dataloader_pin_memory": bool,
            "torch_dtype": torch.dtype
        }
    
    Example:
        >>> config = get_device_config("mps")
        >>> print(f"MPS ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
        >>> print(f"FP16 ì‚¬ìš©: {config['fp16']}")
    """
```

---

## ì‚¬ìš© ì˜ˆì œ

### ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°

```python
# 1. í•„ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸
from utils.data_utils import DataProcessor
from utils.metrics import RougeCalculator  
from core.inference import InferenceEngine
from utils.experiment_utils import ExperimentTracker, ModelRegistry
from utils.device_utils import get_optimal_device

# 2. ë””ë°”ì´ìŠ¤ ì„¤ì •
device = get_optimal_device()
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# 3. ë°ì´í„° ë¡œë”©
processor = DataProcessor()
train_data = processor.load_multi_reference_data("data/train.csv")
print(f"í•™ìŠµ ë°ì´í„°: {len(train_data)} ìƒ˜í”Œ")

# 4. ì‹¤í—˜ ì‹œì‘
tracker = ExperimentTracker()
exp_id = tracker.start_experiment(
    name="baseline_experiment",
    description="KoBART ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜",
    config={
        "model": "kobart",
        "learning_rate": 0.001,
        "batch_size": 8,
        "epochs": 5,
        "device": device
    }
)

# 5. í•™ìŠµ ì‹¤í–‰ (ì˜ˆì‹œ)
# trainer = DialogueSummarizationTrainer(config, experiment_name="baseline")
# trainer.train()

# 6. ì„±ëŠ¥ í‰ê°€ (Multi-reference ROUGE)
calculator = RougeCalculator(use_korean_tokenizer=True)
# ì‹¤ì œ í‰ê°€ëŠ” í•™ìŠµ ì™„ë£Œ í›„
# scores = calculator.compute_multi_reference_rouge(predictions, references_list)

# 7. ëª¨ë¸ ë“±ë¡
registry = ModelRegistry()
# model_id = registry.register_model(
#     name="baseline_kobart_v1",
#     architecture="kobart",
#     config=config,
#     performance=scores,
#     model_path="outputs/best_model",
#     experiment_id=exp_id
# )

# 8. ì¶”ë¡  ì‹¤í–‰
engine = InferenceEngine(
    model_path="outputs/best_model",  # ìƒëŒ€ ê²½ë¡œ
    device=device
)

# íŒŒì¼ ê¸°ë°˜ ì¶”ë¡  (test.csv â†’ submission.csv)
processed_count = engine.predict_from_file(
    input_file="data/test.csv",      # ìƒëŒ€ ê²½ë¡œ
    output_file="outputs/submission.csv",  # ìƒëŒ€ ê²½ë¡œ
    batch_size=8
)

# 9. ì œì¶œ íŒŒì¼ ê²€ì¦
is_valid = processor.validate_submission_format("outputs/submission.csv")
print(f"ì œì¶œ íŒŒì¼ ê²€ì¦: {'PASS' if is_valid else 'FAIL'}")
```

### ë‹¨ì¼ ì˜ˆì¸¡ ë° í‰ê°€

```python
# ë‹¨ì¼ ëŒ€í™” ìš”ì•½
engine = InferenceEngine("outputs/best_model")
dialogue = """
í™”ì1: ì˜¤ëŠ˜ íšŒì˜ì—ì„œ ì–´ë–¤ ì•ˆê±´ì„ ë‹¤ë£° ì˜ˆì •ì¸ê°€ìš”?
í™”ì2: ì£¼ë¡œ ì‹ ì œí’ˆ ì¶œì‹œ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì— ëŒ€í•´ ë…¼ì˜í•  ì˜ˆì •ì…ë‹ˆë‹¤.
í™”ì1: ì–¸ì œê¹Œì§€ ìµœì¢… ê²°ì •ì„ ë‚´ë ¤ì•¼ í•˜ë‚˜ìš”?
í™”ì2: ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼ê¹Œì§€ ëª¨ë“  ì„¸ë¶€ì‚¬í•­ì„ í™•ì •í•´ì•¼ í•©ë‹ˆë‹¤.
"""

summary = engine.predict_single(dialogue)
print(f"ìƒì„±ëœ ìš”ì•½: {summary}")

# Multi-reference í‰ê°€
calculator = RougeCalculator()
predictions = [summary]
references_list = [[
    "íšŒì˜ì—ì„œ ì‹ ì œí’ˆ ì¶œì‹œì™€ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê³  ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼ê¹Œì§€ ê²°ì • ì˜ˆì •",
    "ì‹ ì œí’ˆ ì¶œì‹œ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµ íšŒì˜, ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼ ìµœì¢… ê²°ì •",
    "íšŒì˜ ì•ˆê±´ì€ ì‹ ì œí’ˆê³¼ ë§ˆì¼€íŒ…ì´ë©° ë‹¤ìŒì£¼ ê¸ˆìš”ì¼ê¹Œì§€ í™•ì • í•„ìš”"
]]

scores = calculator.compute_multi_reference_rouge(predictions, references_list)
print(f"ROUGE-1 F1: {scores['rouge1']['f1']:.4f}")
print(f"ROUGE-2 F1: {scores['rouge2']['f1']:.4f}")
print(f"ROUGE-L F1: {scores['rougeL']['f1']:.4f}")
print(f"Combined F1: {scores['rouge_combined_f1']:.4f}")
```

### ì‹¤í—˜ ê´€ë¦¬ ì›Œí¬í”Œë¡œìš°

```python
# ì—¬ëŸ¬ ì‹¤í—˜ ì‹¤í–‰ ë° ë¹„êµ
tracker = ExperimentTracker()
registry = ModelRegistry()

# ì‹¤í—˜ 1: ë² ì´ìŠ¤ë¼ì¸
exp1_id = tracker.start_experiment(
    name="baseline_kobart",
    description="KoBART baseline model",
    config={"model": "kobart", "lr": 0.001, "epochs": 5}
)

# ë©”íŠ¸ë¦­ ë¡œê¹… (í•™ìŠµ ì¤‘)
for epoch in range(5):
    metrics = {
        "rouge1_f1": 0.3 + epoch * 0.05,
        "rouge2_f1": 0.2 + epoch * 0.04,
        "rougeL_f1": 0.25 + epoch * 0.045,
        "loss": 2.0 - epoch * 0.3
    }
    tracker.log_metrics(metrics, step=epoch)

# ì‹¤í—˜ 1 ì¢…ë£Œ
final_metrics = {"best_rouge_combined_f1": 0.45}
tracker.end_experiment(final_metrics, "completed")

# ëª¨ë¸ ë“±ë¡
model1_id = registry.register_model(
    name="kobart_baseline",
    architecture="kobart",
    config={"lr": 0.001, "epochs": 5},
    performance=final_metrics,
    experiment_id=exp1_id
)

# ì‹¤í—˜ 2: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
exp2_id = tracker.start_experiment(
    name="kobart_tuned",
    description="KoBART with tuned hyperparameters",
    config={"model": "kobart", "lr": 0.0005, "epochs": 7}
)

# ... ì‹¤í—˜ 2 ì‹¤í–‰ ...

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¡°íšŒ
best_model = registry.get_best_model("kobart")
if best_model:
    print(f"Best KoBART model: {best_model['name']}")
    print(f"Performance: {best_model['performance']['rouge_combined_f1']}")

# ëª¨ë“  ì‹¤í—˜ ìš”ì•½ ì¡°íšŒ
exp_summary = tracker.get_experiment_summary()
print(exp_summary[['name', 'status', 'device', 'best_rouge_combined_f1']])

# ëª¨ë“  ëª¨ë¸ ìš”ì•½ ì¡°íšŒ
models_summary = registry.get_models_summary()
print(models_summary[['name', 'architecture', 'rouge_combined_f1']])
```

---

## ì—ëŸ¬ ì²˜ë¦¬

### ì¼ë°˜ì ì¸ ì˜ˆì™¸ íƒ€ì…

#### `ValueError`
- **ì›ì¸**: ì ˆëŒ€ ê²½ë¡œ ì…ë ¥, ì˜ëª»ëœ íŒŒë¼ë¯¸í„° ê°’
- **í•´ê²°**: ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©, íŒŒë¼ë¯¸í„° ê°’ í™•ì¸

```python
try:
    processor = DataProcessor()
    data = processor.load_multi_reference_data("/absolute/path/file.csv")  # ì˜ëª»ëœ ì‚¬ìš©
except ValueError as e:
    print(f"ê²½ë¡œ ì˜¤ë¥˜: {e}")
    print("í•´ê²°ì±…: ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: 'data/train.csv')")
```

#### `FileNotFoundError`
- **ì›ì¸**: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼/ë””ë ‰í† ë¦¬ ì ‘ê·¼
- **í•´ê²°**: ê²½ë¡œ í™•ì¸, íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸

```python
try:
    engine = InferenceEngine("nonexistent/model/path")
except FileNotFoundError as e:
    print(f"íŒŒì¼ ì—†ìŒ: {e}")
    print("í•´ê²°ì±…: ëª¨ë¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
```

#### `torch.cuda.OutOfMemoryError`
- **ì›ì¸**: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
- **í•´ê²°**: ë°°ì¹˜ í¬ê¸° ê°ì†Œ, ëª¨ë¸ ê²½ëŸ‰í™”

```python
try:
    summaries = engine.predict_batch(dialogues, batch_size=32)
except torch.cuda.OutOfMemoryError:
    print("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
    print("í•´ê²°ì±…: ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ FP16ì„ ì‚¬ìš©í•˜ì„¸ìš”")
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    # ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„
    summaries = engine.predict_batch(dialogues, batch_size=8)
```

### ì—ëŸ¬ ë””ë²„ê¹… ê°€ì´ë“œ

#### ê²½ë¡œ ê´€ë ¨ ë¬¸ì œ

```python
# ê²½ë¡œ ë””ë²„ê¹… í•¨ìˆ˜
def debug_path_issue(file_path: str):
    """ê²½ë¡œ ê´€ë ¨ ë¬¸ì œ ë””ë²„ê¹…"""
    
    from pathlib import Path
    from utils.path_utils import PathManager
    
    print(f"ì…ë ¥ ê²½ë¡œ: {file_path}")
    print(f"ì ˆëŒ€ ê²½ë¡œ ì—¬ë¶€: {Path(file_path).is_absolute()}")
    
    try:
        resolved_path = PathManager.resolve_path(file_path)
        print(f"í•´ê²°ëœ ê²½ë¡œ: {resolved_path}")
        print(f"ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€: {resolved_path.exists()}")
        
        if resolved_path.exists():
            print(f"ê²½ë¡œ íƒ€ì…: {'íŒŒì¼' if resolved_path.is_file() else 'ë””ë ‰í† ë¦¬'}")
            if resolved_path.is_file():
                print(f"íŒŒì¼ í¬ê¸°: {resolved_path.stat().st_size} bytes")
        
    except Exception as e:
        print(f"ê²½ë¡œ í•´ê²° ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
debug_path_issue("data/train.csv")
```

#### ë””ë°”ì´ìŠ¤ ê´€ë ¨ ë¬¸ì œ

```python
def debug_device_issue():
    """ë””ë°”ì´ìŠ¤ ê´€ë ¨ ë¬¸ì œ ë””ë²„ê¹…"""
    
    import torch
    import platform
    from utils.device_utils import get_optimal_device, get_device_config
    
    print("=== ì‹œìŠ¤í…œ ì •ë³´ ===")
    print(f"í”Œë«í¼: {platform.system()} {platform.machine()}")
    print(f"Python: {platform.python_version()}")
    print(f"PyTorch: {torch.__version__}")
    
    print("\n=== ë””ë°”ì´ìŠ¤ ì •ë³´ ===")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        print(f"í˜„ì¬ GPU: {torch.cuda.get_device_name()}")
    
    if hasattr(torch.backends, 'mps'):
        print(f"MPS ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}")
    
    print("\n=== ê¶Œì¥ ì„¤ì • ===")
    device = get_optimal_device()
    config = get_device_config(device)
    print(f"ê¶Œì¥ ë””ë°”ì´ìŠ¤: {device}")
    print(f"ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
    print(f"FP16 ì‚¬ìš©: {config['fp16']}")

# ì‚¬ìš© ì˜ˆì‹œ
debug_device_issue()
```

---

## ì„±ëŠ¥ ìµœì í™”

### ë°°ì¹˜ í¬ê¸° ìµœì í™”

```python
def find_optimal_batch_size(engine: InferenceEngine, 
                          test_dialogues: List[str],
                          max_batch_size: int = 32) -> int:
    """
    ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰
    
    Args:
        engine: ì¶”ë¡  ì—”ì§„
        test_dialogues: í…ŒìŠ¤íŠ¸ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
        max_batch_size: ì‹œë„í•  ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
    
    Returns:
        int: ìµœì  ë°°ì¹˜ í¬ê¸°
    """
    
    import torch
    import time
    
    optimal_batch_size = 1
    
    for batch_size in [2, 4, 8, 16, 32][:max_batch_size.bit_length()]:
        try:
            print(f"ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸
            test_sample = test_dialogues[:batch_size * 2]
            
            start_time = time.time()
            summaries = engine.predict_batch(
                test_sample, 
                batch_size=batch_size,
                show_progress=False
            )
            end_time = time.time()
            
            processing_time = end_time - start_time
            samples_per_second = len(test_sample) / processing_time
            
            print(f"âœ… ë°°ì¹˜ í¬ê¸° {batch_size}: {samples_per_second:.2f} samples/sec")
            optimal_batch_size = batch_size
            
        except torch.cuda.OutOfMemoryError:
            print(f"âŒ ë°°ì¹˜ í¬ê¸° {batch_size}: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
            if engine.device == "cuda":
                torch.cuda.empty_cache()
            break
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ í¬ê¸° {batch_size}: {e}")
            break
    
    print(f"ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
    return optimal_batch_size
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```python
def monitor_memory_usage(func):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    
    import functools
    import psutil
    import torch
    
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # ì‹œì‘ ë©”ëª¨ë¦¬
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        if torch.cuda.is_available():
            gpu_memory_before = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        else:
            gpu_memory_before = 0
        
        try:
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ì¢…ë£Œ ë©”ëª¨ë¦¬
            memory_after = process.memory_info().rss / 1024 / 1024
            memory_diff = memory_after - memory_before
            
            if torch.cuda.is_available():
                gpu_memory_after = torch.cuda.memory_allocated() / 1024 / 1024
                gpu_memory_diff = gpu_memory_after - gpu_memory_before
            else:
                gpu_memory_diff = 0
            
            print(f"ğŸ’¾ {func.__name__} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
            print(f"  ì‹œìŠ¤í…œ: {memory_diff:+.1f} MB")
            if gpu_memory_diff:
                print(f"  GPU: {gpu_memory_diff:+.1f} MB")
            
            return result
            
        finally:
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    return wrapper

# ì‚¬ìš© ì˜ˆì‹œ
@monitor_memory_usage
def memory_intensive_function():
    # ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
    engine = InferenceEngine("outputs/best_model")
    dialogues = ["test dialogue"] * 100
    summaries = engine.predict_batch(dialogues)
    return summaries
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

```python
def benchmark_inference_performance(engine: InferenceEngine,
                                   test_dialogues: List[str],
                                   batch_sizes: List[int] = [1, 4, 8, 16]) -> pd.DataFrame:
    """
    ì¶”ë¡  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
    
    Args:
        engine: ì¶”ë¡  ì—”ì§„
        test_dialogues: í…ŒìŠ¤íŠ¸ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
        batch_sizes: í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ í¬ê¸° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        pd.DataFrame: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    """
    
    import time
    import torch
    
    results = []
    
    for batch_size in batch_sizes:
        try:
            print(f"ë°°ì¹˜ í¬ê¸° {batch_size} ë²¤ì¹˜ë§ˆí‚¹...")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if engine.device == "cuda":
                torch.cuda.empty_cache()
            
            # ì›Œë°ì—…
            warmup_dialogues = test_dialogues[:batch_size]
            engine.predict_batch(warmup_dialogues, batch_size=batch_size, show_progress=False)
            
            # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
            start_time = time.time()
            summaries = engine.predict_batch(
                test_dialogues[:50],  # 50ê°œ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸
                batch_size=batch_size,
                show_progress=False
            )
            end_time = time.time()
            
            total_time = end_time - start_time
            samples_per_second = len(summaries) / total_time
            avg_time_per_sample = total_time / len(summaries)
            
            results.append({
                'batch_size': batch_size,
                'total_time': round(total_time, 2),
                'samples_per_second': round(samples_per_second, 2),
                'avg_time_per_sample': round(avg_time_per_sample, 3),
                'device': engine.device
            })
            
            print(f"  {samples_per_second:.2f} samples/sec")
            
        except torch.cuda.OutOfMemoryError:
            print(f"  GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
            results.append({
                'batch_size': batch_size,
                'total_time': None,
                'samples_per_second': None,
                'avg_time_per_sample': None,
                'device': engine.device,
                'error': 'OOM'
            })
            
        except Exception as e:
            print(f"  ì˜¤ë¥˜: {e}")
            results.append({
                'batch_size': batch_size,
                'total_time': None,
                'samples_per_second': None,
                'avg_time_per_sample': None,
                'device': engine.device,
                'error': str(e)
            })
    
    benchmark_df = pd.DataFrame(results)
    print("\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
    print(benchmark_df)
    
    return benchmark_df
```

---

## íƒ€ì… ì •ì˜

### ê³µí†µ íƒ€ì… ë³„ì¹­

```python
from typing import Union, List, Dict, Any, Optional, Tuple
from pathlib import Path
import pandas as pd

# ê²½ë¡œ ê´€ë ¨ íƒ€ì…
PathLike = Union[str, Path]

# ë°ì´í„°í”„ë ˆì„ íƒ€ì…
DialogueDataFrame = pd.DataFrame  # í•„ìˆ˜ ì»¬ëŸ¼: fname, dialogue
MultiRefDataFrame = pd.DataFrame  # í•„ìˆ˜ ì»¬ëŸ¼: fname, dialogue, summaries
SubmissionDataFrame = pd.DataFrame  # í•„ìˆ˜ ì»¬ëŸ¼: fname, summary

# ROUGE ì ìˆ˜ íƒ€ì…
RougeScores = Dict[str, Dict[str, float]]
# {"rouge1": {"precision": float, "recall": float, "f1": float}, ...}

# ì‹¤í—˜ ë©”íŠ¸ë¦­ íƒ€ì…
MetricsDict = Dict[str, float]

# ë””ë°”ì´ìŠ¤ íƒ€ì…
DeviceType = str  # "mps", "cuda", "cpu"

# ëª¨ë¸ ì •ë³´ íƒ€ì…
ModelInfo = Dict[str, Any]

# ì„¤ì • íƒ€ì…
ConfigDict = Dict[str, Any]
```

### í”„ë¡œí† ì½œ ì •ì˜

```python
from typing import Protocol

class DataProcessorProtocol(Protocol):
    def load_multi_reference_data(self, file_path: PathLike) -> MultiRefDataFrame: ...
    def export_submission_format(self, predictions: List[str], fnames: List[str], output_path: PathLike) -> SubmissionDataFrame: ...

class RougeCalculatorProtocol(Protocol):
    def compute_multi_reference_rouge(self, predictions: List[str], references_list: List[List[str]]) -> RougeScores: ...

class InferenceEngineProtocol(Protocol):
    def predict_single(self, dialogue: str, **kwargs) -> str: ...
    def predict_batch(self, dialogues: List[str], batch_size: int = 8, **kwargs) -> List[str]: ...
```

---

## ìƒìˆ˜ ë° ì„¤ì •

### ê¸°ë³¸ ì„¤ì •ê°’

```python
# ë””ë°”ì´ìŠ¤ë³„ ê¸°ë³¸ ì„¤ì •
DEFAULT_CONFIGS = {
    "mps": {
        "batch_size": 8,
        "fp16": False,
        "dataloader_pin_memory": False,
        "max_sequence_length": 512
    },
    "cuda": {
        "batch_size": 16,
        "fp16": True,
        "dataloader_pin_memory": True,
        "max_sequence_length": 1024
    },
    "cpu": {
        "batch_size": 4,
        "fp16": False,
        "dataloader_pin_memory": False,
        "max_sequence_length": 256
    }
}

# ì§€ì›í•˜ëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜
SUPPORTED_ARCHITECTURES = [
    "kobart",      # gogamza/kobart-base-v2
    "kt5",         # KETI-AIR/ke-t5-base
    "mt5",         # google/mt5-base
    "kogpt2"       # skt/kogpt2-base-v2
]

# ROUGE ë©”íŠ¸ë¦­ ì¢…ë¥˜
ROUGE_METRICS = ["rouge1", "rouge2", "rougeL"]

# íŒŒì¼ í™•ì¥ì
SUPPORTED_FILE_EXTENSIONS = [".csv", ".json", ".jsonl"]

# ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°
DEFAULT_DIRECTORIES = {
    "data": "data",
    "outputs": "outputs",
    "models": "outputs/models",
    "experiments": "outputs/experiments",
    "submissions": "outputs/submissions"
}
```

---

## ë²„ì „ ì •ë³´

### API ë²„ì „ íˆìŠ¤í† ë¦¬

#### v1.0.0 (í˜„ì¬)
- **ì¶œì‹œ ë‚ ì§œ**: 2024ë…„ 7ì›”
- **ì£¼ìš” ê¸°ëŠ¥**:
  - Multi-reference ROUGE ì™„ì „ ì§€ì›
  - ìƒëŒ€ ê²½ë¡œ ê¸°ë°˜ PathManager ë„ì…
  - MPS/CUDA ìë™ ê°ì§€ ë° ìµœì í™”
  - ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ êµ¬ì¶•
  - ëŒ€íšŒ ì œì¶œ í˜•ì‹ ì™„ë²½ ì§€ì›

#### í˜¸í™˜ì„± ì •ë³´

```python
# ìµœì†Œ ìš”êµ¬ì‚¬í•­
MINIMUM_REQUIREMENTS = {
    "python": "3.8+",
    "torch": "2.0.0+",
    "transformers": "4.30.0+",
    "pandas": "1.5.0+",
    "numpy": "1.24.0+"
}

# í”Œë«í¼ ì§€ì›
SUPPORTED_PLATFORMS = {
    "macOS": "12.0+ (Apple Silicon ìµœì í™”)",
    "Ubuntu": "20.04+ (CUDA ì§€ì›)",
    "Windows": "10+ (ì‹¤í—˜ì  ì§€ì›)"
}
```

---

## í™•ì¥ ê°€ì´ë“œ

### ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ ì¶”ê°€

```python
class CustomRougeCalculator(RougeCalculator):
    """í™•ì¥ëœ ROUGE ê³„ì‚°ê¸° ì˜ˆì œ"""
    
    def compute_bertscore(self, 
                         predictions: List[str], 
                         references: List[str]) -> Dict[str, float]:
        """BERTScore ê³„ì‚° (í™•ì¥ ì˜ˆì œ)"""
        
        try:
            from bert_score import score
            P, R, F1 = score(predictions, references, lang="ko")
            
            return {
                "bertscore_precision": float(P.mean()),
                "bertscore_recall": float(R.mean()),
                "bertscore_f1": float(F1.mean())
            }
        except ImportError:
            print("âš ï¸ bert-score íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return {}
    
    def compute_comprehensive_metrics(self, 
                                    predictions: List[str],
                                    references_list: List[List[str]]) -> Dict[str, Any]:
        """ì¢…í•© ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        # ê¸°ë³¸ ROUGE
        rouge_scores = self.compute_multi_reference_rouge(predictions, references_list)
        
        # BERTScore (ì²« ë²ˆì§¸ ì°¸ì¡° ì‚¬ìš©)
        single_refs = [refs[0] for refs in references_list if refs]
        bert_scores = self.compute_bertscore(predictions, single_refs)
        
        return {
            "rouge": rouge_scores,
            "bertscore": bert_scores
        }
```

### ìƒˆë¡œìš´ ë°ì´í„° í˜•ì‹ ì§€ì›

```python
class ExtendedDataProcessor(DataProcessor):
    """í™•ì¥ëœ ë°ì´í„° í”„ë¡œì„¸ì„œ ì˜ˆì œ"""
    
    def load_jsonl_data(self, file_path: PathLike) -> pd.DataFrame:
        """JSONL í˜•ì‹ ë°ì´í„° ë¡œë”©"""
        
        if isinstance(file_path, str):
            file_path = Path(file_path)
        
        if file_path.is_absolute():
            raise ValueError(f"ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”: {file_path}")
        
        full_path = PathManager.resolve_path(file_path)
        
        import json
        data = []
        
        with open(full_path, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line.strip()))
        
        df = pd.DataFrame(data)
        print(f"ğŸ“Š JSONL ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df)} ìƒ˜í”Œ")
        
        return df
```

---

## ë§ˆë¬´ë¦¬

ì´ API ì°¸ì¡° ë¬¸ì„œëŠ” NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œì˜ ëª¨ë“  í•µì‹¬ ëª¨ë“ˆì— ëŒ€í•œ ì™„ì „í•œ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì œê³µí•©ë‹ˆë‹¤. 

### í•µì‹¬ íŠ¹ì§•
- **ğŸ“‚ ìƒëŒ€ ê²½ë¡œ ê°•ì œ**: í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„± ë³´ì¥
- **âš¡ ìë™ ìµœì í™”**: ë””ë°”ì´ìŠ¤ë³„ ìµœì  ì„¤ì • ìë™ ì ìš©  
- **ğŸ” ì™„ì „í•œ íƒ€ì… íŒíŠ¸**: IDE ì§€ì› ë° íƒ€ì… ì•ˆì „ì„±
- **ğŸ“Š ì‹¤í—˜ ì¶”ì **: ì²´ê³„ì ì¸ ì‹¤í—˜ ê´€ë¦¬ ë° ë¶„ì„
- **ğŸ¯ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥**: ëª¨ë“  ì˜ˆì œ ì½”ë“œê°€ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥

### ê°œë°œ ì›Œí¬í”Œë¡œìš°
1. **ë°ì´í„° ë¡œë”©**: `DataProcessor`ë¡œ multi-reference ë°ì´í„° ì²˜ë¦¬
2. **ëª¨ë¸ í•™ìŠµ**: ë””ë°”ì´ìŠ¤ ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
3. **ì„±ëŠ¥ í‰ê°€**: `RougeCalculator`ë¡œ ì •í™•í•œ ROUGE ê³„ì‚°
4. **ì‹¤í—˜ ê´€ë¦¬**: `ExperimentTracker`ì™€ `ModelRegistry`ë¡œ ì²´ê³„ì  ì¶”ì 
5. **ì¶”ë¡  ì‹¤í–‰**: `InferenceEngine`ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
6. **ê²°ê³¼ ì œì¶œ**: ëŒ€íšŒ í˜•ì‹ì— ë§ëŠ” ìë™ í¬ë§·íŒ…

### ì‚¬ìš© ê°€ì´ë“œë¼ì¸
- **í•­ìƒ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©**: ì ˆëŒ€ ê²½ë¡œëŠ” `ValueError` ë°œìƒ
- **ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ í™œìš©**: `get_optimal_device()` ì‚¬ìš© ê¶Œì¥
- **ë°°ì¹˜ í¬ê¸° ìµœì í™”**: ë””ë°”ì´ìŠ¤ë³„ ê¶Œì¥ ì„¤ì • ì°¸ì¡°
- **ì‹¤í—˜ ì¶”ì  í•„ìˆ˜**: ëª¨ë“  í•™ìŠµì— `ExperimentTracker` ì‚¬ìš©
- **ë©”ëª¨ë¦¬ ê´€ë¦¬ ì£¼ì˜**: GPU ì‚¬ìš© ì‹œ ì •ê¸°ì ì¸ `torch.cuda.empty_cache()`

ëª¨ë“  APIëŠ” **ì‹¤ì œ ìš´ì˜ í™˜ê²½**ì—ì„œ ê²€ì¦ë˜ì—ˆìœ¼ë©°, **Mac MPS**ì™€ **Ubuntu CUDA** í™˜ê²½ì—ì„œ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
