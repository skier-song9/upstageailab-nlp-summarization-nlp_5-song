# NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œ API ì°¸ì¡°

ì´ ë¬¸ì„œëŠ” NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œì˜ ì „ì²´ APIë¥¼ í¬ê´„ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ê° ëª¨ë“ˆë³„ ìƒì„¸ ê¸°ëŠ¥ê³¼ ì‚¬ìš©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“š ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [í•µì‹¬ ëª¨ë“ˆ](#í•µì‹¬-ëª¨ë“ˆ)
3. [ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ](#ìœ í‹¸ë¦¬í‹°-ëª¨ë“ˆ)
4. [ë°ì´í„° ì¦ê°• ëª¨ë“ˆ](#ë°ì´í„°-ì¦ê°•-ëª¨ë“ˆ)
5. [ìŠ¤í¬ë¦½íŠ¸ ë° ìë™í™”](#ìŠ¤í¬ë¦½íŠ¸-ë°-ìë™í™”)
6. [ì„¤ì¹˜ ë° ì„¤ì •](#ì„¤ì¹˜-ë°-ì„¤ì •)
7. [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)

## ê°œìš”

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
nlp-sum-lyj/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ core/                    # í•µì‹¬ ì¶”ë¡  ì—”ì§„
â”‚   â”œâ”€â”€ utils/                   # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ data_augmentation/       # ë°ì´í„° ì¦ê°•
â”‚   â”œâ”€â”€ trainer.py               # ë©”ì¸ íŠ¸ë ˆì´ë„ˆ
â”‚   â””â”€â”€ auto_experiment_runner.py # ìë™ ì‹¤í—˜ ì‹¤í–‰
â”œâ”€â”€ config/                      # ì„¤ì • íŒŒì¼
â”œâ”€â”€ data/                       # ë°ì´í„° ë””ë ‰í† ë¦¬
â””â”€â”€ docs/                       # ë¬¸ì„œ
```

### ì£¼ìš” íŠ¹ì§•

- **ëª¨ë“ˆì‹ ì„¤ê³„**: ê° ê¸°ëŠ¥ì´ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
- **ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€**: CUDA/MPS/CPU ìë™ ìµœì í™”
- **WandB í†µí•©**: ì‹¤í—˜ ì¶”ì  ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- **ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›**: BART, GPT, T5 ë“± ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜
- **ë°°ì¹˜ ì²˜ë¦¬**: íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

## í•µì‹¬ ëª¨ë“ˆ

### ğŸ“– [trainer.py](./trainer_api.md) - ë©”ì¸ íŠ¸ë ˆì´ë„ˆ ëª¨ë“ˆ

ëŒ€í™” ìš”ì•½ ëª¨ë¸ì˜ í•™ìŠµ, í‰ê°€, ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆì…ë‹ˆë‹¤.

```python
from trainer import DialogueSummarizationTrainer, create_trainer

# íŠ¸ë ˆì´ë„ˆ ìƒì„±
trainer = create_trainer("config/base_config.yaml")

# í•™ìŠµ ì‹¤í–‰
datasets = trainer.prepare_data()
result = trainer.train(datasets)
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `DialogueSummarizationTrainer`: ë©”ì¸ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤
- `TrainingResult`: í•™ìŠµ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤
- `WandbCallback`: WandB ë¡œê¹… ì½œë°±

**ì£¼ìš” ê¸°ëŠ¥:**
- ì™„ì „ ìë™í™”ëœ ì‹¤í—˜ ê´€ë¦¬
- WandB Sweep í†µí•©
- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¶”ì 
- ëª¨ë¸ ë“±ë¡ ì‹œìŠ¤í…œ

### ğŸ” [core/inference.py](./core_api.md) - ì¶”ë¡  ì—”ì§„

ë…ë¦½ì ì¸ ì¶”ë¡  ì—”ì§„ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬ ë° ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤.

```python
from core.inference import InferenceEngine, InferenceConfig

# ì¶”ë¡  ì—”ì§„ ì„¤ì •
config = InferenceConfig(
    model_path="models/best_model",
    batch_size=16,
    max_target_length=256
)

# ì¶”ë¡  ì‹¤í–‰
engine = InferenceEngine(config)
result = engine.predict_single("ëŒ€í™” í…ìŠ¤íŠ¸")
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `InferenceEngine`: ë©”ì¸ ì¶”ë¡  ì—”ì§„
- `InferenceConfig`: ì¶”ë¡  ì„¤ì • í´ë˜ìŠ¤

**ì£¼ìš” ê¸°ëŠ¥:**
- ë‹¨ì¼/ë°°ì¹˜ ì˜ˆì¸¡
- DataFrame ì²˜ë¦¬
- ìºì‹œ ì‹œìŠ¤í…œ
- ìë™ ë””ë°”ì´ìŠ¤ ìµœì í™”

### ğŸ¤– [auto_experiment_runner.py](./automation_api.md) - ìë™ ì‹¤í—˜ ì‹¤í–‰

YAML ì„¤ì • ê¸°ë°˜ì˜ ì™„ì „ ìë™í™”ëœ ì‹¤í—˜ ì‹¤í–‰ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

```python
from auto_experiment_runner import AutoExperimentRunner

# ìë™ ì‹¤í—˜ ì‹¤í–‰
runner = AutoExperimentRunner("experiments/")
runner.run_all_experiments()
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `AutoExperimentRunner`: ìë™ ì‹¤í—˜ ì‹¤í–‰ê¸°

**ì£¼ìš” ê¸°ëŠ¥:**
- YAML ì„¤ì • ê¸°ë°˜ ì‹¤í—˜ ì •ì˜
- ìˆœì°¨ì  ì‹¤í—˜ ì‹¤í–‰
- ê²°ê³¼ ìë™ ì¶”ì 
- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬

## ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

### âš™ï¸ [utils/config_manager.py](./utils_api.md#config-manager) - ì„¤ì • ê´€ë¦¬

YAML ì„¤ì • íŒŒì¼ ë¡œë”©, ë³‘í•©, ê²€ì¦ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

```python
from utils.config_manager import ConfigManager

config_manager = ConfigManager()
config = config_manager.load_config("config/base_config.yaml")

# Sweep íŒŒë¼ë¯¸í„° ë³‘í•©
merged_config = config_manager.merge_sweep_params(config, wandb.config)
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `ConfigManager`: ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤

**ì£¼ìš” ê¸°ëŠ¥:**
- YAML ì„¤ì • ë¡œë”©
- ë™ì  íŒŒë¼ë¯¸í„° ë³‘í•©
- ì„¤ì • ê²€ì¦
- í™˜ê²½ë³€ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ

### ğŸ“Š [utils/data_utils.py](./utils_api.md#data-utils) - ë°ì´í„° ì²˜ë¦¬

ë°ì´í„° ì „ì²˜ë¦¬, í† í¬ë‚˜ì´ì§•, ë°ì´í„°ì…‹ ë³€í™˜ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

```python
from utils.data_utils import DataProcessor, TextPreprocessor

processor = DataProcessor(tokenizer, config)
dataset = processor.process_data(raw_data, is_training=True)
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `DataProcessor`: ë©”ì¸ ë°ì´í„° í”„ë¡œì„¸ì„œ
- `TextPreprocessor`: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê¸°
- `DialogueSummarizationDataset`: PyTorch ë°ì´í„°ì…‹

**ì£¼ìš” ê¸°ëŠ¥:**
- í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ì œ
- í† í¬ë‚˜ì´ì§• ë° ì¸ì½”ë”©
- ë°ì´í„° í†µê³„ ë¶„ì„
- ë°°ì¹˜ ì²˜ë¦¬

### ğŸ”§ [utils/device_utils.py](./utils_api.md#device-utils) - ë””ë°”ì´ìŠ¤ ê´€ë¦¬

CUDA, MPS, CPU ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ìµœì í™”ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.

```python
from utils.device_utils import get_optimal_device, setup_device_config

device, device_info = get_optimal_device()
opt_config = setup_device_config(device_info, model_size="base")
```

**ì£¼ìš” í•¨ìˆ˜:**
- `get_optimal_device()`: ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
- `setup_device_config()`: ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •
- `detect_cuda_devices()`: CUDA ë””ë°”ì´ìŠ¤ ì •ë³´
- `detect_mps_device()`: MPS ë””ë°”ì´ìŠ¤ ì •ë³´

**ì£¼ìš” ê¸°ëŠ¥:**
- í”Œë«í¼ë³„ ìµœì í™”
- ë©”ëª¨ë¦¬ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ì¡°ì •
- Mixed Precision ì„¤ì •

### ğŸ“ˆ [utils/metrics.py](./utils_api.md#metrics) - í‰ê°€ ë©”íŠ¸ë¦­

ROUGE ì ìˆ˜ ê³„ì‚° ë° ë‹¤ì¤‘ ì°¸ì¡° í‰ê°€ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.

```python
from utils.metrics import RougeCalculator, MultiReferenceROUGE

calculator = RougeCalculator(tokenizer)
scores = calculator.compute_metrics(predictions, references)
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `RougeCalculator`: ROUGE ì ìˆ˜ ê³„ì‚°ê¸°
- `MultiReferenceROUGE`: ë‹¤ì¤‘ ì°¸ì¡° ROUGE
- `MetricTracker`: ë©”íŠ¸ë¦­ ì¶”ì ê¸°

**ì£¼ìš” ê¸°ëŠ¥:**
- ë‹¨ì¼/ë‹¤ì¤‘ ì°¸ì¡° ROUGE
- í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì§€ì›
- HuggingFace Trainer í˜¸í™˜
- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¶”ì 

### ğŸ§ª [utils/experiment_utils.py](./utils_api.md#experiment-utils) - ì‹¤í—˜ ê´€ë¦¬

ì‹¤í—˜ ì¶”ì , ëª¨ë¸ ë“±ë¡, ì„±ëŠ¥ ë¹„êµë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.

```python
from utils.experiment_utils import ExperimentTracker, ModelRegistry

tracker = ExperimentTracker()
experiment_id = tracker.start_experiment(name="test_exp", config=config)

registry = ModelRegistry()
model_id = registry.register_model(name="best_model", performance=metrics)
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `ExperimentTracker`: ì‹¤í—˜ ì¶”ì ê¸°
- `ModelRegistry`: ëª¨ë¸ ë“±ë¡ì†Œ
- `ExperimentConfig`: ì‹¤í—˜ ì„¤ì •

**ì£¼ìš” ê¸°ëŠ¥:**
- ì‹¤í—˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬
- ëª¨ë¸ ë²„ì „ ê´€ë¦¬
- ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„
- WandB ì—°ë™

### ğŸ“‚ [utils/path_utils.py](./utils_api.md#path-utils) - ê²½ë¡œ ê´€ë¦¬

í¬ë¡œìŠ¤ í”Œë«í¼ ê²½ë¡œ ê´€ë¦¬ ë° ë””ë ‰í† ë¦¬ ìë™ ìƒì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

```python
from utils.path_utils import PathManager, path_manager

# ì „ì—­ ê²½ë¡œ ê´€ë¦¬ì ì‚¬ìš©
data_path = path_manager.get_data_path("train.csv")
output_path = path_manager.get_output_path("experiment_1")
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `PathManager`: ê²½ë¡œ ê´€ë¦¬ í´ë˜ìŠ¤

**ì£¼ìš” ê¸°ëŠ¥:**
- í”„ë¡œì íŠ¸ ë£¨íŠ¸ ìë™ ê°ì§€
- í”Œë«í¼ ë…ë¦½ì  ê²½ë¡œ
- ë””ë ‰í† ë¦¬ ìë™ ìƒì„±
- ìƒëŒ€ ê²½ë¡œ í•´ê²°

## ë°ì´í„° ì¦ê°• ëª¨ë“ˆ

### ğŸ”„ [data_augmentation/simple_augmentation.py](./data_augmentation_api.md#simple) - ê¸°ë³¸ ì¦ê°•

ë™ì˜ì–´ ì¹˜í™˜, ë¬¸ì¥ ìˆœì„œ ë³€ê²½ ë“±ì˜ ê¸°ë³¸ ë°ì´í„° ì¦ê°• ê¸°ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

```python
from data_augmentation.simple_augmentation import SimpleAugmenter

augmenter = SimpleAugmenter()
augmented_data = augmenter.augment_dataset(dataset, augment_ratio=0.3)
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `SimpleAugmenter`: ê¸°ë³¸ ì¦ê°•ê¸°
- `SynonymReplacement`: ë™ì˜ì–´ ì¹˜í™˜
- `SentenceReorder`: ë¬¸ì¥ ìˆœì„œ ë³€ê²½

### ğŸŒ [data_augmentation/backtranslation.py](./data_augmentation_api.md#backtranslation) - ë°±ë²ˆì—­

ë‹¤êµ­ì–´ ë°±ë²ˆì—­ì„ í†µí•œ ê³ ê¸‰ ë°ì´í„° ì¦ê°•ì„ ì œê³µí•©ë‹ˆë‹¤.

```python
from data_augmentation.backtranslation import BackTranslationAugmenter

augmenter = BackTranslationAugmenter(method="google")
augmented_data = augmenter.augment(text, target_lang="en")
```

**ì£¼ìš” í´ë˜ìŠ¤:**
- `BackTranslationAugmenter`: ë°±ë²ˆì—­ ì¦ê°•ê¸°
- `MultilingualBackTranslation`: ë‹¤êµ­ì–´ ë°±ë²ˆì—­

## ìŠ¤í¬ë¦½íŠ¸ ë° ìë™í™”

### ğŸ”„ [sweep_runner.py](./scripts_api.md#sweep) - Sweep ì‹¤í–‰

WandB Sweepì„ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

```bash
python sweep_runner.py --config config/sweep/basic_sweep.yaml
```

### âš¡ [parallel_sweep_runner.py](./scripts_api.md#parallel-sweep) - ë³‘ë ¬ Sweep

ë‹¤ì¤‘ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•œ ë³‘ë ¬ Sweep ì‹¤í–‰ì„ ì œê³µí•©ë‹ˆë‹¤.

```bash
python parallel_sweep_runner.py --config config/sweep/parallel_sweep.yaml --agents 4
```

### ğŸ¯ [run_inference.py](./scripts_api.md#inference) - ì¶”ë¡  ì‹¤í–‰

ëŒ€íšŒ ì œì¶œìš© ì¶”ë¡  ê²°ê³¼ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

```bash
python run_inference.py --model models/best_model --input data/test.csv --output submissions/
```

## ì„¤ì¹˜ ë° ì„¤ì •

### ìš”êµ¬ì‚¬í•­

```bash
pip install -r requirements.txt
```

**ì£¼ìš” ì˜ì¡´ì„±:**
- `torch >= 1.12.0`
- `transformers >= 4.20.0`
- `datasets >= 2.0.0`
- `wandb >= 0.13.0`
- `pandas >= 1.3.0`
- `numpy >= 1.21.0`

### í™˜ê²½ ì„¤ì •

```bash
# WandB ì„¤ì •
wandb login

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ì„ íƒì‚¬í•­)
export WANDB_PROJECT="nlp-dialogue-summarization"
export CUDA_VISIBLE_DEVICES="0"
```

## ë¹ ë¥¸ ì‹œì‘

### 1. ê¸°ë³¸ í•™ìŠµ

```python
from trainer import create_trainer

# ì„¤ì • ë¡œë”© ë° íŠ¸ë ˆì´ë„ˆ ìƒì„±
trainer = create_trainer("config/base_config.yaml")

# ë°ì´í„° ì¤€ë¹„
datasets = trainer.prepare_data(
    train_path="data/train.csv",
    val_path="data/validation.csv"
)

# í•™ìŠµ ì‹¤í–‰
result = trainer.train(datasets)
print(f"Best ROUGE F1: {result.best_metrics['rouge_combined_f1']:.4f}")
```

### 2. ì¶”ë¡  ì‹¤í–‰

```python
from core.inference import InferenceEngine, InferenceConfig

# ì¶”ë¡  ì„¤ì •
config = InferenceConfig(
    model_path="models/best_model",
    batch_size=16
)

# ì¶”ë¡  ì—”ì§„ ìƒì„± ë° ì‹¤í–‰
engine = InferenceEngine(config)
result = engine.predict_single("ë‘ ì‚¬ëŒì´ ì»¤í”¼ìˆì—ì„œ ë§Œë‚˜ì„œ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ´ë‹¤.")
print(f"ìš”ì•½: {result}")
```

### 3. ìë™ ì‹¤í—˜ ì‹¤í–‰

```python
from auto_experiment_runner import AutoExperimentRunner

# ì‹¤í—˜ ì‹¤í–‰ê¸° ìƒì„±
runner = AutoExperimentRunner("config/experiments/")

# ëª¨ë“  ì‹¤í—˜ ìë™ ì‹¤í–‰
results = runner.run_all_experiments()
for exp_name, result in results.items():
    print(f"{exp_name}: {result['best_metrics']['rouge_combined_f1']:.4f}")
```

### 4. ë°ì´í„° ì¦ê°•

```python
from data_augmentation.simple_augmentation import SimpleAugmenter

# ì¦ê°•ê¸° ìƒì„±
augmenter = SimpleAugmenter()

# ë°ì´í„° ì¦ê°• ì‹¤í–‰
original_data = [
    {"input": "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.", "target": "ë‚ ì”¨ ì¸ì‚¬"}
]

augmented_data = augmenter.augment_dataset(
    original_data, 
    augment_ratio=0.5
)
print(f"Original: {len(original_data)}, Augmented: {len(augmented_data)}")
```

## ì—ëŸ¬ ì²˜ë¦¬

### ì¼ë°˜ì ì¸ ì—ëŸ¬ì™€ í•´ê²°ë°©ë²•

1. **CUDA Out of Memory**
   ```python
   # ë°°ì¹˜ í¬ê¸° ê°ì†Œ
   config['training']['per_device_train_batch_size'] = 4
   config['training']['gradient_accumulation_steps'] = 4
   ```

2. **ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨**
   ```python
   # ê²½ë¡œ í™•ì¸
   from utils.path_utils import path_manager
   model_path = path_manager.resolve_path("models/best_model")
   print(f"Resolved path: {model_path}")
   ```

3. **WandB ì—°ê²° ì‹¤íŒ¨**
   ```bash
   wandb login
   export WANDB_MODE=offline  # ì˜¤í”„ë¼ì¸ ëª¨ë“œ
   ```

## ì„±ëŠ¥ ìµœì í™”

### ê¶Œì¥ ì„¤ì •

```python
# CUDA ì‚¬ìš© ì‹œ
config = {
    'training': {
        'fp16': True,
        'gradient_checkpointing': True,
        'per_device_train_batch_size': 32
    }
}

# MPS (Apple Silicon) ì‚¬ìš© ì‹œ
config = {
    'training': {
        'fp16': False,
        'per_device_train_batch_size': 16,
        'dataloader_num_workers': 2
    }
}

# CPU ì‚¬ìš© ì‹œ
config = {
    'training': {
        'per_device_train_batch_size': 4,
        'gradient_accumulation_steps': 8,
        'dataloader_num_workers': 1
    }
}
```

## ë¼ì´ì„¼ìŠ¤ ë° ê¸°ì—¬

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ê¸°ì—¬ë¥¼ ì›í•˜ì‹œë©´ ì´ìŠˆë¥¼ ë“±ë¡í•˜ê±°ë‚˜ í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ ì œì¶œí•´ ì£¼ì„¸ìš”.

## ê´€ë ¨ ë¬¸ì„œ

- [ì‚¬ìš©ì ê°€ì´ë“œ](../02_user_guides/README.md)
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](./system_architecture.md)
- [ì„±ëŠ¥ ìµœì í™”](./performance_optimization.md)
- [ë¬¸ì œ í•´ê²°](../06_troubleshooting/README.md)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-01-28  
**ë²„ì „**: 2.0.0  
**ì‘ì„±ì**: NLP Summarization Team
