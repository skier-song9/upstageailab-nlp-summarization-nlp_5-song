# âš¡ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

ì‹œìŠ¤í…œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ìµœì í™” ì „ëµê³¼ êµ¬í˜„ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ìµœì í™” ì „ëµ](#ìµœì í™”-ì „ëµ)
- [ë©”ëª¨ë¦¬ ìµœì í™”](#ë©”ëª¨ë¦¬-ìµœì í™”)
- [ì—°ì‚° ìµœì í™”](#ì—°ì‚°-ìµœì í™”)
- [I/O ìµœì í™”](#io-ìµœì í™”)

## ğŸ¯ ìµœì í™” ì „ëµ

### ì„±ëŠ¥ ëª©í‘œ
- **ì¶”ë¡  ì†ë„**: ë°°ì¹˜ë‹¹ ì²˜ë¦¬ ì‹œê°„ < 100ms
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: GPU ë©”ëª¨ë¦¬ < 16GB
- **ì²˜ë¦¬ëŸ‰**: ì‹œê°„ë‹¹ 10,000 ìƒ˜í”Œ ì²˜ë¦¬
- **ì‘ë‹µ ì‹œê°„**: ì‚¬ìš©ì ìš”ì²­ ì‘ë‹µ < 2ì´ˆ

### ìµœì í™” ìš°ì„ ìˆœìœ„
1. **ë³‘ëª© ì§€ì  ì‹ë³„**: í”„ë¡œíŒŒì¼ë§ì„ í†µí•œ ì„±ëŠ¥ ì €í•˜ êµ¬ê°„ íŒŒì•…
2. **ì•Œê³ ë¦¬ì¦˜ ìµœì í™”**: ê³„ì‚° ë³µì¡ë„ ê°œì„ 
3. **í•˜ë“œì›¨ì–´ í™œìš©**: GPU/TPU ë³‘ë ¬ ì²˜ë¦¬ ê·¹ëŒ€í™”
4. **ìºì‹± ì „ëµ**: ì¤‘ë³µ ê³„ì‚° ë°©ì§€

## ğŸ§  ë©”ëª¨ë¦¬ ìµœì í™”

### Gradient Checkpointing
```python
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ
model.gradient_checkpointing_enable()

# ë˜ëŠ” íŠ¹ì • ë ˆì´ì–´ì—ë§Œ ì ìš©
@torch.utils.checkpoint.checkpoint
def forward_block(x, layer):
    return layer(x)
```

### í˜¼í•© ì •ë°€ë„ í•™ìŠµ
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
```python
def adaptive_batch_size(model, initial_batch_size=32):
    batch_size = initial_batch_size
    while batch_size > 1:
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
            test_batch = torch.randn(batch_size, seq_len, hidden_dim)
            _ = model(test_batch)
            return batch_size
        except RuntimeError as e:
            if "out of memory" in str(e):
                batch_size //= 2
                torch.cuda.empty_cache()
            else:
                raise e
```

## ğŸš€ ì—°ì‚° ìµœì í™”

### íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
```python
# Flash Attention ì‚¬ìš©
from flash_attn import flash_attn_func

def efficient_attention(q, k, v):
    return flash_attn_func(q, k, v, dropout_p=0.1, causal=True)
```

### ëª¨ë¸ ì–‘ìí™”
```python
# 8ë¹„íŠ¸ ì–‘ìí™”
import torch.quantization as quant

model_quantized = quant.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# ë˜ëŠ” ë” ê³µê²©ì ì¸ ì–‘ìí™”
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)
```

### ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
```python
# ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬
model = torch.nn.DataParallel(model)

# ë˜ëŠ” ë¶„ì‚° ì²˜ë¦¬
from torch.nn.parallel import DistributedDataParallel as DDP
model = DDP(model, device_ids=[local_rank])
```

## ğŸ’¾ I/O ìµœì í™”

### ë¹„ë™ê¸° ë°ì´í„° ë¡œë”©
```python
class AsyncDataLoader:
    def __init__(self, dataset, batch_size, num_workers=4):
        self.dataloader = DataLoader(
            dataset, 
            batch_size=batch_size,
            num_workers=num_workers,
            pin_memory=True,
            prefetch_factor=2
        )
    
    def __iter__(self):
        return iter(self.dataloader)
```

### ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```python
# GPUì—ì„œ ì „ì²˜ë¦¬ ìˆ˜í–‰
def gpu_preprocessing(batch):
    batch = batch.to(device, non_blocking=True)
    # í† í°í™” ë° ì •ê·œí™”ë¥¼ GPUì—ì„œ ìˆ˜í–‰
    return preprocess_on_gpu(batch)
```

### ìºì‹± ì‹œìŠ¤í…œ
```python
from functools import lru_cache
import diskcache as dc

# ë©”ëª¨ë¦¬ ìºì‹œ
@lru_cache(maxsize=1000)
def cached_tokenize(text):
    return tokenizer(text)

# ë””ìŠ¤í¬ ìºì‹œ
cache = dc.Cache('/tmp/model_cache')

@cache.memoize(expire=3600)  # 1ì‹œê°„ ìºì‹œ
def cached_inference(input_text):
    return model.generate(input_text)
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### í”„ë¡œíŒŒì¼ë§ ë„êµ¬
```python
# PyTorch Profiler ì‚¬ìš©
with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    record_shapes=True,
    profile_memory=True,
) as prof:
    model(inputs)

print(prof.key_averages().table(sort_by="cuda_time_total"))
```

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **GPU ì‚¬ìš©ë¥ **: nvidia-smië¥¼ í†µí•œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: í”¼í¬ ë° í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
- **ì²˜ë¦¬ëŸ‰**: ì´ˆë‹¹ ì²˜ë¦¬ ìƒ˜í”Œ ìˆ˜ ì¸¡ì •
- **ì§€ì—° ì‹œê°„**: ìš”ì²­-ì‘ë‹µ ì‹œê°„ ë¶„í¬ ë¶„ì„

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- **ì—°ê³„**: [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](./system_architecture.md)
- **ì—°ê³„**: [ì—ëŸ¬ ì²˜ë¦¬](./error_handling.md)
- **ì‹¬í™”**: [ëª¨ë¸ í•™ìŠµ](../02_user_guides/model_training/README.md)

---
ğŸ“ **ìœ„ì¹˜**: `docs/03_technical_docs/performance_optimization.md`
