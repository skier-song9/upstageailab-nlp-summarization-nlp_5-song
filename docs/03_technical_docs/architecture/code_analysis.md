# ğŸ” ì½”ë“œ ë¶„ì„ ê°€ì´ë“œ

í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ë“¤ì— ëŒ€í•œ ìƒì„¸í•œ ì½”ë“œ ë¶„ì„ê³¼ êµ¬í˜„ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
1. [ì „ì²´ ì•„í‚¤í…ì²˜ ë¶„ì„](#ì „ì²´-ì•„í‚¤í…ì²˜-ë¶„ì„)
2. [ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë¶„ì„](#ë°ì´í„°-íŒŒì´í”„ë¼ì¸-ë¶„ì„)
3. [ëª¨ë¸ êµ¬í˜„ ë¶„ì„](#ëª¨ë¸-êµ¬í˜„-ë¶„ì„)
4. [í•™ìŠµ ì‹œìŠ¤í…œ ë¶„ì„](#í•™ìŠµ-ì‹œìŠ¤í…œ-ë¶„ì„)
5. [ì¶”ë¡  ì—”ì§„ ë¶„ì„](#ì¶”ë¡ -ì—”ì§„-ë¶„ì„)
6. [ì„±ëŠ¥ ìµœì í™” ë¶„ì„](#ì„±ëŠ¥-ìµœì í™”-ë¶„ì„)

---

## ì „ì²´ ì•„í‚¤í…ì²˜ ë¶„ì„

### ë ˆì´ì–´ë“œ ì•„í‚¤í…ì²˜ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Presentation Layer       â”‚  â† CLI, API Endpoints
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Application Layer        â”‚  â† trainers.py, inference.py
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Domain Layer           â”‚  â† core/models, core/training
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Infrastructure Layer      â”‚  â† utils/, config/
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì„¤ê³„ ì›ì¹™:**
- **ê´€ì‹¬ì‚¬ ë¶„ë¦¬**: ê° ë ˆì´ì–´ëŠ” ëª…í™•í•œ ì±…ì„
- **ì˜ì¡´ì„± ì—­ì „**: ìƒìœ„ ë ˆì´ì–´ê°€ í•˜ìœ„ ë ˆì´ì–´ì— ì˜ì¡´
- **ìƒëŒ€ ê²½ë¡œ ê°•ì œ**: ëª¨ë“  íŒŒì¼ ì ‘ê·¼ì€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€

### í•µì‹¬ ë””ìì¸ íŒ¨í„´

#### 1. Factory Pattern (ëª¨ë¸ ìƒì„±)
```python
# core/models/model_factory.py
class ModelFactory:
    SUPPORTED_MODELS = {
        "kobart": {
            "base_model": "gogamza/kobart-base-v2",
            "tokenizer_class": "BartTokenizer",
            "model_class": "BartForConditionalGeneration"
        }
    }
    
    @classmethod
    def create_model_and_tokenizer(cls, model_name: str, device: str):
        """íŒ©í† ë¦¬ ë©”ì„œë“œë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ìƒì„±"""
        config = cls.SUPPORTED_MODELS[model_name]
        
        # ë™ì  í´ë˜ìŠ¤ ë¡œë”©
        tokenizer_class = getattr(transformers, config["tokenizer_class"])
        model_class = getattr(transformers, config["model_class"])
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë””ë°”ì´ìŠ¤ ìµœì í™”
        tokenizer = tokenizer_class.from_pretrained(config["base_model"])
        model = model_class.from_pretrained(config["base_model"])
        
        return model.to(device), tokenizer, config
```

**ì¥ì :**
- ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€ ì‹œ ì½”ë“œ ìˆ˜ì • ìµœì†Œí™”
- ì¼ê´€ëœ ëª¨ë¸ ë¡œë”© ì¸í„°í˜ì´ìŠ¤
- ì„¤ì • ê¸°ë°˜ ëª¨ë¸ ê´€ë¦¬

#### 2. Strategy Pattern (ë””ë°”ì´ìŠ¤ ìµœì í™”)
```python
# utils/device_utils.py
class DeviceStrategy:
    """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì „ëµ"""
    
    @abstractmethod
    def get_config(self) -> Dict[str, Any]:
        pass
    
    @abstractmethod
    def optimize_model(self, model):
        pass

class CudaStrategy(DeviceStrategy):
    def get_config(self):
        return {
            "batch_size": 16,
            "fp16": True,
            "dataloader_pin_memory": True
        }
    
    def optimize_model(self, model):
        return model.cuda().half()  # FP16 ìµœì í™”

class MPSStrategy(DeviceStrategy):
    def get_config(self):
        return {
            "batch_size": 8,
            "fp16": False,  # MPS FP16 ì´ìŠˆ íšŒí”¼
            "dataloader_pin_memory": False
        }
    
    def optimize_model(self, model):
        return model.to("mps")
```

#### 3. Observer Pattern (ì‹¤í—˜ ì¶”ì )
```python
# utils/experiment_utils.py
class ExperimentObserver:
    """ì‹¤í—˜ ìƒíƒœ ë³€í™” ê°ì§€"""
    
    def __init__(self):
        self.observers = []
    
    def attach(self, observer):
        self.observers.append(observer)
    
    def notify(self, event, data):
        for observer in self.observers:
            observer.update(event, data)

class WandBObserver:
    def update(self, event, data):
        if event == "metrics_updated":
            wandb.log(data)

class FileObserver:
    def update(self, event, data):
        if event == "experiment_completed":
            self.save_results(data)
```

---

## ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë¶„ì„

### Multi-Reference ë°ì´í„° ì²˜ë¦¬

```python
# utils/data_utils.py
class DataProcessor:
    """ë‹¤ì¤‘ ì°¸ì¡° ìš”ì•½ ë°ì´í„° ì „ìš© í”„ë¡œì„¸ì„œ"""
    
    def _detect_and_convert_format(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë‹¤ì¤‘ ì°¸ì¡° í˜•ì‹ ìë™ ê°ì§€ ë° í‘œì¤€í™”"""
        
        # í˜•ì‹ 1: ê°œë³„ ì»¬ëŸ¼ (summary1, summary2, summary3)
        if all(col in df.columns for col in ['summary1', 'summary2', 'summary3']):
            df['summaries'] = df[['summary1', 'summary2', 'summary3']].apply(
                lambda x: [str(val) if pd.notna(val) else "" for val in x], axis=1
            )
            
        # í˜•ì‹ 2: êµ¬ë¶„ì ë¶„ë¦¬ (summary ì»¬ëŸ¼ì— ||| êµ¬ë¶„ì)
        elif 'summary' in df.columns:
            df['summaries'] = df['summary'].apply(self._parse_multiple_summaries)
            
        return df
```

**í•µì‹¬ ì„¤ê³„ ê²°ì •:**
- **ìœ ì—°í•œ ì…ë ¥ í˜•ì‹**: 3ê°€ì§€ ë‹¤ì¤‘ ì°¸ì¡° í˜•ì‹ ìë™ ê°ì§€
- **í‘œì¤€í™”ëœ ì¶œë ¥**: í•­ìƒ `summaries` ì»¬ëŸ¼ì— ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥
- **ì—ëŸ¬ ì²˜ë¦¬**: ì˜ëª»ëœ í˜•ì‹ì— ëŒ€í•œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€

### ë°ì´í„° ê²€ì¦ ì²´ê³„

```python
def validate_submission_format(self, file_path: Union[str, Path]) -> bool:
    """ì œì¶œ íŒŒì¼ í˜•ì‹ ê²€ì¦"""
    
    try:
        df = pd.read_csv(full_path)
        
        # 1. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['fname', 'summary']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            self.logger.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            return False
        
        # 2. ë°ì´í„° íƒ€ì… í™•ì¸
        if not all(isinstance(fname, str) for fname in df['fname']):
            self.logger.error("fname ì»¬ëŸ¼ì´ ë¬¸ìì—´ íƒ€ì…ì´ ì•„ë‹™ë‹ˆë‹¤")
            return False
        
        # 3. ë¹ˆ ê°’ í™•ì¸
        if df['summary'].isna().any():
            self.logger.error("summary ì»¬ëŸ¼ì— ë¹ˆ ê°’ì´ ìˆìŠµë‹ˆë‹¤")
            return False
            
        return True
        
    except Exception as e:
        self.logger.error(f"íŒŒì¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False
```

---

## ëª¨ë¸ êµ¬í˜„ ë¶„ì„

### ëŒ€í™” ìš”ì•½ ëª¨ë¸ ë˜í¼

```python
# core/models/summarization_model.py
class DialogueSummarizationModel:
    """ëŒ€í™” ìš”ì•½ ì „ìš© ëª¨ë¸ ë˜í¼"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        if device == "auto":
            device = get_optimal_device()
        
        self.device = device
        self.model, self.tokenizer, self.config = ModelFactory.create_model_and_tokenizer(
            model_name, device
        )
        
        # ëŒ€í™” ìš”ì•½ íŠ¹í™” ì„¤ì •
        self._setup_generation_config()
        self._setup_special_tokens()
    
    def _setup_generation_config(self):
        """ìƒì„± ì„¤ì • ìµœì í™”"""
        self.generation_config = {
            "max_length": 128,
            "min_length": 10,
            "num_beams": 4,
            "length_penalty": 1.0,
            "no_repeat_ngram_size": 3,
            "early_stopping": True,
            "do_sample": False
        }
    
    def preprocess_dialogue(self, dialogue: str) -> str:
        """ëŒ€í™” ì „ì²˜ë¦¬"""
        
        # ê¸°ë³¸ ì •ë¦¬
        dialogue = dialogue.strip()
        
        # í™”ì êµ¬ë¶„ ì •ê·œí™” (ì˜µì…˜)
        dialogue = re.sub(r'í™”ì\s*(\d+)\s*:', r'<speaker\1>:', dialogue)
        
        # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        dialogue = re.sub(r'\s+', ' ', dialogue)
        
        return dialogue
```

**ì„¤ê³„ íŠ¹ì§•:**
- **ì–´ëŒ‘í„° íŒ¨í„´**: Transformers ëª¨ë¸ì„ ëŒ€í™” ìš”ì•½ì— íŠ¹í™”
- **í…œí”Œë¦¿ ë©”ì„œë“œ**: ê³µí†µ ì „ì²˜ë¦¬ ë¡œì§ê³¼ ëª¨ë¸ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§• ë¶„ë¦¬
- **ì„¤ì • ì£¼ì…**: ìƒì„± íŒŒë¼ë¯¸í„°ë¥¼ ì™¸ë¶€ì—ì„œ ì£¼ì… ê°€ëŠ¥

---

## í•™ìŠµ ì‹œìŠ¤í…œ ë¶„ì„

### ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬

```python
# core/training/adaptive_trainer.py
class AdaptiveBatchProcessor:
    """ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì • í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, device: str, initial_batch_size: int = 8):
        self.device = device
        self.current_batch_size = initial_batch_size
        self.max_batch_size = self._get_max_batch_size()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.success_count = 0
        self.oom_count = 0
    
    def process_batch(self, data_loader, process_fn):
        """ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬"""
        
        results = []
        
        for batch in data_loader:
            try:
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹œë„
                batch_results = process_fn(batch)
                results.extend(batch_results)
                
                self.success_count += 1
                
                # ì„±ê³µ ì‹œ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê³ ë ¤
                if (self.success_count % 10 == 0 and 
                    self.current_batch_size < self.max_batch_size):
                    self.current_batch_size = min(
                        self.current_batch_size + 1, 
                        self.max_batch_size
                    )
                
            except torch.cuda.OutOfMemoryError:
                # OOM ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
                self._handle_oom()
                continue
        
        return results
    
    def _handle_oom(self):
        """OOM ì²˜ë¦¬ ë¡œì§"""
        self.oom_count += 1
        old_size = self.current_batch_size
        self.current_batch_size = max(
            self.current_batch_size // 2, 
            1  # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
        )
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device == "cuda":
            torch.cuda.empty_cache()
```

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜:**
1. **ì ì§„ì  ì¦ê°€**: ì„±ê³µ ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì²œì²œíˆ ì¦ê°€
2. **ê¸‰ê²©í•œ ê°ì†Œ**: OOM ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
3. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ìºì‹œ ì •ë¦¬

---

## ì¶”ë¡  ì—”ì§„ ë¶„ì„

### ë°°ì¹˜ ì¶”ë¡  ìµœì í™”

```python
# core/inference/inference_engine.py
class InferenceEngine:
    """ìµœì í™”ëœ ì¶”ë¡  ì—”ì§„"""
    
    def predict_batch(self, dialogues: List[str], batch_size: int = 8, **kwargs) -> List[str]:
        """ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰"""
        
        # ë””ë°”ì´ìŠ¤ë³„ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
        device_config = get_device_config(self.device)
        optimal_batch_size = min(batch_size, device_config['batch_size'])
        
        results = []
        
        for i in tqdm(range(0, len(dialogues), optimal_batch_size)):
            batch_dialogues = dialogues[i:i + optimal_batch_size]
            
            # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                batch_dialogues,
                max_length=self.max_input_length,
                truncation=True,
                padding=True,
                return_tensors="pt"
            ).to(self.device)
            
            # ë°°ì¹˜ ì¶”ë¡ 
            with torch.no_grad():
                if self.device == "mps":
                    # MPS ìµœì í™”
                    with torch.autocast(device_type="cpu", enabled=False):
                        outputs = self.model.generate(**inputs, **self.generation_config)
                else:
                    outputs = self.model.generate(**inputs, **self.generation_config)
            
            # ë°°ì¹˜ ë””ì½”ë”©
            batch_summaries = self.tokenizer.batch_decode(
                outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True
            )
            
            results.extend([summary.strip() for summary in batch_summaries])
        
        return results
```

**ìµœì í™” ê¸°ë²•:**
- **ë””ë°”ì´ìŠ¤ë³„ ë°°ì¹˜ í¬ê¸°**: ë©”ëª¨ë¦¬ì— ë§ëŠ” ìë™ ì¡°ì •
- **MPS íŠ¹ë³„ ì²˜ë¦¬**: Apple Siliconì˜ autocast ì´ìŠˆ íšŒí”¼
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: `torch.no_grad()` ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©

---

## ì„±ëŠ¥ ìµœì í™” ë¶„ì„

### ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

```python
# utils/performance.py
def profile_memory_usage(func):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§ ë°ì½”ë ˆì´í„°"""
    
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        import psutil
        import torch
        
        # ì‹œì‘ ë©”ëª¨ë¦¬
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        if torch.cuda.is_available():
            gpu_memory_before = torch.cuda.memory_allocated() / 1024 / 1024
        else:
            gpu_memory_before = 0
        
        try:
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ì¢…ë£Œ ë©”ëª¨ë¦¬
            memory_after = process.memory_info().rss / 1024 / 1024
            memory_diff = memory_after - memory_before
            
            if torch.cuda.is_available():
                gpu_memory_after = torch.cuda.memory_allocated() / 1024 / 1024
                gpu_memory_diff = gpu_memory_after - gpu_memory_before
            else:
                gpu_memory_diff = 0
            
            print(f"ğŸ’¾ {func.__name__} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
            print(f"  System: {memory_diff:+.1f} MB")
            if gpu_memory_diff:
                print(f"  GPU: {gpu_memory_diff:+.1f} MB")
            
            return result
            
        finally:
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    return wrapper
```

### ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„

**1. ë°ì´í„° ë¡œë”© ë³‘ëª©**
```python
# ê°œì„  ì „: ìˆœì°¨ ì²˜ë¦¬
def load_data_slow(file_paths):
    data = []
    for file_path in file_paths:
        df = pd.read_csv(file_path)
        data.append(df)
    return pd.concat(data)

# ê°œì„  í›„: ë³‘ë ¬ ì²˜ë¦¬
def load_data_fast(file_paths):
    with ThreadPoolExecutor(max_workers=4) as executor:
        dfs = list(executor.map(pd.read_csv, file_paths))
    return pd.concat(dfs)
```

**2. í† í¬ë‚˜ì´ì§• ë³‘ëª©**
```python
# ê°œì„  ì „: ë°˜ë³µ í† í¬ë‚˜ì´ì§•
def tokenize_slow(dialogues, tokenizer):
    tokens = []
    for dialogue in dialogues:
        token = tokenizer(dialogue, return_tensors="pt")
        tokens.append(token)
    return tokens

# ê°œì„  í›„: ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
def tokenize_fast(dialogues, tokenizer):
    return tokenizer(
        dialogues, 
        padding=True, 
        truncation=True, 
        return_tensors="pt"
    )
```

**3. ëª¨ë¸ ì¶”ë¡  ë³‘ëª©**
```python
# ê°œì„  ì „: ê°œë³„ ì¶”ë¡ 
def predict_slow(model, inputs):
    results = []
    for input_tensor in inputs:
        with torch.no_grad():
            output = model(input_tensor.unsqueeze(0))
        results.append(output)
    return results

# ê°œì„  í›„: ë°°ì¹˜ ì¶”ë¡ 
def predict_fast(model, inputs):
    with torch.no_grad():
        batch_inputs = torch.stack(inputs)
        batch_outputs = model(batch_inputs)
    return batch_outputs
```

---

## ğŸ”§ ì½”ë“œ í’ˆì§ˆ ë¶„ì„

### 1. ë³µì¡ë„ ë¶„ì„

**ìˆœí™˜ ë³µì¡ë„ (Cyclomatic Complexity):**
- `DataProcessor._detect_and_convert_format()`: 4 (ì–‘í˜¸)
- `InferenceEngine.predict_batch()`: 6 (ë³´í†µ)
- `ExperimentTracker.log_metrics()`: 3 (ì–‘í˜¸)

**ê°œì„  ê¶Œì¥ì‚¬í•­:**
```python
# ë³µì¡ë„ê°€ ë†’ì€ í•¨ìˆ˜ ë¶„í•´
def complex_function(data):
    # Before: ë†’ì€ ë³µì¡ë„
    if condition1:
        if condition2:
            if condition3:
                # ë³µì¡í•œ ë¡œì§
                pass
    
    # After: í•¨ìˆ˜ ë¶„í•´
    if not self._validate_conditions(data):
        return None
    
    return self._process_data(data)

def _validate_conditions(self, data):
    return condition1 and condition2 and condition3

def _process_data(self, data):
    # ë¶„ë¦¬ëœ ì²˜ë¦¬ ë¡œì§
    pass
```

### 2. ì˜ì¡´ì„± ë¶„ì„

**ì˜ì¡´ì„± ê·¸ë˜í”„:**
```
core/models -> utils/device_utils
core/training -> core/models, utils/metrics
core/inference -> core/models, utils/path_utils
utils/* -> (ìˆœí™˜ ì˜ì¡´ì„± ì—†ìŒ)
```

**ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´:**
```python
# ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í–¥ìƒ
class ModelTrainer:
    def __init__(self, 
                 model_factory: ModelFactory,
                 metric_calculator: RougeCalculator,
                 logger: StructuredLogger):
        self.model_factory = model_factory
        self.metric_calculator = metric_calculator
        self.logger = logger
```

### 3. í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„

**í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ì„¤ê³„:**
```python
# ìˆœìˆ˜ í•¨ìˆ˜ë¡œ ì„¤ê³„ (í…ŒìŠ¤íŠ¸ ìš©ì´)
def normalize_text(text: str) -> str:
    """ë¶€ì‘ìš© ì—†ëŠ” ìˆœìˆ˜ í•¨ìˆ˜"""
    return re.sub(r'\s+', ' ', text.strip())

# ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ëª¨í‚¹ ê°€ëŠ¥
class DataProcessor:
    def __init__(self, path_manager: PathManager = None):
        self.path_manager = path_manager or PathManager()
```

---

## ğŸš€ ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­

### 1. ë©”ëª¨ë¦¬ ìµœì í™”
- **ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •**: OOM ë°©ì§€
- **ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì ˆì•½
- **ëª¨ë¸ ë³‘ë ¬í™”**: í° ëª¨ë¸ ë¶„ì‚° ì²˜ë¦¬

### 2. ì†ë„ ìµœì í™”
- **JIT ì»´íŒŒì¼**: `torch.jit.script()` ì‚¬ìš©
- **ONNX ë³€í™˜**: ì¶”ë¡  ì†ë„ 2-3ë°° í–¥ìƒ
- **í…ì„œ ì—°ì‚° ìµœì í™”**: ë¶ˆí•„ìš”í•œ CPU-GPU ì „ì†¡ ì œê±°

### 3. ì½”ë“œ í’ˆì§ˆ ê°œì„ 
- **íƒ€ì… íŒíŠ¸ ì™„ì„±**: mypy ì •ì  ë¶„ì„
- **ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í™•ëŒ€**: 90% ì´ìƒ ì»¤ë²„ë¦¬ì§€ ëª©í‘œ
- **ì½”ë“œ ë¦¬ë·° ìë™í™”**: pre-commit í›… í™œìš©

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [í•µì‹¬ ëª¨ë“ˆ API](./core_modules.md) - API ì°¸ì¡°
- [ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜](./utilities.md) - í—¬í¼ í•¨ìˆ˜ë“¤
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](../architecture/project_structure.md) - ì „ì²´ êµ¬ì¡°
- [ì„±ëŠ¥ ìµœì í™”](../../02_user_guides/evaluation/performance_analysis.md) - ì„±ëŠ¥ ê°€ì´ë“œ

---

ì´ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  íš¨ê³¼ì ì¸ í™•ì¥ ë° ê°œì„ ì„ ì§„í–‰í•˜ì„¸ìš”.
