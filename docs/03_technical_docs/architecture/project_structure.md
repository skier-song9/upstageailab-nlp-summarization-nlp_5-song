# í”„ë¡œì íŠ¸ êµ¬ì¡° ë° ì¶”ê°€ íŒŒì¼ ë¶„ì„

## 1. í”„ë¡œì íŠ¸ ì „ì²´ êµ¬ì¡°

```
nlp-sum-lyj/
â”œâ”€â”€ code/                   # ì†ŒìŠ¤ ì½”ë“œ ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ baseline.ipynb     # ë©”ì¸ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œ
â”‚   â”œâ”€â”€ solar_api.ipynb    # Solar API í™œìš© ì½”ë“œ
â”‚   â”œâ”€â”€ config.yaml        # ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ requirements.txt   # í•„ìš” íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ data/                  # ë°ì´í„° ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ train.csv         # í•™ìŠµ ë°ì´í„° (12,457ê°œ)
â”‚   â”œâ”€â”€ dev.csv           # ê²€ì¦ ë°ì´í„° (499ê°œ)
â”‚   â”œâ”€â”€ test.csv          # í…ŒìŠ¤íŠ¸ ë°ì´í„° (250ê°œ)
â”‚   â””â”€â”€ sample_submission.csv  # ì œì¶œ ì–‘ì‹
â””â”€â”€ docs/                  # ë¬¸ì„œ ë””ë ‰í† ë¦¬
    â”œâ”€â”€ competition_overview.md     # ëŒ€íšŒ ê°œìš”
    â”œâ”€â”€ baseline_code_analysis.md   # ë² ì´ìŠ¤ë¼ì¸ ë¶„ì„
    â””â”€â”€ rouge_metrics_detail.md     # ROUGE ì§€í‘œ ì„¤ëª…
```

## 2. config.yaml ìƒì„¸ ë¶„ì„

### 2.1 ì „ì²´ êµ¬ì¡°

```yaml
general:          # ì¼ë°˜ ì„¤ì •
tokenizer:        # í† í¬ë‚˜ì´ì € ì„¤ì •
training:         # í•™ìŠµ ì„¤ì •
wandb:           # ì‹¤í—˜ ê´€ë¦¬ ì„¤ì •
inference:       # ì¶”ë¡  ì„¤ì •
```

### 2.2 General ì„¤ì •

```yaml
general:
  data_path: ../data/                    # ë°ì´í„° ê²½ë¡œ
  model_name: digit82/kobart-summarization  # ì‚¬ìš© ëª¨ë¸
  output_dir: ./                         # ì¶œë ¥ ë””ë ‰í† ë¦¬
```

**ì„¤ëª…**:
- `data_path`: train.csv, dev.csv, test.csvê°€ ìˆëŠ” ë””ë ‰í† ë¦¬
- `model_name`: Hugging Face Model Hubì˜ ëª¨ë¸ ì´ë¦„
- `output_dir`: ì²´í¬í¬ì¸íŠ¸ì™€ ë¡œê·¸ê°€ ì €ì¥ë  ìœ„ì¹˜

### 2.3 Tokenizer ì„¤ì •

```yaml
tokenizer:
  bos_token: <s>           # Beginning of Sentence
  eos_token: </s>          # End of Sentence
  encoder_max_len: 512     # ì…ë ¥ ìµœëŒ€ ê¸¸ì´
  decoder_max_len: 100     # ì¶œë ¥ ìµœëŒ€ ê¸¸ì´
  special_tokens:          # íŠ¹ìˆ˜ í† í° ëª©ë¡
    - '#Person1#'          # í™”ì 1
    - '#Person2#'          # í™”ì 2
    - '#Person3#'          # í™”ì 3
    - '#PhoneNumber#'      # ì „í™”ë²ˆí˜¸ ë§ˆìŠ¤í‚¹
    - '#Address#'          # ì£¼ì†Œ ë§ˆìŠ¤í‚¹
    - '#PassportNumber#'   # ì—¬ê¶Œë²ˆí˜¸ ë§ˆìŠ¤í‚¹
```

**íŠ¹ìˆ˜ í† í°ì˜ ì¤‘ìš”ì„±**:
- í™”ì êµ¬ë¶„ì(`#Person1#` ë“±)ê°€ ì„œë¸Œì›Œë“œë¡œ ë¶„í•´ë˜ì§€ ì•Šë„ë¡ ë³´í˜¸
- ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ í† í°ë„ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬

### 2.4 Training ì„¤ì •

```yaml
training:
  # ê¸°ë³¸ í•™ìŠµ ì„¤ì •
  num_train_epochs: 20              # ì—í­ ìˆ˜
  learning_rate: 1.0e-05           # í•™ìŠµë¥  (0.00001)
  per_device_train_batch_size: 50  # GPUë‹¹ ë°°ì¹˜ í¬ê¸°
  per_device_eval_batch_size: 32   # í‰ê°€ì‹œ ë°°ì¹˜ í¬ê¸°
  
  # ìµœì í™” ì„¤ì •
  optim: adamw_torch               # AdamW ì˜µí‹°ë§ˆì´ì €
  lr_scheduler_type: cosine        # ì½”ì‚¬ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
  warmup_ratio: 0.1                # 10% ì›Œë°ì—…
  weight_decay: 0.01               # ê°€ì¤‘ì¹˜ ê°ì‡ 
  
  # í•™ìŠµ ì „ëµ
  gradient_accumulation_steps: 1    # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
  fp16: true                       # í˜¼í•© ì •ë°€ë„ í•™ìŠµ
  seed: 42                         # ëœë¤ ì‹œë“œ
  
  # í‰ê°€ ë° ì €ì¥
  evaluation_strategy: epoch        # ì—í­ë§ˆë‹¤ í‰ê°€
  save_strategy: epoch             # ì—í­ë§ˆë‹¤ ì €ì¥
  save_total_limit: 5              # ìµœëŒ€ 5ê°œ ì²´í¬í¬ì¸íŠ¸
  load_best_model_at_end: true     # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
  
  # ì¡°ê¸° ì¢…ë£Œ
  early_stopping_patience: 3        # 3ì—í­ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ
  early_stopping_threshold: 0.001   # ìµœì†Œ ê°œì„  í­
  
  # ìƒì„± ì„¤ì •
  predict_with_generate: true       # ìƒì„± ëª¨ë“œë¡œ í‰ê°€
  generation_max_length: 100        # ìƒì„± ìµœëŒ€ ê¸¸ì´
  
  # ë¡œê¹…
  logging_dir: ./logs              # ë¡œê·¸ ë””ë ‰í† ë¦¬
  logging_strategy: epoch          # ì—í­ë§ˆë‹¤ ë¡œê¹…
  report_to: wandb                 # wandbì— ë¦¬í¬íŠ¸
```

### 2.5 Inference ì„¤ì •

```yaml
inference:
  batch_size: 32                    # ì¶”ë¡  ë°°ì¹˜ í¬ê¸°
  ckt_path: model ckt path         # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
  result_path: ./prediction/        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
  
  # ìƒì„± ì„¤ì •
  no_repeat_ngram_size: 2          # 2-gram ë°˜ë³µ ë°©ì§€
  early_stopping: true             # ì¡°ê¸° ì¢…ë£Œ ì‚¬ìš©
  generate_max_length: 100         # ìµœëŒ€ ìƒì„± ê¸¸ì´
  num_beams: 4                     # ë¹” ì„œì¹˜ í¬ê¸°
  
  # í›„ì²˜ë¦¬
  remove_tokens:                   # ì œê±°í•  í† í°
    - <usr>
    - <s>
    - </s>
    - <pad>
```

### 2.6 WandB ì„¤ì •

```yaml
wandb:
  entity: wandb_repo    # WandB ê³„ì •/ì¡°ì§ëª…
  project: project_name # í”„ë¡œì íŠ¸ëª…
  name: run_name       # ì‹¤í–‰ ì´ë¦„
```

## 3. requirements.txt ë¶„ì„

### 3.1 í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

```txt
# ë°ì´í„° ì²˜ë¦¬
pandas==2.1.4          # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
numpy==1.23.5          # ìˆ˜ì¹˜ ì—°ì‚°

# ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
pytorch_lightning==2.1.2    # PyTorch ë˜í¼
transformers[torch]==4.35.2  # Hugging Face Transformers

# í‰ê°€ ë° ëª¨ë‹ˆí„°ë§
rouge==1.0.1          # ROUGE í‰ê°€ ì§€í‘œ
wandb==0.16.1         # ì‹¤í—˜ ì¶”ì  ë° ì‹œê°í™”

# ìœ í‹¸ë¦¬í‹°
tqdm==4.66.1          # ì§„í–‰ ë°” í‘œì‹œ

# ê°œë°œ í™˜ê²½
jupyter==1.0.0        # Jupyter ë…¸íŠ¸ë¶
jupyterlab==4.0.9     # JupyterLab í™˜ê²½
```

### 3.2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í˜¸í™˜ì„±

- **PyTorch**: transformers[torch] ì„¤ì¹˜ ì‹œ ìë™ìœ¼ë¡œ í˜¸í™˜ ë²„ì „ ì„¤ì¹˜
- **CUDA**: GPU ì‚¬ìš© ì‹œ CUDA 11.x ì´ìƒ ê¶Œì¥
- **Python**: 3.8 ì´ìƒ ê¶Œì¥

## 4. ë°ì´í„° í˜•ì‹ ë¶„ì„

### 4.1 train.csv / dev.csv êµ¬ì¡°

```csv
fname,dialogue,summary,topic
train_0,"#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤...",
"Mr. Smithê°€ ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ëŸ¬ ì™”ìŠµë‹ˆë‹¤...",health
```

**ì»¬ëŸ¼ ì„¤ëª…**:
- `fname`: ëŒ€í™” ê³ ìœ  ID (train_0, train_1, ...)
- `dialogue`: ëŒ€í™” ë‚´ìš© (í™”ìëŠ” #PersonN#ìœ¼ë¡œ êµ¬ë¶„)
- `summary`: ìš”ì•½ë¬¸ (ì •ë‹µ)
- `topic`: ëŒ€í™” ì£¼ì œ (ì°¸ê³ ìš©, í•™ìŠµì— ë¯¸ì‚¬ìš©)

### 4.2 test.csv êµ¬ì¡°

```csv
fname,dialogue
test_0,"#Person1#: ëŒ€í™” ë‚´ìš©..."
```

- `summary` ì»¬ëŸ¼ ì—†ìŒ (ì˜ˆì¸¡ ëŒ€ìƒ)

### 4.3 sample_submission.csv êµ¬ì¡°

```csv
fname,summary
test_0,"ì˜ˆì¸¡ëœ ìš”ì•½ë¬¸"
test_1,"ì˜ˆì¸¡ëœ ìš”ì•½ë¬¸"
...
```

## 5. ì‹¤í–‰ ê°€ì´ë“œ

### 5.1 í™˜ê²½ ì„¤ì •

#### ë°©ë²• 1: ê¸°ì¡´ ë°©ì‹ (pip)
```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv dialogue_sum_env
source dialogue_sum_env/bin/activate  # Windows: .\dialogue_sum_env\Scripts\activate

# 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 3. CUDA í™•ì¸ (GPU ì‚¬ìš© ì‹œ)
python -c "import torch; print(torch.cuda.is_available())"
```

#### ë°©ë²• 2: uv ì‚¬ìš© (ê¶Œì¥ - 10ë°° ì´ìƒ ë¹ ë¦„)
```bash
# 1. uv ì„¤ì¹˜ (ì²˜ìŒ í•œ ë²ˆë§Œ)
pip install uv

# 2. ê°€ìƒí™˜ê²½ ìƒì„± (0.1ì´ˆ!)
uv venv dialogue_sum_env
source dialogue_sum_env/bin/activate  # Windows: .\dialogue_sum_env\Scripts\activate

# 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ë§¤ìš° ë¹ ë¦„!)
uv pip install -r requirements.txt

# 4. Lock íŒŒì¼ ìƒì„± (íŒ€ í˜‘ì—…ìš©)
uv pip compile requirements.txt -o requirements.lock

# 5. CUDA í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"
```

> ğŸ’¡ **ì‹¤ì œ ì¸¡ì • ê²°ê³¼**: 
> - pip: ì•½ 90ì´ˆ
> - uv: ì•½ 7ì´ˆ (12ë°° ë¹ ë¦„!)
> 
> ìì„¸í•œ ë‚´ìš©ì€ [uv íŒ¨í‚¤ì§€ ê´€ë¦¬ì ê°€ì´ë“œ](uv_package_manager_guide.md) ì°¸ê³ 

### 5.2 í•™ìŠµ ì‹¤í–‰

```python
# baseline.ipynbì—ì„œ

# 1. config ìˆ˜ì •
loaded_config['general']['data_path'] = "your_data_path"
loaded_config['wandb']['entity'] = "your_wandb_account"
loaded_config['wandb']['project'] = "your_project_name"

# 2. í•™ìŠµ ì‹¤í–‰
main(loaded_config)
```

### 5.3 ì¶”ë¡  ì‹¤í–‰

```python
# 1. ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •
loaded_config['inference']['ckt_path'] = "./checkpoint-best"

# 2. ì¶”ë¡  ì‹¤í–‰
output = inference(loaded_config)

# 3. ê²°ê³¼ í™•ì¸
print(output.head())
```

## 6. ì„±ëŠ¥ ìµœì í™” íŒ

### 6.1 ë©”ëª¨ë¦¬ ìµœì í™”

```python
# ë°°ì¹˜ í¬ê¸° ì¡°ì •
config['training']['per_device_train_batch_size'] = 32  # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ê°ì†Œ

# Gradient Accumulation ì‚¬ìš©
config['training']['gradient_accumulation_steps'] = 2  # ì‹¤íš¨ ë°°ì¹˜ í¬ê¸° = 32 * 2 = 64

# fp16 ì‚¬ìš© (ì´ë¯¸ ì„¤ì •ë¨)
config['training']['fp16'] = True
```

### 6.2 í•™ìŠµ ì†ë„ ê°œì„ 

```python
# DataLoader ì›Œì»¤ ìˆ˜ ì¦ê°€ (ì½”ë“œ ìˆ˜ì • í•„ìš”)
dataloader = DataLoader(..., num_workers=4)

# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹ˆë„ ì¡°ì •
config['training']['save_strategy'] = 'steps'
config['training']['save_steps'] = 500
```

#### í™˜ê²½ ì„¤ì • ì†ë„ ê°œì„  (uv ì‚¬ìš©)
```bash
# ê¸°ì¡´ pip (ì•½ 90ì´ˆ)
time pip install -r requirements.txt

# uv ì‚¬ìš© (ì•½ 7ì´ˆ - 12ë°° ë¹ ë¦„!)
time uv pip install -r requirements.txt

# CI/CD íŒŒì´í”„ë¼ì¸ì—ì„œ íŠ¹íˆ ìœ ìš©
# ë¹Œë“œ ì‹œê°„ 90% ë‹¨ì¶• ê°€ëŠ¥
```

### 6.3 ì„±ëŠ¥ í–¥ìƒ

```python
# í•™ìŠµë¥  ì¡°ì •
config['training']['learning_rate'] = 5e-5  # ë˜ëŠ” 3e-5

# ì—í­ ìˆ˜ ì¦ê°€
config['training']['num_train_epochs'] = 30

# ë¹” ì„œì¹˜ í¬ê¸° ì¦ê°€
config['inference']['num_beams'] = 8

# ìµœëŒ€ ê¸¸ì´ ì¡°ì •
config['tokenizer']['decoder_max_len'] = 150
config['inference']['generate_max_length'] = 150
```

## 7. ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

### 7.1 CUDA Out of Memory

```python
# í•´ê²° ë°©ë²•
1. ë°°ì¹˜ í¬ê¸° ê°ì†Œ
2. gradient_accumulation_steps ì¦ê°€
3. ëª¨ë¸ì„ CPUë¡œ ì´ë™ í›„ ì¶”ë¡ 
4. torch.cuda.empty_cache() ì‚¬ìš©
```

### 7.2 í•™ìŠµì´ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ

```python
# í•´ê²° ë°©ë²•
1. í•™ìŠµë¥  ì¡°ì • (ë” ì‘ê²Œ)
2. ì›Œë°ì—… ë¹„ìœ¨ ì¦ê°€
3. ë°ì´í„° í’ˆì§ˆ í™•ì¸
4. ë” ê¸´ ì—í­ í•™ìŠµ
```

### 7.3 ìƒì„±ëœ ìš”ì•½ë¬¸ í’ˆì§ˆ ë¬¸ì œ

```python
# í•´ê²° ë°©ë²•
1. no_repeat_ngram_size ì¡°ì •
2. temperature íŒŒë¼ë¯¸í„° ì¶”ê°€ (0.8~1.0)
3. top_k, top_p ìƒ˜í”Œë§ ì‚¬ìš©
4. ë” í° ëª¨ë¸ ì‚¬ìš© (ì˜ˆ: kobart-base â†’ kobart-large)
```
