# ğŸš€ GPU ë©”ëª¨ë¦¬ë³„ ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ê°€ì´ë“œ

## ğŸ“‹ ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­

### mT5 ëª¨ë¸ (1.2B íŒŒë¼ë¯¸í„°)
- **ëª¨ë¸ ê°€ì¤‘ì¹˜**: ~2.4GB
- **í•™ìŠµì‹œ ë©”ëª¨ë¦¬**: ~10-15GB 
- **ì¶”ë¡ ì‹œ ë©”ëª¨ë¦¬**: ~4-6GB

### eenzeenee ëª¨ë¸ (220M íŒŒë¼ë¯¸í„°)  
- **ëª¨ë¸ ê°€ì¤‘ì¹˜**: ~440MB
- **í•™ìŠµì‹œ ë©”ëª¨ë¦¬**: ~3-5GB
- **ì¶”ë¡ ì‹œ ë©”ëª¨ë¦¬**: ~1-2GB

## âš™ï¸ GPU ë©”ëª¨ë¦¬ë³„ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°

### ğŸ”¥ V100 (16GB) - AIStages ì¼ë°˜ì  ì‚¬ì–‘
```yaml
# mT5 ëª¨ë¸ ì„¤ì •
xlsum_mt5:
  training:
    per_device_train_batch_size: 1    # ì•ˆì „
    per_device_eval_batch_size: 2     # ì ì ˆ
    gradient_accumulation_steps: 4    # íš¨ê³¼ì  ë°°ì¹˜=4
  inference:
    batch_size: 2                     # ì•ˆì „

# eenzeenee ëª¨ë¸ ì„¤ì •  
eenzeenee:
  training:
    per_device_train_batch_size: 4    # í˜„ì¬ ì„¤ì • (ì ì ˆ)
    per_device_eval_batch_size: 4     # í˜„ì¬ ì„¤ì • (ì ì ˆ)
    gradient_accumulation_steps: 2    # íš¨ê³¼ì  ë°°ì¹˜=8
  inference:
    batch_size: 8                     # í˜„ì¬ ì„¤ì • (ì ì ˆ)
```

### ğŸš€ A100 (40GB) - ê³ ì„±ëŠ¥ ì„œë²„
```yaml
# mT5 ëª¨ë¸ ì„¤ì •
xlsum_mt5:
  training:
    per_device_train_batch_size: 4    # ê¶Œì¥
    per_device_eval_batch_size: 8     # ê¶Œì¥
    gradient_accumulation_steps: 2    # íš¨ê³¼ì  ë°°ì¹˜=8
  inference:
    batch_size: 8                     # ê¶Œì¥

# eenzeenee ëª¨ë¸ ì„¤ì •
eenzeenee:
  training:
    per_device_train_batch_size: 8    # í–¥ìƒ ê°€ëŠ¥
    per_device_eval_batch_size: 8     # í–¥ìƒ ê°€ëŠ¥
    gradient_accumulation_steps: 2    # íš¨ê³¼ì  ë°°ì¹˜=16
  inference:
    batch_size: 16                    # í–¥ìƒ ê°€ëŠ¥
```

### ğŸ’¾ RTX 3080/4090 (10-24GB) - ë¡œì»¬ ê°œë°œ
```yaml
# mT5 ëª¨ë¸ ì„¤ì • (ì£¼ì˜ í•„ìš”)
xlsum_mt5:
  training:
    per_device_train_batch_size: 1    # ìµœì†Œ
    per_device_eval_batch_size: 1     # ìµœì†Œ  
    gradient_accumulation_steps: 8    # íš¨ê³¼ì  ë°°ì¹˜=8
  inference:
    batch_size: 1                     # ìµœì†Œ

# eenzeenee ëª¨ë¸ ì„¤ì •
eenzeenee:
  training:
    per_device_train_batch_size: 2    # ì•ˆì „
    per_device_eval_batch_size: 4     # ì¶”ë¡ ì€ ë” ì—¬ìœ 
  inference:
    batch_size: 4                     # ì ì ˆ
```

## ğŸ› ï¸ ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ë²•

### 1. Gradient Accumulation í™œìš©
```yaml
# ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = per_device_batch_size Ã— gradient_accumulation_steps
per_device_train_batch_size: 2
gradient_accumulation_steps: 4  # íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° = 8
```

### 2. í˜¼í•© ì •ë°€ë„ í•™ìŠµ
```yaml
training:
  fp16: true          # ë©”ëª¨ë¦¬ 50% ì ˆì•½
  # ë˜ëŠ”
  bf16: true          # A100ì—ì„œ ê¶Œì¥
```

### 3. DeepSpeed í™œìš© (í•„ìš”ì‹œ)
```yaml
training:
  deepspeed: "ds_config.json"
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
```

## ğŸ§ª ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. GPU ë©”ëª¨ë¦¬ í™•ì¸
```bash
# í˜„ì¬ GPU ìƒíƒœ í™•ì¸
nvidia-smi

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ 
watch -n 1 nvidia-smi
```

### 2. ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸
```bash
# ì‘ì€ ë°°ì¹˜ë¶€í„° ì‹œì‘
uv run python code/trainer.py \
    --config config.yaml \
    --config-section eenzeenee \
    --max_steps 10 \
    --save_steps 1000  # ì €ì¥ ë°©ì§€

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ í›„ ì ì§„ì  ì¦ê°€
```

### 3. ìë™ ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
```python
# Trainerì—ì„œ ìë™ íƒì§€ (ì‹¤í—˜ì )
training_args = TrainingArguments(
    auto_find_batch_size=True,  # ìë™ ë°°ì¹˜ í¬ê¸° íƒì§€
    # ... ê¸°íƒ€ ì„¤ì •
)
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì§•í›„
```bash
âŒ CUDA out of memory
âŒ RuntimeError: unable to create new native thread  
âŒ Killed (í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ)
```

### ëŒ€ì‘ ë°©ë²•
1. **ë°°ì¹˜ í¬ê¸° ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ**
2. **gradient_accumulation_steps 2ë°° ì¦ê°€**  
3. **sequence length ë‹¨ì¶•** (512 â†’ 256)
4. **fp16/bf16 í™œì„±í™”**

## ğŸ“Š ì„±ëŠ¥ vs ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„

| ë°°ì¹˜ í¬ê¸° | í•™ìŠµ ì†ë„ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ëª¨ë¸ ì„±ëŠ¥ |
|-----------|-----------|---------------|-----------|
| 1 | ëŠë¦¼ | ìµœì†Œ | ë¶ˆì•ˆì • |
| 2-4 | ë³´í†µ | ì ì • | ì•ˆì • |
| 8-16 | ë¹ ë¦„ | ë§ìŒ | ìµœì  |
| 32+ | ë§¤ìš° ë¹ ë¦„ | ê³¼ë‹¤ | ìˆ˜ë ´ ì–´ë ¤ì›€ |

## ğŸ¯ ê²°ë¡ 

**ì¡°ì¥ë‹˜ì˜ ì¡°ì–¸ëŒ€ë¡œ ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •ì´ í•„ìˆ˜ì…ë‹ˆë‹¤:**

1. **mT5 (1.2B)**: ë°°ì¹˜ í¬ê¸° 1-4 ê¶Œì¥
2. **eenzeenee (220M)**: í˜„ì¬ ì„¤ì •(4-8) ì ì ˆ  
3. **gradient_accumulation_steps**ë¡œ íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° í™•ë³´
4. **GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§** í•„ìˆ˜

**AIStages ì„œë²„ì—ì„œ ì•ˆì „í•œ ì‹¤í—˜ì„ ìœ„í•´ ë³´ìˆ˜ì ì¸ ë°°ì¹˜ í¬ê¸°ë¶€í„° ì‹œì‘í•˜ì„¸ìš”!** ğŸš€
