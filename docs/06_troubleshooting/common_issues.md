# ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

## ğŸ¯ ê°œìš”

ì´ ë¬¸ì„œëŠ” NLP ëŒ€í™” ìš”ì•½ í”„ë¡œì íŠ¸ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” **ì¼ë°˜ì ì¸ ë¬¸ì œë“¤ê³¼ í•´ê²° ë°©ë²•**ì„ ì •ë¦¬í•©ë‹ˆë‹¤. ë¬¸ì œ ìƒí™©ë³„ë¡œ ì²´ê³„ì ì¸ í•´ê²° ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ† ìµœì‹  ê¸°ìˆ  ìŠ¤íƒ ê´€ë ¨ ë¬¸ì œ (2024.12 ì—…ë°ì´íŠ¸)

### torch 2.6.0 í˜¸í™˜ì„± ë¬¸ì œ

#### transformers 4.54.0 ë²„ì „ ì¶©ëŒ
**ì¦ìƒ**:
```bash
VersionConflict: transformers 4.54.0 requires torch>=2.0.0
AttributeError: module 'torch' has no attribute 'compile'
```

**í•´ê²° ë°©ë²•**:
```bash
# 1. ì²´ê³„ì  ì—…ê·¸ë ˆì´ë“œ (ê¶Œì¥)
uv pip uninstall torch torchvision torchaudio transformers
uv pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0
uv pip install transformers==4.54.0

# 2. í˜¸í™˜ì„± í™•ì¸
python -c "
import torch
import transformers
print(f'torch: {torch.__version__}')
print(f'transformers: {transformers.__version__}')
print(f'torch.compile ì§€ì›: {hasattr(torch, "compile")}')
"
```

#### pytorch_lightning 2.5.2 ì—…ê·¸ë ˆì´ë“œ ë¬¸ì œ
**ì¦ìƒ**:
```bash
ImportError: cannot import name 'LightningModule' from 'pytorch_lightning'
AttributeError: 'Trainer' object has no attribute 'fit_loop'
```

**í•´ê²° ë°©ë²•**:
```bash
# ê¸°ì¡´ pytorch_lightning ì™„ì „ ì œê±°
uv pip uninstall pytorch_lightning lightning

# ìµœì‹  ë²„ì „ ì„¤ì¹˜
uv pip install pytorch_lightning==2.5.2

# ë˜ëŠ” ëŒ€ì•ˆì ìœ¼ë¡œ lightning ì„¤ì¹˜
uv pip install lightning==2.5.2
```

### unsloth/QLoRA ì„¤ì • ë¬¸ì œ

#### macOSì—ì„œ unsloth ì„¤ì¹˜ ì‹¤íŒ¨
**ì¦ìƒ**:
```bash
ERROR: Failed building wheel for sentencepiece
ERROR: Could not build wheels for xformers
```

**í•´ê²° ë°©ë²•**:
```yaml
# config.yaml ìˆ˜ì • - QLoRA ëª¨ë“œ ì‚¬ìš©
qlora:
  use_unsloth: false  # macOSì—ì„œëŠ” ë¹„í™œì„±í™”
  use_qlora: true     # QLoRAë¡œ ëŒ€ì²´
  lora_rank: 16
  load_in_4bit: true
```

```python
# ìˆ˜ë™ í™•ì¸
try:
    import unsloth
    print('âœ… unsloth ì‚¬ìš© ê°€ëŠ¥')
except ImportError:
    print('âš ï¸  unsloth ì—†ìŒ, QLoRA ëª¨ë“œ ì‚¬ìš©')
    
try:
    import peft, bitsandbytes
    print('âœ… QLoRA ì§€ì› (peft + bitsandbytes)')
except ImportError:
    print('âŒ QLoRA ì§€ì› ì—†ìŒ')
```

#### bitsandbytes CUDA ë²„ì „ ë¬¸ì œ
**ì¦ìƒ**:
```bash
RuntimeError: CUDA version mismatch: bitsandbytes was compiled with CUDA 11.8
```

**í•´ê²° ë°©ë²•**:
```bash
# CUDA ë²„ì „ í™•ì¸
nvcc --version
python -c "import torch; print(torch.version.cuda)"

# í˜¸í™˜ ë²„ì „ ì„¤ì¹˜
# CUDA 11.8 ì‚¬ìš© ì‹œ
uv pip install bitsandbytes==0.41.1

# CUDA 12.x ì‚¬ìš© ì‹œ
uv pip install bitsandbytes==0.43.0

# CPU ëª¨ë“œë¡œ ëŒ€ì²´
uv pip uninstall bitsandbytes
# config.yamlì—ì„œ load_in_4bit: falseë¡œ ì„¤ì •
```

### gradient checkpointing ë¬¸ì œ

#### use_reentrant ê²½ê³ 
**ì¦ìƒ**:
```bash
UserWarning: use_reentrant parameter should be passed explicitly
```

**í•´ê²° ë°©ë²•**:
```yaml
# config.yaml ìˆ˜ì •
training:
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false  # ìµœì‹  ë²„ì „ì—ì„œ ê¶Œì¥
```

---

## ğŸš¨ ê¸´ê¸‰ ë¬¸ì œ í•´ê²°

### ì‹œìŠ¤í…œì´ ì „í˜€ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°

#### 1. í™˜ê²½ ì„¤ì • ë¬¸ì œ

**ì¦ìƒ**: ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜, íŒ¨í‚¤ì§€ ì—†ìŒ ì˜¤ë¥˜
```bash
ModuleNotFoundError: No module named 'utils'
ImportError: No module named 'transformers'
```

**ì§„ë‹¨ ë‹¨ê³„**:
```bash
# 1. Python í™˜ê²½ í™•ì¸
python --version
# 3.8+ ë²„ì „ì´ì–´ì•¼ í•¨

# 2. ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
which python
# ê°€ìƒí™˜ê²½ ê²½ë¡œì—¬ì•¼ í•¨

# 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
pip list | grep transformers
pip list | grep torch
```

**í•´ê²° ë°©ë²•**:
```bash
# Option 1: UV í™˜ê²½ ì¬ì„¤ì • (ê¶Œì¥)
./scripts/setup_aistages.sh

# Option 2: ìˆ˜ë™ ì¬ì„¤ì¹˜
pip install -r code/requirements.txt

# Option 3: ìƒˆ í™˜ê²½ ìƒì„±
python -m venv nlp_env
source nlp_env/bin/activate  # Linux/Mac
# nlp_env\Scripts\activate  # Windows
pip install -r code/requirements.txt
```

#### 2. ê²½ë¡œ ë¬¸ì œ

**ì¦ìƒ**: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ì ˆëŒ€ ê²½ë¡œ ì˜¤ë¥˜
```bash
FileNotFoundError: [Errno 2] No such file or directory: '/Users/jayden/...'
```

**ì§„ë‹¨ ë‹¨ê³„**:
```python
# í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸
import os
print(f"Current directory: {os.getcwd()}")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ í™•ì¸
from pathlib import Path
project_files = ['code', 'config', 'docs']
for file in project_files:
    print(f"{file} exists: {Path(file).exists()}")
```

**í•´ê²° ë°©ë²•**:
```bash
# 1. ì˜¬ë°”ë¥¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /path/to/nlp-sum-lyj

# 2. í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
ls -la
# code/, config/, docs/ ë””ë ‰í† ë¦¬ê°€ ìˆì–´ì•¼ í•¨

# 3. Python ê²½ë¡œ ì„¤ì •
export PYTHONPATH="${PYTHONPATH}:$(pwd)/code"
```

---

## ğŸ”§ í™˜ê²½ ì„¤ì • ë¬¸ì œ

### CUDA/GPU ê´€ë ¨ ë¬¸ì œ

#### GPUë¥¼ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°

**ì¦ìƒ**:
```python
torch.cuda.is_available()  # False ë°˜í™˜
```

**ì§„ë‹¨ ë‹¨ê³„**:
```bash
# 1. NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# 2. CUDA ë²„ì „ í™•ì¸
nvcc --version

# 3. PyTorch CUDA ì§€ì› í™•ì¸
python -c "import torch; print(torch.version.cuda)"
```

**í•´ê²° ë°©ë²•**:
```bash
# CUDA í˜¸í™˜ PyTorch ì¬ì„¤ì¹˜
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# ë˜ëŠ” CPU ë²„ì „ ì‚¬ìš©
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

#### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜

**ì¦ìƒ**:
```bash
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**ì§„ë‹¨ ë‹¨ê³„**:
```python
import torch
print(f"GPU ë©”ëª¨ë¦¬ í˜„ì¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
print(f"GPU ë©”ëª¨ë¦¬ ìµœëŒ€ ì‚¬ìš©ëŸ‰: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB")
print(f"GPU ì „ì²´ ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
```

**í•´ê²° ë°©ë²•**:
```yaml
# config/base_config.yaml ìˆ˜ì •
training:
  per_device_train_batch_size: 2  # ê¸°ë³¸ê°’ 8ì—ì„œ 2ë¡œ ê°ì†Œ
  gradient_accumulation_steps: 4  # ë°°ì¹˜ í¬ê¸° ê°ì†Œ ë³´ìƒ
  fp16: true  # í˜¼í•© ì •ë°€ë„ í•™ìŠµ í™œì„±í™”

tokenizer:
  encoder_max_len: 512  # ê¸°ë³¸ê°’ 1024ì—ì„œ 512ë¡œ ê°ì†Œ
  decoder_max_len: 128  # ê¸°ë³¸ê°’ 256ì—ì„œ 128ë¡œ ê°ì†Œ
```

### íŒ¨í‚¤ì§€ ì˜ì¡´ì„± ì¶©ëŒ

#### ë²„ì „ ì¶©ëŒ ë¬¸ì œ

**ì¦ìƒ**:
```bash
ERROR: pip's dependency resolver does not currently take into account all the packages
```

**í•´ê²° ë°©ë²•**:
```bash
# 1. ëª¨ë“  íŒ¨í‚¤ì§€ ì œê±° í›„ ì¬ì„¤ì¹˜
pip freeze > installed_packages.txt
pip uninstall -r installed_packages.txt -y
pip install -r code/requirements.txt

# 2. UV ì‚¬ìš© (ê¶Œì¥)
curl -LsSf https://astral.sh/uv/install.sh | sh
uv venv
source .venv/bin/activate
uv pip install -r code/requirements.txt
```

---

## ğŸ“Š ë°ì´í„° ì²˜ë¦¬ ë¬¸ì œ

### ë°ì´í„° ë¡œë”© ì‹¤íŒ¨

#### ì¸ì½”ë”© ë¬¸ì œ

**ì¦ìƒ**:
```bash
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 0
```

**í•´ê²° ë°©ë²•**:
```python
# ìë™ ì¸ì½”ë”© ê°ì§€ ë° ì²˜ë¦¬
import pandas as pd
import chardet

def load_csv_with_encoding_detection(file_path):
    # ì¸ì½”ë”© ê°ì§€
    with open(file_path, 'rb') as f:
        result = chardet.detect(f.read())
        encoding = result['encoding']
    
    print(f"ê°ì§€ëœ ì¸ì½”ë”©: {encoding}")
    
    # ì—¬ëŸ¬ ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„
    encodings = [encoding, 'utf-8', 'cp949', 'euc-kr', 'latin-1']
    
    for enc in encodings:
        try:
            df = pd.read_csv(file_path, encoding=enc)
            print(f"ì„±ê³µí•œ ì¸ì½”ë”©: {enc}")
            return df
        except UnicodeDecodeError:
            continue
    
    raise ValueError("ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
```

#### CSV íŒŒì‹± ì˜¤ë¥˜

**ì¦ìƒ**:
```bash
pandas.errors.ParserError: Error tokenizing data
```

**í•´ê²° ë°©ë²•**:
```python
# ì•ˆì „í•œ CSV ì½ê¸°
import pandas as pd

def safe_read_csv(file_path):
    try:
        # ê¸°ë³¸ ì‹œë„
        df = pd.read_csv(file_path)
        return df
    except pd.errors.ParserError:
        try:
            # ì¿¼íŒ… ë¬¸ì œ í•´ê²°
            df = pd.read_csv(file_path, quoting=3)  # QUOTE_NONE
            return df
        except:
            try:
                # êµ¬ë¶„ì ë¬¸ì œ í•´ê²°
                df = pd.read_csv(file_path, sep=None, engine='python')
                return df
            except:
                # ìˆ˜ë™ íŒŒì‹±
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                print(f"ì²« 5ì¤„ í™•ì¸: {lines[:5]}")
                raise ValueError("CSV íŒŒì‹± ì‹¤íŒ¨ - ìˆ˜ë™ í™•ì¸ í•„ìš”")
```

### Multi-Reference ë°ì´í„° ë¬¸ì œ

#### ì •ë‹µ ìš”ì•½ë¬¸ í˜•ì‹ ì˜¤ë¥˜

**ì¦ìƒ**:
```bash
ValueError: summaries column not found or invalid format
```

**ì§„ë‹¨ ë‹¨ê³„**:
```python
import pandas as pd

# ë°ì´í„° êµ¬ì¡° í™•ì¸
df = pd.read_csv("data/train.csv")
print(f"ì»¬ëŸ¼ëª…: {df.columns.tolist()}")
print(f"ì²« ë²ˆì§¸ í–‰: {df.iloc[0].to_dict()}")

# summary ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
summary_cols = [col for col in df.columns if 'summary' in col.lower()]
print(f"Summary ì»¬ëŸ¼ë“¤: {summary_cols}")
```

**í•´ê²° ë°©ë²•**:
```python
# ìœ ì—°í•œ multi-reference ì²˜ë¦¬
def parse_multi_reference_summaries(df):
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ multi-reference ë°ì´í„° íŒŒì‹±"""
    
    # ë°©ë²• 1: summary1, summary2, summary3 ì»¬ëŸ¼
    if all(col in df.columns for col in ['summary1', 'summary2', 'summary3']):
        df['summaries'] = df[['summary1', 'summary2', 'summary3']].values.tolist()
        return df
    
    # ë°©ë²• 2: summary ì»¬ëŸ¼ì— êµ¬ë¶„ìë¡œ ë¶„ë¦¬
    elif 'summary' in df.columns:
        def split_summaries(summary_text):
            if pd.isna(summary_text):
                return ["", "", ""]
            
            # ë‹¤ì–‘í•œ êµ¬ë¶„ì ì‹œë„
            for delimiter in ['|||', ';;', '|', '\n---\n']:
                if delimiter in summary_text:
                    summaries = [s.strip() for s in summary_text.split(delimiter)]
                    # 3ê°œë¡œ ë§ì¶”ê¸°
                    while len(summaries) < 3:
                        summaries.append("")
                    return summaries[:3]
            
            # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ë‹¨ì¼ ìš”ì•½ë¬¸ ë³µì‚¬
            return [summary_text, summary_text, summary_text]
        
        df['summaries'] = df['summary'].apply(split_summaries)
        return df
    
    else:
        raise ValueError("ì§€ì›ë˜ëŠ” summary í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
```

---

## ğŸ¤– ëª¨ë¸ í•™ìŠµ ë¬¸ì œ

### ë©”ëª¨ë¦¬ ê´€ë ¨ ë¬¸ì œ

#### í›ˆë ¨ ì¤‘ ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**:
```bash
RuntimeError: CUDA out of memory
torch.cuda.OutOfMemoryError
```

**ì¦‰ì‹œ í•´ê²° ë°©ë²•**:
```python
# ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬
import torch
import gc

def emergency_memory_cleanup():
    """ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    print("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

emergency_memory_cleanup()
```

**ì„¤ì • ìµœì í™”**:
```yaml
# config/base_config.yaml - ë©”ëª¨ë¦¬ ì ˆì•½ ì„¤ì •
training:
  per_device_train_batch_size: 1  # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
  gradient_accumulation_steps: 16  # íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° = 1 * 16 = 16
  fp16: true  # ë°˜ì •ë°€ë„ ì‚¬ìš©
  dataloader_num_workers: 2  # ì›Œì»¤ ìˆ˜ ê°ì†Œ
  save_strategy: "epoch"  # ì²´í¬í¬ì¸íŠ¸ ë¹ˆë„ ê°ì†Œ
  logging_steps: 100  # ë¡œê¹… ë¹ˆë„ ê°ì†Œ

tokenizer:
  encoder_max_len: 256  # ìµœëŒ€ ê¸¸ì´ í¬ê²Œ ê°ì†Œ
  decoder_max_len: 64   # ì¶œë ¥ ê¸¸ì´ ê°ì†Œ

model:
  gradient_checkpointing: true  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
```

#### ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¬¸ì œ

**ì§„ë‹¨ ë°©ë²•**:
```python
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
import psutil
import torch
import time

def monitor_memory(duration=60):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    start_time = time.time()
    
    while time.time() - start_time < duration:
        # CPU ë©”ëª¨ë¦¬
        cpu_memory = psutil.virtual_memory().used / 1024**3
        
        # GPU ë©”ëª¨ë¦¬
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated(0) / 1024**3
            gpu_cached = torch.cuda.memory_reserved(0) / 1024**3
            print(f"CPU: {cpu_memory:.2f}GB, GPU: {gpu_memory:.2f}GB (Cached: {gpu_cached:.2f}GB)")
        else:
            print(f"CPU: {cpu_memory:.2f}GB")
        
        time.sleep(5)

# ì‚¬ìš©ë²•
monitor_memory(60)  # 1ë¶„ê°„ ëª¨ë‹ˆí„°ë§
```

### í•™ìŠµ ìˆ˜ë ´ ë¬¸ì œ

#### ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•ŠëŠ” ê²½ìš°

**ì§„ë‹¨ ë‹¨ê³„**:
```python
# í•™ìŠµë¥  ì§„ë‹¨
def diagnose_learning_rate(config):
    lr = config['training']['learning_rate']
    
    if lr > 1e-3:
        print(f"âš ï¸ í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤: {lr}")
        print("ê¶Œì¥: 1e-4 ~ 5e-5")
    elif lr < 1e-6:
        print(f"âš ï¸ í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤: {lr}")
        print("ê¶Œì¥: 1e-4 ~ 5e-5")
    else:
        print(f"âœ… í•™ìŠµë¥ ì´ ì ì ˆí•©ë‹ˆë‹¤: {lr}")

# ë°ì´í„° ì§„ë‹¨
def diagnose_data(dataloader):
    batch = next(iter(dataloader))
    
    print(f"ë°°ì¹˜ í¬ê¸°: {len(batch['input_ids'])}")
    print(f"ì…ë ¥ ê¸¸ì´ í‰ê· : {batch['input_ids'].shape[1]}")
    print(f"ì¶œë ¥ ê¸¸ì´ í‰ê· : {batch['labels'].shape[1]}")
    
    # í† í° ID ë¶„í¬ í™•ì¸
    print(f"ì…ë ¥ í† í° ë²”ìœ„: {batch['input_ids'].min().item()} ~ {batch['input_ids'].max().item()}")
    print(f"ë¼ë²¨ í† í° ë²”ìœ„: {batch['labels'].min().item()} ~ {batch['labels'].max().item()}")
```

**í•´ê²° ë°©ë²•**:
```yaml
# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì¶”ê°€
training:
  learning_rate: 5e-5  # ì•ˆì „í•œ ì´ˆê¸° í•™ìŠµë¥ 
  warmup_ratio: 0.1    # ì›Œë°ì—… ì¶”ê°€
  lr_scheduler_type: "cosine"  # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬
  weight_decay: 0.01   # ê°€ì¤‘ì¹˜ ê°ì‡  ì¶”ê°€
```

#### ê³¼ì í•© ë¬¸ì œ

**ì¦ìƒ**: í›ˆë ¨ ì†ì‹¤ì€ ê°ì†Œí•˜ì§€ë§Œ ê²€ì¦ ì†ì‹¤ì´ ì¦ê°€

**í•´ê²° ë°©ë²•**:
```yaml
# ì •ê·œí™” ê°•í™”
training:
  learning_rate: 3e-5  # í•™ìŠµë¥  ê°ì†Œ
  weight_decay: 0.05   # ê°€ì¤‘ì¹˜ ê°ì‡  ì¦ê°€
  warmup_ratio: 0.1
  num_train_epochs: 3  # ì—í­ ìˆ˜ ê°ì†Œ
  
  # Early stopping (WandB sweepì—ì„œ ì„¤ì •)
  metric_for_best_model: "eval_rouge_combined_f1"
  greater_is_better: true
  load_best_model_at_end: true
  save_total_limit: 2
```

---

## ğŸ§® ROUGE ê³„ì‚° ë¬¸ì œ

### ROUGE ì ìˆ˜ê°€ 0 ë˜ëŠ” ë§¤ìš° ë‚®ì€ ê²½ìš°

#### í† í°í™” ë¬¸ì œ

**ì§„ë‹¨ ë‹¨ê³„**:
```python
# í† í°í™” ê²°ê³¼ í™•ì¸
from utils.metrics import RougeCalculator

calculator = RougeCalculator(use_korean_tokenizer=True)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°
prediction = "ì•ˆë…•í•˜ì„¸ìš” ì¢‹ì€ í•˜ë£¨ì…ë‹ˆë‹¤"
reference = "ì•ˆë…•í•˜ì„¸ìš” ì¢‹ì€ ë‚ ì´ì—ìš”"

# í† í°í™” í™•ì¸ (ë‚´ë¶€ ë©”ì„œë“œê°€ ìˆë‹¤ë©´)
print(f"ì˜ˆì¸¡ í† í°: {calculator._tokenize(prediction)}")
print(f"ì •ë‹µ í† í°: {calculator._tokenize(reference)}")

# ê¸°ë³¸ ROUGE ê³„ì‚°
scores = calculator.compute_korean_rouge([prediction], [reference])
print(f"ROUGE ì ìˆ˜: {scores}")
```

**í•´ê²° ë°©ë²•**:
```python
# í† í¬ë‚˜ì´ì € ì„¤ì • ì¡°ì •
calculator = RougeCalculator(
    use_korean_tokenizer=True,
    korean_tokenizer="okt"  # mecab ëŒ€ì‹  okt ì‹œë„
)

# ë˜ëŠ” ì˜ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©
calculator_en = RougeCalculator(use_korean_tokenizer=False)
```

#### Multi-Reference ê³„ì‚° ì˜¤ë¥˜

**ì§„ë‹¨ ë‹¨ê³„**:
```python
# Multi-reference ë°ì´í„° êµ¬ì¡° í™•ì¸
predictions = ["í…ŒìŠ¤íŠ¸ ìš”ì•½ë¬¸"]
references_list = [["ì •ë‹µ1", "ì •ë‹µ2", "ì •ë‹µ3"]]

print(f"ì˜ˆì¸¡ ê°œìˆ˜: {len(predictions)}")
print(f"ì •ë‹µ ê·¸ë£¹ ê°œìˆ˜: {len(references_list)}")
print(f"ì²« ë²ˆì§¸ ê·¸ë£¹ ì •ë‹µ ê°œìˆ˜: {len(references_list[0])}")

# ê° ì •ë‹µê³¼ ê°œë³„ ë¹„êµ
for i, ref in enumerate(references_list[0]):
    single_score = calculator.compute_korean_rouge(predictions, [ref])
    print(f"ì •ë‹µ {i+1} ëŒ€ë¹„ ì ìˆ˜: {single_score['rouge1']['f1']:.4f}")
```

---

## ğŸš€ ì¶”ë¡  ë° ì œì¶œ ë¬¸ì œ

### ì¶”ë¡  ì†ë„ê°€ ë„ˆë¬´ ëŠë¦° ê²½ìš°

**ì§„ë‹¨ ë‹¨ê³„**:
```python
import time
from core.inference import InferenceEngine

# ì¶”ë¡  ì†ë„ ì¸¡ì •
engine = InferenceEngine("path/to/model")
test_dialogues = ["í…ŒìŠ¤íŠ¸ ëŒ€í™”"] * 10

start_time = time.time()
predictions = engine.predict_batch(test_dialogues, batch_size=1)
single_time = time.time() - start_time

start_time = time.time()
predictions = engine.predict_batch(test_dialogues, batch_size=5)
batch_time = time.time() - start_time

print(f"ë‹¨ì¼ ì²˜ë¦¬: {single_time:.2f}ì´ˆ ({len(test_dialogues)/single_time:.2f} samples/sec)")
print(f"ë°°ì¹˜ ì²˜ë¦¬: {batch_time:.2f}ì´ˆ ({len(test_dialogues)/batch_time:.2f} samples/sec)")
```

**ìµœì í™” ë°©ë²•**:
```python
# 1. ë°°ì¹˜ í¬ê¸° ìµœì í™”
optimal_batch_size = 8  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •

# 2. ìƒì„± íŒŒë¼ë¯¸í„° ìµœì í™”
generation_config = {
    "num_beams": 3,  # ë¹” ìˆ˜ ê°ì†Œ (ê¸°ë³¸ 5 -> 3)
    "max_length": 128,  # ìµœëŒ€ ê¸¸ì´ ì œí•œ
    "early_stopping": True,
    "no_repeat_ngram_size": 2
}

# 3. ëª¨ë¸ ìµœì í™” (ê³ ê¸‰)
import torch
model = torch.jit.script(model)  # JIT ì»´íŒŒì¼
```

### ì œì¶œ íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜

#### ì»¬ëŸ¼ëª… ë˜ëŠ” ìˆœì„œ ë¬¸ì œ

**ì§„ë‹¨ ë‹¨ê³„**:
```python
import pandas as pd

# ì œì¶œ íŒŒì¼ í™•ì¸
submission_df = pd.read_csv("submission.csv")
print(f"ì»¬ëŸ¼ëª…: {submission_df.columns.tolist()}")
print(f"ë°ì´í„° í˜•íƒœ: {submission_df.shape}")
print(f"ì²« 5í–‰:\n{submission_df.head()}")

# ëŒ€íšŒ ìš”êµ¬ì‚¬í•­ í™•ì¸
required_columns = ["fname", "summary"]
missing_columns = [col for col in required_columns if col not in submission_df.columns]
if missing_columns:
    print(f"âŒ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_columns}")
else:
    print("âœ… ì»¬ëŸ¼ í˜•ì‹ ì •í™•")
```

**í•´ê²° ë°©ë²•**:
```python
# ì •í™•í•œ ì œì¶œ í˜•ì‹ ìƒì„±
from utils.data_utils import DataProcessor

processor = DataProcessor()

# ì˜ˆì¸¡ ê²°ê³¼ì™€ íŒŒì¼ëª…
predictions = ["ìš”ì•½1", "ìš”ì•½2", "ìš”ì•½3"]
fnames = ["file1.txt", "file2.txt", "file3.txt"]

# ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
submission_df = processor.export_submission_format(
    predictions=predictions,
    fnames=fnames,
    output_path="submission.csv"
)

# í˜•ì‹ ê²€ì¦
is_valid = processor.validate_submission_format("submission.csv")
print(f"ì œì¶œ í˜•ì‹ ìœ íš¨ì„±: {is_valid}")
```

---

## ğŸ” ì„±ëŠ¥ ìµœì í™” ë¬¸ì œ

### í•™ìŠµ ì†ë„ê°€ ë„ˆë¬´ ëŠë¦° ê²½ìš°

#### ë°ì´í„° ë¡œë”© ë³‘ëª©

**ì§„ë‹¨ ë°©ë²•**:
```python
import time
from torch.utils.data import DataLoader

# ë°ì´í„° ë¡œë”© ì†ë„ ì¸¡ì •
def measure_dataloader_speed(dataloader, num_batches=10):
    start_time = time.time()
    
    for i, batch in enumerate(dataloader):
        if i >= num_batches:
            break
        # ì‹¤ì œ ì²˜ë¦¬ ì—†ì´ ë¡œë”©ë§Œ ì¸¡ì •
        pass
    
    end_time = time.time()
    avg_time_per_batch = (end_time - start_time) / num_batches
    
    print(f"ë°°ì¹˜ë‹¹ ë¡œë”© ì‹œê°„: {avg_time_per_batch:.3f}ì´ˆ")
    return avg_time_per_batch

# ìµœì í™”ëœ DataLoader ì„¤ì •
optimized_dataloader = DataLoader(
    dataset,
    batch_size=8,
    num_workers=4,  # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •
    pin_memory=True,  # GPU ì‚¬ìš© ì‹œ
    persistent_workers=True,  # ì›Œì»¤ ì¬ì‚¬ìš©
    prefetch_factor=2  # ë¯¸ë¦¬ ê°€ì ¸ì˜¬ ë°°ì¹˜ ìˆ˜
)
```

#### GPU í™œìš©ë„ ë¬¸ì œ

**ì§„ë‹¨ ë°©ë²•**:
```bash
# GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
nvidia-smi -l 1  # 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸

# ë˜ëŠ” Pythonì—ì„œ
import pynvml
pynvml.nvmlInit()

def monitor_gpu_utilization():
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    
    for _ in range(60):  # 1ë¶„ê°„ ëª¨ë‹ˆí„°ë§
        util = pynvml.nvmlDeviceGetUtilizationRates(handle)
        memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
        
        print(f"GPU ì‚¬ìš©ë¥ : {util.gpu}%, ë©”ëª¨ë¦¬: {memory.used/memory.total*100:.1f}%")
        time.sleep(1)
```

**ìµœì í™” ë°©ë²•**:
```yaml
# íš¨ìœ¨ì ì¸ í•™ìŠµ ì„¤ì •
training:
  per_device_train_batch_size: 8  # GPUì— ë§ê²Œ ì¡°ì •
  gradient_accumulation_steps: 2
  fp16: true  # í˜¼í•© ì •ë°€ë„
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  remove_unused_columns: false
  
generation:
  num_beams: 5
  length_penalty: 1.0
  early_stopping: true
```

---

## ğŸ› ì¼ë°˜ì ì¸ ë²„ê·¸ í•´ê²°

### WandB ì—°ë™ ë¬¸ì œ

#### ë¡œê·¸ì¸ ì‹¤íŒ¨

**í•´ê²° ë°©ë²•**:
```bash
# WandB ì¬ë¡œê·¸ì¸
wandb logout
wandb login

# API í‚¤ ì§ì ‘ ì„¤ì •
export WANDB_API_KEY="your_api_key_here"

# ë˜ëŠ” Pythonì—ì„œ
import wandb
wandb.login(key="your_api_key_here")
```

#### ì‹¤í—˜ ë¡œê¹… ì‹¤íŒ¨

**ì§„ë‹¨ ë° í•´ê²°**:
```python
import wandb

# WandB ìƒíƒœ í™•ì¸
print(f"WandB ë¡œê·¸ì¸ ìƒíƒœ: {wandb.api.api_key is not None}")
print(f"í˜„ì¬ í”„ë¡œì íŠ¸: {wandb.run.project if wandb.run else 'None'}")

# ì•ˆì „í•œ ë¡œê¹… í•¨ìˆ˜
def safe_wandb_log(metrics, step=None):
    try:
        if wandb.run is not None:
            wandb.log(metrics, step=step)
        else:
            print(f"WandB ë¯¸ì—°ê²° - ë©”íŠ¸ë¦­: {metrics}")
    except Exception as e:
        print(f"WandB ë¡œê¹… ì‹¤íŒ¨: {e}")
        print(f"ë©”íŠ¸ë¦­: {metrics}")
```

### ì„¤ì • íŒŒì¼ ë¬¸ì œ

#### YAML íŒŒì‹± ì˜¤ë¥˜

**ì¦ìƒ**:
```bash
yaml.scanner.ScannerError: while parsing a block mapping
```

**í•´ê²° ë°©ë²•**:
```python
import yaml

# YAML íŒŒì¼ ê²€ì¦
def validate_yaml_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        print(f"âœ… YAML íŒŒì¼ ìœ íš¨: {file_path}")
        return data
    except yaml.YAMLError as e:
        print(f"âŒ YAML íŒŒì‹± ì˜¤ë¥˜: {e}")
        print(f"íŒŒì¼: {file_path}")
        
        # ë¼ì¸ë³„ í™•ì¸
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for i, line in enumerate(lines, 1):
                if ':' in line and not line.strip().startswith('#'):
                    if line.count('"') % 2 != 0:
                        print(f"ë¼ì¸ {i}ì— ë”°ì˜´í‘œ ë¶ˆì¼ì¹˜: {line.strip()}")
        return None

# ì‚¬ìš©ë²•
config = validate_yaml_file("config/base_config.yaml")
```

---

## ğŸ“ ê³ ê¸‰ ë¬¸ì œ í•´ê²°

### ë¶„ì‚° í•™ìŠµ ë¬¸ì œ

#### ë©€í‹° GPU ì„¤ì • ì˜¤ë¥˜

**í•´ê²° ë°©ë²•**:
```python
# GPU ê°œìˆ˜ í™•ì¸
import torch
num_gpus = torch.cuda.device_count()
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {num_gpus}")

# ë¶„ì‚° í•™ìŠµ ì„¤ì •
if num_gpus > 1:
    # configì—ì„œ ë¶„ì‚° í•™ìŠµ í™œì„±í™”
    training_args = {
        "local_rank": -1,  # ìë™ ì„¤ì •
        "ddp_find_unused_parameters": False,
        "dataloader_num_workers": 2,  # GPUë‹¹ ì›Œì»¤ ìˆ˜
    }
```

### ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

#### ìƒì„¸ ë©”ëª¨ë¦¬ ë¶„ì„

**ë„êµ¬ ì„¤ì¹˜ ë° ì‚¬ìš©**:
```bash
# ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ëŸ¬ ì„¤ì¹˜
pip install memory-profiler
pip install psutil

# í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰
python -m memory_profiler code/trainer.py
```

```python
# ì½”ë“œ ë‚´ í”„ë¡œíŒŒì¼ë§
from memory_profiler import profile

@profile
def memory_intensive_function():
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•˜ê³  ì‹¶ì€ í•¨ìˆ˜
    pass

# ë˜ëŠ” ë¼ì¸ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
from memory_profiler import LineProfiler

profiler = LineProfiler()
profiler.add_function(your_function)
profiler.enable_by_count()
# ... í•¨ìˆ˜ ì‹¤í–‰ ...
profiler.print_stats()
```

---

## ğŸš¨ ì‘ê¸‰ ë³µêµ¬ ì ˆì°¨

### ì™„ì „ ì´ˆê¸°í™”

ëª¨ë“  ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•Šì„ ë•Œ:

```bash
# 1. ë°±ì—… ìƒì„±
cp -r outputs/ outputs_backup_$(date +%Y%m%d_%H%M%S)/
cp -r logs/ logs_backup_$(date +%Y%m%d_%H%M%S)/

# 2. í™˜ê²½ ì™„ì „ ì¬êµ¬ì„±
rm -rf .venv/  # ê°€ìƒí™˜ê²½ ì‚­ì œ
rm -rf __pycache__/  # ìºì‹œ ì‚­ì œ
find . -name "*.pyc" -delete  # ì»´íŒŒì¼ëœ íŒŒì¼ ì‚­ì œ

# 3. ìƒˆë¡œìš´ í™˜ê²½ êµ¬ì„±
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r code/requirements.txt

# 4. ì„¤ì • ì´ˆê¸°í™”
cp config/base_config.yaml config/base_config_backup.yaml
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë³µì› (Gitì—ì„œ)
git checkout config/base_config.yaml

# 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -c "from utils.path_utils import PathManager; print('âœ… ì‹œìŠ¤í…œ ë³µêµ¬ ì™„ë£Œ')"
```

### ë°ì´í„° ë³µêµ¬

```bash
# ì›ë³¸ ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
python -c "
import pandas as pd
try:
    df = pd.read_csv('data/train.csv')
    print(f'âœ… í›ˆë ¨ ë°ì´í„° ì •ìƒ: {len(df)} ìƒ˜í”Œ')
except Exception as e:
    print(f'âŒ í›ˆë ¨ ë°ì´í„° ì˜¤ë¥˜: {e}')

try:
    df = pd.read_csv('data/test.csv')
    print(f'âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ìƒ: {len(df)} ìƒ˜í”Œ')
except Exception as e:
    print(f'âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜¤ë¥˜: {e}')
"
```

---

## ğŸ“ ì§€ì› ìš”ì²­ ê°€ì´ë“œ

### íš¨ê³¼ì ì¸ ë¬¸ì œ ë³´ê³ 

ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë³´ê³ í•˜ì„¸ìš”:

#### 1. í™˜ê²½ ì •ë³´
```bash
# í™˜ê²½ ì •ë³´ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
python -c "
import sys
import torch
import transformers
import pandas as pd
import platform

print('=== í™˜ê²½ ì •ë³´ ===')
print(f'OS: {platform.system()} {platform.release()}')
print(f'Python: {sys.version}')
print(f'PyTorch: {torch.__version__}')
print(f'Transformers: {transformers.__version__}')
print(f'Pandas: {pd.__version__}')
print(f'CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA ë²„ì „: {torch.version.cuda}')
    print(f'GPU ê°œìˆ˜: {torch.cuda.device_count()}')
print('================')
"
```

#### 2. ì˜¤ë¥˜ ì¬í˜„ ë‹¨ê³„
1. ì •í™•í•œ ëª…ë ¹ì–´ ë˜ëŠ” ì½”ë“œ
2. ì…ë ¥ ë°ì´í„° ìƒ˜í”Œ
3. ì˜ˆìƒ ê²°ê³¼ vs ì‹¤ì œ ê²°ê³¼
4. ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ë¬¸

#### 3. ì‹œë„í•œ í•´ê²° ë°©ë²•
- ì´ë¯¸ ì‹œë„í•œ í•´ê²°ì±…ë“¤
- ì°¸ê³ í•œ ë¬¸ì„œë‚˜ ìë£Œ
- ì„ì‹œ í•´ê²°ì±… ì—¬ë¶€

### ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì ˆì°¨

1. **Level 1**: ë¬¸ì„œ ìì²´ í•´ê²° (30ë¶„)
2. **Level 2**: íŒ€ ë‚´ ê¸°ìˆ  ê²€í†  (1ì‹œê°„)
3. **Level 3**: ì™¸ë¶€ ì „ë¬¸ê°€ ìƒë‹´ (í•„ìš”ì‹œ)

---

ì´ ë¬¸ì œ í•´ê²° ê°€ì´ë“œë¥¼ í†µí•´ ëŒ€ë¶€ë¶„ì˜ ì¼ë°˜ì ì¸ ë¬¸ì œë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë¬¸ì œê°€ ë°œê²¬ë˜ë©´ ì´ ë¬¸ì„œì— ì§€ì†ì ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.
