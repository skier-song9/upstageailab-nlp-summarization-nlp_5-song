# ğŸ” ë””ë²„ê¹… ê°€ì´ë“œ

í”„ë¡œì íŠ¸ ì§„í–‰ ì¤‘ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì§„ë‹¨í•˜ê³  í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

## ğŸ› ï¸ ë””ë²„ê¹… í”„ë¡œì„¸ìŠ¤

### 1. ë¬¸ì œ íŒŒì•… ë‹¨ê³„

#### ì—ëŸ¬ ë©”ì‹œì§€ ë¶„ì„
```bash
# ì—ëŸ¬ ë¡œê·¸ í™•ì¸
tail -f logs/training.log

# Python ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ë¶„ì„
python -c "import traceback; traceback.print_exc()"
```

#### í™˜ê²½ ì •ë³´ ìˆ˜ì§‘
```python
import torch
import platform

print(f"Platform: {platform.system()} {platform.machine()}")
print(f"Python: {platform.python_version()}")
print(f"PyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
```

### 2. ì¼ë°˜ì ì¸ ë””ë²„ê¹… ê¸°ë²•

#### ë¡œê¹… í™œìš©
```python
import logging

# ìƒì„¸ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# ì¤‘ìš” ì§€ì ì— ë¡œê·¸ ì¶”ê°€
logger.debug(f"Input shape: {input_tensor.shape}")
logger.info(f"Processing batch {batch_idx}")
```

#### ë‹¨ê³„ë³„ ê²€ì¦
```python
# ë°ì´í„° ë¡œë”© ê²€ì¦
def debug_dataloader(dataloader):
    for i, batch in enumerate(dataloader):
        print(f"Batch {i}: {batch.keys()}")
        if i >= 2:  # ì²˜ìŒ ëª‡ ê°œë§Œ í™•ì¸
            break

# ëª¨ë¸ ì¶œë ¥ ê²€ì¦
def debug_model_output(model, sample_input):
    model.eval()
    with torch.no_grad():
        output = model(sample_input)
        print(f"Output shape: {output.shape}")
        print(f"Output range: {output.min():.4f} ~ {output.max():.4f}")
```

## ğŸ› ì¹´í…Œê³ ë¦¬ë³„ ë””ë²„ê¹…

### ë°ì´í„° ê´€ë ¨ ë¬¸ì œ

#### ë°ì´í„° ë¡œë”© ì˜¤ë¥˜
```python
# ë°ì´í„°ì…‹ ê²€ì¦
def validate_dataset(dataset):
    print(f"Dataset size: {len(dataset)}")
    
    # ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸
    sample = dataset[0]
    print(f"Sample keys: {sample.keys()}")
    for key, value in sample.items():
        print(f"{key}: {type(value)} - {value}")
```

#### í† í¬ë‚˜ì´ì§• ë¬¸ì œ
```python
# í† í¬ë‚˜ì´ì € ê²€ì¦
def debug_tokenizer(tokenizer, text):
    tokens = tokenizer.tokenize(text)
    input_ids = tokenizer.encode(text, return_tensors="pt")
    
    print(f"Original text: {text}")
    print(f"Tokens: {tokens}")
    print(f"Input IDs: {input_ids}")
    print(f"Decoded: {tokenizer.decode(input_ids[0])}")
```

### ëª¨ë¸ í•™ìŠµ ë¬¸ì œ

#### í•™ìŠµ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
```python
# í•™ìŠµ ê³¼ì • ë””ë²„ê¹…
class DebugCallback:
    def on_epoch_start(self, epoch):
        print(f"Epoch {epoch} started")
    
    def on_batch_end(self, batch_idx, loss):
        if batch_idx % 100 == 0:
            print(f"Batch {batch_idx}, Loss: {loss:.4f}")
    
    def on_epoch_end(self, epoch, metrics):
        print(f"Epoch {epoch} ended, Metrics: {metrics}")
```

#### ê·¸ë˜ë””ì–¸íŠ¸ ë¬¸ì œ ì§„ë‹¨
```python
# ê·¸ë˜ë””ì–¸íŠ¸ í™•ì¸
def debug_gradients(model):
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm()
            print(f"{name}: grad_norm = {grad_norm:.6f}")
        else:
            print(f"{name}: No gradient")
```

### ë©”ëª¨ë¦¬ ê´€ë ¨ ë¬¸ì œ

#### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
```python
import psutil
import torch

def monitor_memory():
    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
    memory = psutil.virtual_memory()
    print(f"System Memory: {memory.percent}% used")
    
    # GPU ë©”ëª¨ë¦¬
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3   # GB
        print(f"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
```

#### ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€
```python
import gc

def debug_memory_leak():
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì „í›„ ê°ì²´ ìˆ˜ ë¹„êµ
    before = len(gc.get_objects())
    
    # ì˜ì‹¬ë˜ëŠ” ì½”ë“œ ì‹¤í–‰
    # your_function()
    
    gc.collect()
    after = len(gc.get_objects())
    
    print(f"Objects before: {before}, after: {after}, diff: {after-before}")
```

## ğŸ”§ ë„êµ¬ë³„ ë””ë²„ê¹…

### PyTorch ë””ë²„ê¹…

#### ëª¨ë¸ êµ¬ì¡° í™•ì¸
```python
from torchsummary import summary

# ëª¨ë¸ ìš”ì•½ ì¶œë ¥
summary(model, input_size=(max_length,))

# ëª¨ë¸ ê·¸ë˜í”„ ì‹œê°í™”
import torch.utils.tensorboard as tb
with tb.SummaryWriter() as writer:
    writer.add_graph(model, sample_input)
```

#### autograd ë””ë²„ê¹…
```python
# ìë™ ë¯¸ë¶„ ê²€ì¦
torch.autograd.set_detect_anomaly(True)

# ê³„ì‚° ê·¸ë˜í”„ í™•ì¸
x = torch.randn(2, 2, requires_grad=True)
y = x * 2
y.retain_grad()  # ì¤‘ê°„ ë³€ìˆ˜ì˜ gradient ë³´ì¡´
```

### Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë””ë²„ê¹…

#### ëª¨ë¸ ì¶œë ¥ ë¶„ì„
```python
# ëª¨ë¸ ì¶œë ¥ ìƒì„¸ ë¶„ì„
outputs = model(**inputs, output_attentions=True, output_hidden_states=True)

print(f"Last hidden state shape: {outputs.last_hidden_state.shape}")
print(f"Number of attention layers: {len(outputs.attentions)}")
print(f"Number of hidden states: {len(outputs.hidden_states)}")
```

#### í† í¬ë‚˜ì´ì € íŠ¹ìˆ˜ í† í° í™•ì¸
```python
# íŠ¹ìˆ˜ í† í° ê²€ì¦
print(f"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
print(f"UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})")
print(f"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
print(f"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
```

## ğŸš¨ ì„±ëŠ¥ ë””ë²„ê¹…

### ë³‘ëª© ì§€ì  ì°¾ê¸°

#### í”„ë¡œíŒŒì¼ë§
```python
import cProfile
import pstats

# ì½”ë“œ í”„ë¡œíŒŒì¼ë§
profiler = cProfile.Profile()
profiler.enable()

# í”„ë¡œíŒŒì¼ë§í•  ì½”ë“œ
your_function()

profiler.disable()

# ê²°ê³¼ ë¶„ì„
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(10)  # ìƒìœ„ 10ê°œ í•¨ìˆ˜
```

#### ì‹œê°„ ì¸¡ì •
```python
import time
from contextlib import contextmanager

@contextmanager
def timer(description):
    start = time.time()
    yield
    elapsed = time.time() - start
    print(f"{description}: {elapsed:.4f} seconds")

# ì‚¬ìš© ì˜ˆì‹œ
with timer("Data loading"):
    data = load_data()

with timer("Model inference"):
    output = model(data)
```

### ë°°ì¹˜ í¬ê¸° ìµœì í™”

#### ìë™ ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
```python
def find_optimal_batch_size(model, sample_input, max_batch_size=128):
    batch_size = 1
    
    while batch_size <= max_batch_size:
        try:
            batch_input = sample_input.repeat(batch_size, 1)
            
            with torch.no_grad():
                output = model(batch_input)
            
            print(f"âœ… Batch size {batch_size}: Success")
            batch_size *= 2
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"âŒ Batch size {batch_size}: OOM")
                optimal_size = batch_size // 2
                print(f"Optimal batch size: {optimal_size}")
                return optimal_size
            else:
                raise e
    
    return max_batch_size
```

## ğŸ” ê³ ê¸‰ ë””ë²„ê¹… ê¸°ë²•

### í…ì„œ ê°’ ì¶”ì 
```python
# í…ì„œ ê°’ ë³€í™” ì¶”ì 
class TensorTracker:
    def __init__(self):
        self.values = []
    
    def track(self, tensor, name=""):
        self.values.append({
            'name': name,
            'shape': tensor.shape,
            'mean': tensor.mean().item(),
            'std': tensor.std().item(),
            'min': tensor.min().item(),
            'max': tensor.max().item()
        })
    
    def summary(self):
        for i, v in enumerate(self.values):
            print(f"Step {i} - {v['name']}: "
                  f"shape={v['shape']}, mean={v['mean']:.4f}, "
                  f"std={v['std']:.4f}, range=[{v['min']:.4f}, {v['max']:.4f}]")

# ì‚¬ìš© ì˜ˆì‹œ
tracker = TensorTracker()
x = torch.randn(2, 3)
tracker.track(x, "input")
y = torch.relu(x)
tracker.track(y, "after_relu")
tracker.summary()
```

### ëª¨ë¸ ì›¨ì´íŠ¸ ë¶„ì„
```python
def analyze_model_weights(model):
    for name, param in model.named_parameters():
        weight_norm = param.norm().item()
        weight_mean = param.mean().item()
        weight_std = param.std().item()
        
        print(f"{name}:")
        print(f"  Norm: {weight_norm:.6f}")
        print(f"  Mean: {weight_mean:.6f}")
        print(f"  Std:  {weight_std:.6f}")
        
        # ì´ìƒ ê°’ ê°ì§€
        if weight_norm > 100:
            print(f"  âš ï¸ Large weight norm detected!")
        if abs(weight_mean) > 10:
            print(f"  âš ï¸ Large weight mean detected!")
```

## ğŸ“‹ ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸

### í™˜ê²½ ì„¤ì • í™•ì¸
- [ ] Python ë²„ì „ í˜¸í™˜ì„±
- [ ] íŒ¨í‚¤ì§€ ë²„ì „ ì¼ì¹˜
- [ ] CUDA ì„¤ì • ì •ìƒ
- [ ] ë©”ëª¨ë¦¬ ì¶©ë¶„

### ë°ì´í„° í™•ì¸
- [ ] ë°ì´í„° í˜•ì‹ ì˜¬ë°”ë¦„
- [ ] í† í¬ë‚˜ì´ì§• ì •ìƒ
- [ ] ë°°ì¹˜ í¬ê¸° ì ì ˆ
- [ ] ë ˆì´ë¸” ì •í™•ì„±

### ëª¨ë¸ í™•ì¸
- [ ] ì•„í‚¤í…ì²˜ ì„¤ì • ì˜¬ë°”ë¦„
- [ ] ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì •ìƒ
- [ ] ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ì •ìƒ
- [ ] ì¶œë ¥ ì°¨ì› ì¼ì¹˜

### í•™ìŠµ ê³¼ì • í™•ì¸
- [ ] ì†ì‹¤ í•¨ìˆ˜ ì ì ˆ
- [ ] í•™ìŠµë¥  ì ì ˆ
- [ ] ìˆ˜ë ´ íŒ¨í„´ ì •ìƒ
- [ ] ì˜¤ë²„í”¼íŒ… í™•ì¸

## ğŸ†˜ ê¸´ê¸‰ ë””ë²„ê¹…

### ë¹ ë¥¸ ë¬¸ì œ í•´ê²°
1. **ì¬ì‹œì‘**: ì»¤ë„/í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘
2. **ë©”ëª¨ë¦¬ ì •ë¦¬**: `torch.cuda.empty_cache()`
3. **ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸**: ìµœì†Œ ì˜ˆì œë¡œ ì¬í˜„
4. **ë²„ì „ ë¡¤ë°±**: ì´ì „ ì •ìƒ ë²„ì „ìœ¼ë¡œ ë³µì›

### ë„ì›€ ìš”ì²­ ì „ ì¤€ë¹„ì‚¬í•­
- ì—ëŸ¬ ë©”ì‹œì§€ ì „ë¬¸
- ì¬í˜„ ê°€ëŠ¥í•œ ìµœì†Œ ì½”ë“œ
- í™˜ê²½ ì •ë³´ (OS, Python, íŒ¨í‚¤ì§€ ë²„ì „)
- ì‹œë„í•œ í•´ê²° ë°©ë²•ë“¤

## ğŸ”— ê´€ë ¨ ë„êµ¬

- **pdb**: Python ë‚´ì¥ ë””ë²„ê±°
- **ipdb**: IPython ë””ë²„ê±°  
- **PyTorch profiler**: ì„±ëŠ¥ ë¶„ì„
- **TensorBoard**: ì‹œê°í™”
- **WandB**: ì‹¤í—˜ ì¶”ì 

---

ì²´ê³„ì ì¸ ë””ë²„ê¹…ì„ í†µí•´ ë¬¸ì œë¥¼ ë¹ ë¥´ê²Œ í•´ê²°í•˜ê³  ì•ˆì •ì ì¸ ê°œë°œ í™˜ê²½ì„ êµ¬ì¶•í•˜ì„¸ìš”.
