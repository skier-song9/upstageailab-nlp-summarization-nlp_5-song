    # í•„ìˆ˜ ê°œë°œ ì‚¬í•­ (Priority 1) - í˜„ì¬ êµ¬í˜„ ìƒíƒœ
    
    ## ğŸ“Š ê°œìš”
    
    ì´ ë¬¸ì„œëŠ” NLP ëŒ€í™” ìš”ì•½ í”„ë¡œì íŠ¸ì˜ **í•„ìˆ˜ êµ¬í˜„ ì‚¬í•­ê³¼ í˜„ì¬ ìƒíƒœ**ë¥¼ ì •ë¦¬í•œ ë¬¸ì„œì…ë‹ˆë‹¤.
    
    **ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-07-26
    **ë¬¸ì„œ ìƒíƒœ**: í˜„ì¬ êµ¬í˜„ ìƒíƒœ ê¸°ì¤€ ì—…ë°ì´íŠ¸
    
    ---
    
    ## ğŸ”´ í˜„ì¬ êµ¬í˜„ ìƒíƒœ ìš”ì•½
    
    ### ì „ì²´ êµ¬í˜„ë¥ : ~40%
    
    | ê¸°ëŠ¥ | ìƒíƒœ | êµ¬í˜„ë¥  | ë¹„ê³  |
    |------|------|---------|------|
    | í¬ë¡œìŠ¤ í”Œë«í¼ ê²½ë¡œ ì‹œìŠ¤í…œ | ğŸ”´ | 0% | PathManager ë¯¸êµ¬í˜„ |
    | MPS ë””ë°”ì´ìŠ¤ ì§€ì› | ğŸ”´ | 0% | CUDAë§Œ ì§€ì› |
    | ë…ë¦½ì ì¸ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ | ğŸ”´ | 0% | core/ ë””ë ‰í† ë¦¬ ì—†ìŒ |
    | Multi-reference ROUGE | ğŸŸ¡ | 60% | ê¸°ë³¸ ê¸°ëŠ¥ë§Œ êµ¬í˜„ |
    | ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ | ğŸŸ¢ | 100% | êµ¬í˜„ ì™„ë£Œ |
    | ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ | ğŸŸ¢ | 100% | êµ¬í˜„ ì™„ë£Œ |
    
    ---
    
    ## ğŸ”´ Phase 1: í¬ë¡œìŠ¤ í”Œë«í¼ ê²½ë¡œ ì‹œìŠ¤í…œ (0% êµ¬í˜„)
    
    ### ğŸ¯ ëª©í‘œ
    ëª¨ë“  íŒŒì¼ì—ì„œ ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„± ë³´ì¥
    
    ### ğŸ” í˜„ì¬ ìƒíƒœ
    - **PathManager í´ë˜ìŠ¤ ë¯¸êµ¬í˜„**
    - **ëª¨ë“  ì½”ë“œì—ì„œ ì ˆëŒ€ ê²½ë¡œ í•˜ë“œì½”ë”©**
    - **Windows/Mac/Linux í˜¸í™˜ì„± ì—†ìŒ**
    
    ### ğŸ“ êµ¬í˜„ í•„ìš” ì‚¬í•­
    
    #### 1.1 PathManager í´ë˜ìŠ¤ êµ¬í˜„
    ```python
    # code/utils/path_utils.py
    from pathlib import Path
    import os
    from typing import Union, Optional
    
    class PathManager:
        """Cross-platform path management system"""
        
        _project_root: Optional[Path] = None
        
        @classmethod
        def get_project_root(cls) -> Path:
            """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ìë™ ê°ì§€"""
            if cls._project_root is None:
                current = Path(__file__).resolve()
                
                # í”„ë¡œì íŠ¸ ë£¨íŠ¸ íŠ¹ì§•: code/, data/, config/ ë””ë ‰í† ë¦¬ ì¡´ì¬
                while current != current.parent:
                    if all((current / d).exists() for d in ['code', 'data', 'config']):
                        cls._project_root = current
                        break
                    current = current.parent
                
                if cls._project_root is None:
                    raise RuntimeError(
                        "Project root not found. Make sure you're running from the project directory."
                    )
            
            return cls._project_root
        
        @staticmethod
        def resolve_path(relative_path: Union[str, Path]) -> Path:
            """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
            if isinstance(relative_path, str):
                relative_path = Path(relative_path)
            
            if relative_path.is_absolute():
                raise ValueError(
                    f"ì ˆëŒ€ ê²½ë¡œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {relative_path}\n"
                    f"ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: 'data/train.csv'"
                )
            
            return PathManager.get_project_root() / relative_path
        
        @staticmethod
        def ensure_dir(directory: Union[str, Path]) -> Path:
            """ë””ë ‰í† ë¦¬ ìƒì„± ë³´ì¥"""
            if isinstance(directory, str):
                directory = Path(directory)
            
            # ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if not directory.is_absolute():
                directory = PathManager.resolve_path(directory)
            
            directory.mkdir(parents=True, exist_ok=True)
            return directory
    ```
    
    #### 1.2 ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • í•„ìš” íŒŒì¼
    - `trainer.py`: ëª¨ë“  ê²½ë¡œ ì²˜ë¦¬ë¥¼ PathManagerë¡œ ìˆ˜ì •
    - `config_manager.py`: ì„¤ì • íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬ ìˆ˜ì •
    - `data_utils.py`: ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬ ìˆ˜ì •
    - `sweep_runner.py`: ì‹¤í—˜ ì¶œë ¥ ê²½ë¡œ ì²˜ë¦¬ ìˆ˜ì •
    
    ### âš ï¸ ì£¼ì˜ì‚¬í•­
    - í˜„ì¬ ëª¨ë“  íŒŒì¼ì— í•˜ë“œì½”ë”©ëœ ì ˆëŒ€ ê²½ë¡œ ì¡´ì¬
    - Windows ê²½ë¡œ êµ¬ë¶„ì í˜¸í™˜ì„± í•„ìš”
    
    ---
    
    ## ğŸ”´ Phase 2: MPS ë””ë°”ì´ìŠ¤ ìµœì í™” (0% êµ¬í˜„)
    
    ### ğŸ¯ ëª©í‘œ
    Mac Apple Silicon (M1/M2) ì‚¬ìš©ìë¥¼ ìœ„í•œ MPS ë””ë°”ì´ìŠ¤ ì§€ì›
    
    ### ğŸ” í˜„ì¬ ìƒíƒœ
    - **MPS ê°ì§€ ì½”ë“œ ì—†ìŒ**
    - **CUDAë§Œ ê³ ë ¤ëœ êµ¬í˜„**
    - **Mac ì‚¬ìš©ìëŠ” CPUë¡œë§Œ ì‹¤í–‰**
    
    ### ğŸ“ êµ¬í˜„ í•„ìš” ì‚¬í•­
    
    #### 2.1 ë””ë°”ì´ìŠ¤ ìœ í‹¸ë¦¬í‹° êµ¬í˜„
    ```python
    # code/utils/device_utils.py
    import torch
    import platform
    from typing import Dict, Any
    import logging
    
    logger = logging.getLogger(__name__)
    
    def get_optimal_device() -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if torch.cuda.is_available():
            device = "cuda"
            logger.info(f"CUDA device detected: {torch.cuda.get_device_name(0)}")
        elif torch.backends.mps.is_available() and platform.system() == "Darwin":
            device = "mps"
            logger.info("Apple MPS device detected")
        else:
            device = "cpu"
            logger.info("Using CPU device")
        
        return device
    
    def setup_device_config(config: Dict[str, Any]) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •"""
        device = get_optimal_device()
        
        # ë””ë°”ì´ìŠ¤ë³„ ê¸°ë³¸ ì„¤ì •
        device_configs = {
            "mps": {
                "fp16": False,  # MPSëŠ” í˜„ì¬ fp16 ë¯¸ì§€ì›
                "dataloader_num_workers": 0,  # MPS í˜¸í™˜ì„±
                "gradient_accumulation_steps": 4,  # ë©”ëª¨ë¦¬ íš¨ìœ¨
                "per_device_train_batch_size": 4
            },
            "cuda": {
                "fp16": True,
                "dataloader_num_workers": 4,
                "gradient_accumulation_steps": 1,
                "per_device_train_batch_size": 8
            },
            "cpu": {
                "fp16": False,
                "dataloader_num_workers": 2,
                "gradient_accumulation_steps": 8,
                "per_device_train_batch_size": 2
            }
        }
        
        # ë””ë°”ì´ìŠ¤ë³„ ì„¤ì • ì ìš©
        device_specific = device_configs.get(device, device_configs["cpu"])
        
        if "training" not in config:
            config["training"] = {}
        
        config["training"].update(device_specific)
        config["device"] = device
        
        return config
    ```
    
    #### 2.2 Trainer í´ë˜ìŠ¤ ìˆ˜ì • í•„ìš”
    - ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë¡œì§ ì¶”ê°€
    - MPSì—ì„œ mixed precision training ë¹„í™œì„±í™”
    - ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
    
    ---
    
    ## ğŸ”´ Phase 3: ë…ë¦½ì ì¸ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ (0% êµ¬í˜„)
    
    ### ğŸ¯ ëª©í‘œ
    baseline.ipynbì—ì„œ ë¶„ë¦¬ëœ ë…ë¦½ì ì¸ ì¶”ë¡  ì—”ì§„ ë° CLI ë„êµ¬
    
    ### ğŸ” í˜„ì¬ ìƒíƒœ
    - **core/ ë””ë ‰í† ë¦¬ ìì²´ê°€ ì—†ìŒ**
    - **ì¶”ë¡  ì½”ë“œê°€ notebookì—ë§Œ ì¡´ì¬**
    - **CLI ë„êµ¬ ì—†ìŒ**
    - **ëŒ€íšŒ ì œì¶œ í˜•ì‹ ì§€ì› ì—†ìŒ**
    
    ### ğŸ“ êµ¬í˜„ í•„ìš” ì‚¬í•­
    
    #### 3.1 InferenceEngine í´ë˜ìŠ¤
    ```python
    # code/core/inference.py
    from typing import List, Union, Optional, Dict, Any
    import torch
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
    from tqdm import tqdm
    import pandas as pd
    from pathlib import Path
    
    from utils.device_utils import get_optimal_device
    from utils.path_utils import PathManager
    
    class InferenceEngine:
        """ë…ë¦½ì ì¸ ì¶”ë¡  ì—”ì§„"""
        
        def __init__(self, model_path: Union[str, Path], config: Optional[Dict[str, Any]] = None):
            self.device = get_optimal_device()
            self.model_path = PathManager.resolve_path(model_path)
            
            # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                str(self.model_path)
            ).to(self.device)
            
            self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_path))
            
            # ê¸°ë³¸ ì„¤ì •
            self.max_input_length = config.get('max_input_length', 512) if config else 512
            self.max_output_length = config.get('max_output_length', 100) if config else 100
            self.batch_size = config.get('batch_size', 8) if config else 8
            
            # ë””ë°”ì´ìŠ¤ë³„ ë°°ì¹˜ í¬ê¸° ì¡°ì •
            if self.device == "mps":
                self.batch_size = min(self.batch_size, 4)
            elif self.device == "cpu":
                self.batch_size = min(self.batch_size, 2)
        
        def predict_single(self, dialogue: str) -> str:
            """ë‹¨ì¼ ëŒ€í™” ìš”ì•½"""
            # ì…ë ¥ ì²˜ë¦¬
            inputs = self.tokenizer(
                dialogue,
                return_tensors="pt",
                max_length=self.max_input_length,
                truncation=True,
                padding=True
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=self.max_output_length,
                    num_beams=4,
                    early_stopping=True,
                    no_repeat_ngram_size=3
                )
            
            # ë””ì½”ë”©
            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return summary
        
        def predict_batch(
            self, 
            dialogues: List[str], 
            batch_size: Optional[int] = None,
            show_progress: bool = True
        ) -> List[str]:
            """ë°°ì¹˜ ì˜ˆì¸¡"""
            if batch_size is None:
                batch_size = self.batch_size
            
            predictions = []
            
            # ì§„í–‰ë¥  í‘œì‹œ
            iterator = range(0, len(dialogues), batch_size)
            if show_progress:
                iterator = tqdm(iterator, desc="Generating summaries")
            
            for i in iterator:
                batch = dialogues[i:i+batch_size]
                
                # ë°°ì¹˜ í† í°í™”
                inputs = self.tokenizer(
                    batch,
                    return_tensors="pt",
                    max_length=self.max_input_length,
                    truncation=True,
                    padding=True
                )
                
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # ë°°ì¹˜ ì¶”ë¡ 
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_length=self.max_output_length,
                        num_beams=4,
                        early_stopping=True,
                        no_repeat_ngram_size=3
                    )
                
                # ë””ì½”ë”©
                batch_summaries = [
                    self.tokenizer.decode(output, skip_special_tokens=True)
                    for output in outputs
                ]
                
                predictions.extend(batch_summaries)
            
            return predictions
        
        def predict_from_dataframe(
            self,
            df: pd.DataFrame,
            dialogue_column: str = "dialogue",
            fname_column: str = "fname"
        ) -> pd.DataFrame:
            """ë°ì´í„°í”„ë ˆì„ì—ì„œ ì§ì ‘ ì˜ˆì¸¡"""
            dialogues = df[dialogue_column].tolist()
            predictions = self.predict_batch(dialogues)
            
            # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            result_df = pd.DataFrame({
                fname_column: df[fname_column],
                'summary': predictions
            })
            
            return result_df
    ```
    
    #### 3.2 CLI ë„êµ¬
    ```python
    # code/run_inference.py
    import argparse
    import pandas as pd
    from pathlib import Path
    import logging
    import sys
    
    # code ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
    sys.path.append(str(Path(__file__).parent))
    
    from core.inference import InferenceEngine
    from utils.path_utils import PathManager
    
    def main():
        parser = argparse.ArgumentParser(
            description="NLP ëŒ€í™” ìš”ì•½ ì¶”ë¡  ë„êµ¬"
        )
        
        parser.add_argument(
            "--model_path",
            type=str,
            required=True,
            help="í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (outputs/best_model ë“±)"
        )
        
        parser.add_argument(
            "--input_file",
            type=str,
            required=True,
            help="ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ (data/test.csv ë“±)"
        )
        
        parser.add_argument(
            "--output_file",
            type=str,
            required=True,
            help="ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ (outputs/submission.csv ë“±)"
        )
        
        parser.add_argument(
            "--batch_size",
            type=int,
            default=8,
            help="ë°°ì¹˜ í¬ê¸° (default: 8)"
        )
        
        args = parser.parse_args()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        logger = logging.getLogger(__name__)
        
        try:
            # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
            logger.info(f"Loading model from {args.model_path}")
            engine = InferenceEngine(
                model_path=args.model_path,
                config={'batch_size': args.batch_size}
            )
            
            # ë°ì´í„° ë¡œë“œ
            logger.info(f"Loading data from {args.input_file}")
            input_path = PathManager.resolve_path(args.input_file)
            df = pd.read_csv(input_path)
            
            # ì¶”ë¡  ì‹¤í–‰
            logger.info(f"Running inference on {len(df)} samples")
            result_df = engine.predict_from_dataframe(df)
            
            # ê²°ê³¼ ì €ì¥
            output_path = PathManager.resolve_path(args.output_file)
            PathManager.ensure_dir(output_path.parent)
            result_df.to_csv(output_path, index=False)
            
            logger.info(f"Results saved to {args.output_file}")
            logger.info(f"Total predictions: {len(result_df)}")
            
        except Exception as e:
            logger.error(f"Inference failed: {e}")
            raise
    
    if __name__ == "__main__":
        main()
    ```
    
    ---
    
    ## ğŸŸ¡ Phase 4: Multi-Reference ROUGE ê³„ì‚° ì‹œìŠ¤í…œ (60% êµ¬í˜„)
    
    ### ğŸ¯ ëª©í‘œ
    3ê°œ ì •ë‹µ ìš”ì•½ë¬¸ì— ëŒ€í•œ ì •í™•í•œ ROUGE ê³„ì‚°
    
    ### ğŸ” í˜„ì¬ ìƒíƒœ
    - **ê¸°ë³¸ RougeCalculator í´ë˜ìŠ¤ êµ¬í˜„ë¨**
    - **Multi-reference ì „ìš© ë©”ì„œë“œ ì—†ìŒ**
    - **ëŒ€íšŒ í‰ê°€ ë°©ì‹ê³¼ì˜ ì¼ì¹˜ì„± ë¯¸í™•ì¸**
    
    ### ğŸ“ ì¶”ê°€ êµ¬í˜„ í•„ìš”
    
    #### 4.1 Multi-reference ROUGE ë©”ì„œë“œ
    ```python
    # utils/metrics.pyì— ì¶”ê°€
    def calculate_multi_reference(
        self,
        prediction: str,
        references: List[str]
    ) -> EvaluationResult:
        """
        ë‹¤ì¤‘ ì°¸ì¡° ROUGE ê³„ì‚° (ëŒ€íšŒ í‰ê°€ ë°©ì‹)
        
        Args:
            prediction: ì˜ˆì¸¡ëœ ìš”ì•½ë¬¸
            references: 3ê°œì˜ ì •ë‹µ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            EvaluationResult: ê° ë©”íŠ¸ë¦­ë³„ ìµœê³  ì ìˆ˜
        """
        if not references:
            return self._create_zero_score()
        
        # ê° ì°¸ì¡°ì™€ì˜ ROUGE ì ìˆ˜ ê³„ì‚°
        all_scores = []
        for ref in references:
            if ref and ref.strip():  # ë¹„ì–´ìˆëŠ” ì°¸ì¡° ê±´ë„ˆë›°ê¸°
                score = self.calculate_single_reference(prediction, ref)
                all_scores.append(score)
        
        if not all_scores:
            return self._create_zero_score()
        
        # ê° ë©”íŠ¸ë¦­ë³„ë¡œ ìµœê³  ì ìˆ˜ ì„ íƒ (ëŒ€íšŒ ê·œì¹™)
        best_rouge1_f1 = max(score.rouge1.f1 for score in all_scores)
        best_rouge2_f1 = max(score.rouge2.f1 for score in all_scores)
        best_rougeL_f1 = max(score.rougeL.f1 for score in all_scores)
        
        # Precisionê³¼ Recallë„ ìµœê³  ì ìˆ˜ ê¸°ì¤€
        best_rouge1_precision = max(score.rouge1.precision for score in all_scores)
        best_rouge1_recall = max(score.rouge1.recall for score in all_scores)
        
        best_rouge2_precision = max(score.rouge2.precision for score in all_scores)
        best_rouge2_recall = max(score.rouge2.recall for score in all_scores)
        
        best_rougeL_precision = max(score.rougeL.precision for score in all_scores)
        best_rougeL_recall = max(score.rougeL.recall for score in all_scores)
        
        # ê²°í•© ì ìˆ˜ (ëŒ€íšŒ ìµœì¢… ì ìˆ˜)
        rouge_combined_f1 = best_rouge1_f1 + best_rouge2_f1 + best_rougeL_f1
        
        return EvaluationResult(
            rouge1=RougeScore(
                precision=best_rouge1_precision,
                recall=best_rouge1_recall,
                f1=best_rouge1_f1
            ),
            rouge2=RougeScore(
                precision=best_rouge2_precision,
                recall=best_rouge2_recall,
                f1=best_rouge2_f1
            ),
            rougeL=RougeScore(
                precision=best_rougeL_precision,
                recall=best_rougeL_recall,
                f1=best_rougeL_f1
            ),
            rouge_combined_f1=rouge_combined_f1
        )
    
    def compute_metrics_for_trainer(
        self,
        predictions: List[str],
        references_list: List[List[str]]
    ) -> Dict[str, float]:
        """
        Trainerì—ì„œ ì‚¬ìš©í•  ë©”íŠ¸ë¦­ ê³„ì‚° (Multi-reference ì§€ì›)
        """
        total_scores = {
            'rouge1_f1': 0,
            'rouge2_f1': 0,
            'rougeL_f1': 0,
            'rouge_combined_f1': 0
        }
        
        for pred, refs in zip(predictions, references_list):
            result = self.calculate_multi_reference(pred, refs)
            
            total_scores['rouge1_f1'] += result.rouge1.f1
            total_scores['rouge2_f1'] += result.rouge2.f1
            total_scores['rougeL_f1'] += result.rougeL.f1
            total_scores['rouge_combined_f1'] += result.rouge_combined_f1
        
        # í‰ê·  ê³„ì‚°
        n = len(predictions)
        return {k: v / n for k, v in total_scores.items()}
    ```
    
    ---
    
    ## ğŸŸ¢ Phase 5: ì™„ë£Œëœ ê¸°ëŠ¥
    
    ### 5.1 ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ (100% êµ¬í˜„)
    - `ExperimentTracker` í´ë˜ìŠ¤ êµ¬í˜„ ì™„ë£Œ
    - `ModelRegistry` í´ë˜ìŠ¤ êµ¬í˜„ ì™„ë£Œ
    - ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ìë™ ì €ì¥
    - WandBì™€ í†µí•© ê°€ëŠ¥
    
    ### 5.2 ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ (100% êµ¬í˜„)
    - `DataProcessor` í´ë˜ìŠ¤ êµ¬í˜„ ì™„ë£Œ
    - `TextPreprocessor` í´ë˜ìŠ¤ êµ¬í˜„ ì™„ë£Œ
    - íŠ¹ìˆ˜ í† í° ì²˜ë¦¬ ì§€ì›
    - DialogSum ë°ì´í„°ì…‹ íŠ¹í™” ì²˜ë¦¬
    
    ---
    
    ## ğŸ“‹ ê°œë°œ ìš°ì„ ìˆœìœ„ ë° ì•¡ì…˜ í”Œëœ
    
    ### ğŸ”´ ê¸´ê¸‰ ê°œë°œ í•„ìš” (1-2ì¼)
    
    1. **PathManager ì‹œìŠ¤í…œ**
       - íŒŒì¼: `code/utils/path_utils.py`
       - ì˜ˆìƒ ì‹œê°„: 4-6ì‹œê°„
       - ì˜í–¥: ëª¨ë“  íŒŒì¼ ìˆ˜ì • í•„ìš”
    
    2. **MPS ë””ë°”ì´ìŠ¤ ì§€ì›**
       - íŒŒì¼: `code/utils/device_utils.py`
       - ì˜ˆìƒ ì‹œê°„: 2-3ì‹œê°„
       - ì˜í–¥: trainer.py ìˆ˜ì •
    
    3. **ë…ë¦½ ì¶”ë¡  ì—”ì§„**
       - íŒŒì¼: `code/core/inference.py`, `code/run_inference.py`
       - ì˜ˆìƒ ì‹œê°„: 6-8ì‹œê°„
       - ì˜í–¥: ëŒ€íšŒ ì œì¶œ í”„ë¡œì„¸ìŠ¤ ê°œì„ 
    
    ### ğŸŸ¡ ê°œì„  í•„ìš” (3-5ì¼)
    
    1. **Multi-reference ROUGE ì™„ì„±**
       - íŒŒì¼: `code/utils/metrics.py`
       - ì˜ˆìƒ ì‹œê°„: 3-4ì‹œê°„
       - ì˜í–¥: í‰ê°€ ì •í™•ì„± í–¥ìƒ
    
    2. **í†µí•© í…ŒìŠ¤íŠ¸ ë° ê²€ì¦**
       - ì˜ˆìƒ ì‹œê°„: 4-5ì‹œê°„
       - ëª¨ë“  ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸
    
    ---
    
    ## ğŸ“ ì°¸ê³ ìë£Œ ë° ì§€ì›
    
    ### ê´€ë ¨ ë¬¸ì„œ
    - [implementation_checklist.md](implementation_checklist.md) - ìƒì„¸ ì²´í¬ë¦¬ìŠ¤íŠ¸
    - [integration_action_plan.md](../team_progress/integration_action_plan.md) - í†µí•© ê°€ì´ë“œ
    - [baseline_code_analysis.md](../baseline_code_analysis.md) - ì½”ë“œ ë¶„ì„
    
    ### ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥í•œ ì‘ì—…
    1. PathManager êµ¬í˜„ â†’ ëª¨ë“  ê²½ë¡œ ì²˜ë¦¬ ìˆ˜ì •
    2. device_utils.py êµ¬í˜„ â†’ trainer.py ìˆ˜ì •
    3. inference.py êµ¬í˜„ â†’ CLI ë„êµ¬ ìƒì„±
    
    ---
    
    **ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-07-26  
    **ë‹¤ìŒ ë‹¨ê³„**: PathManager êµ¬í˜„ í›„ ëª¨ë“  ì½”ë“œ ì—…ë°ì´íŠ¸

## ğŸ“‹ Priority 1 êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ê¸°ë°˜ ì‹œìŠ¤í…œ (Day 1-3)
- [ ] **PathManager ì‹œìŠ¤í…œ** (Day 1)
  - [ ] `code/utils/path_utils.py` êµ¬í˜„
  - [ ] ìƒëŒ€ ê²½ë¡œ ê°•ì œ ë° ì ˆëŒ€ ê²½ë¡œ ê¸ˆì§€
  - [ ] í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„± í™•ì¸
  - [ ] `python validate_paths.py` ê²€ì¦ í†µê³¼

- [ ] **ë””ë°”ì´ìŠ¤ ìµœì í™”** (Day 1)
  - [ ] `code/utils/device_utils.py` êµ¬í˜„
  - [ ] MPS (Mac) / CUDA (Ubuntu) ìë™ ê°ì§€
  - [ ] ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ì ìš©

- [ ] **ê¸°ì¡´ ì½”ë“œ ìˆ˜ì •** (Day 2)
  - [ ] `trainer.py` ê²½ë¡œ ì²˜ë¦¬ ìˆ˜ì •
  - [ ] `config_manager.py` ê²½ë¡œ ì²˜ë¦¬ ìˆ˜ì •
  - [ ] `sweep_runner.py` ê²½ë¡œ ì²˜ë¦¬ ìˆ˜ì •
  - [ ] ëª¨ë“  ì ˆëŒ€ ê²½ë¡œ ì œê±° í™•ì¸

### Phase 2: í•µì‹¬ ê¸°ëŠ¥ (Day 4-7)
- [ ] **Multi-Reference ROUGE** (Day 3-4)
  - [ ] `code/utils/metrics.py` ì™„ì „ êµ¬í˜„
  - [ ] 3ê°œ ì •ë‹µ ìš”ì•½ë¬¸ ìµœì  ì ìˆ˜ ê³„ì‚°
  - [ ] í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í†µí•©
  - [ ] `python test_rouge_calculator.py` ê²€ì¦ í†µê³¼

- [ ] **ì¶”ë¡  íŒŒì´í”„ë¼ì¸** (Day 5-6)
  - [ ] `code/core/inference.py` êµ¬í˜„
  - [ ] ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì¶”ë¡ 
  - [ ] CLI ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
  - [ ] `python test_inference_engine.py` ê²€ì¦ í†µê³¼

- [ ] **ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ** (Day 7)
  - [ ] `code/utils/experiment_utils.py` ì™„ì „ êµ¬í˜„
  - [ ] ExperimentTracker, ModelRegistry êµ¬í˜„
  - [ ] `python test_experiment_utils.py` ê²€ì¦ í†µê³¼

### Phase 3: ë°ì´í„° ì²˜ë¦¬ (Day 8-9)
- [ ] **Multi-Reference ë°ì´í„° ì²˜ë¦¬** (Day 8-9)
  - [ ] `code/utils/data_utils.py` í™•ì¥
  - [ ] 3ê°œ ì •ë‹µ ìš”ì•½ë¬¸ ë¡œë”©
  - [ ] ëŒ€íšŒ ì œì¶œ í˜•ì‹ ì™„ë²½ ì§€ì›
  - [ ] `python test_data_processor.py` ê²€ì¦ í†µê³¼

### Phase 4: í†µí•© ë° ê²€ì¦ (Day 10)
- [ ] **ì „ì²´ ì‹œìŠ¤í…œ í†µí•©**
  - [ ] ëª¨ë“  ëª¨ë“ˆ ê°„ ì—°ë™ í™•ì¸
  - [ ] í¬ë¡œìŠ¤ í”Œë«í¼ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
  - [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
  - [ ] ì„±ëŠ¥ ê¸°ì¤€ ë‹¬ì„± í™•ì¸

---

## âš ï¸ í•µì‹¬ ì£¼ì˜ì‚¬í•­

### 1. ìƒëŒ€ ê²½ë¡œ ì—„ê²© ì¤€ìˆ˜
```python
# âŒ ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš© ê¸ˆì§€
"/Users/jayden/project/data/train.csv"
"C:\\Users\\project\\data\\train.csv"

# âœ… ìƒëŒ€ ê²½ë¡œë§Œ í—ˆìš©
"data/train.csv"
"outputs/model/best_model"
"config/base_config.yaml"
```

### 2. ë””ë°”ì´ìŠ¤ ìµœì í™” í•„ìˆ˜
```python
# ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹œ ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ì ìš©
device = get_optimal_device()  # "mps", "cuda", "cpu"

# ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ì ìš©
if device == "mps":
    model = model.to("mps")
    torch_dtype = torch.float32  # MPSëŠ” float32 ê¶Œì¥
elif device == "cuda":
    model = model.to("cuda") 
    torch_dtype = torch.float16  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
```

### 3. ì‹¤í–‰ ì¤‘ì‹¬ ê²€ì¦
- **í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„± ë¶ˆí•„ìš”**
- **ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì œê³µ**
- **ì—ëŸ¬ ë°œìƒ ì‹œ ë°”ë¡œ ìˆ˜ì •í•˜ëŠ” ë°©ì‹**

---

## ğŸ¯ ìµœì¢… ì„±ê³µ ê¸°ì¤€

### ê¸°ìˆ ì  ì„±ê³µ ê¸°ì¤€
- [ ] **Mac (MPS) / Ubuntu (CUDA)ì—ì„œ ë™ì¼ ê²°ê³¼**
- [ ] **ëª¨ë“  ê²½ë¡œê°€ ìƒëŒ€ ê²½ë¡œ ê¸°ë°˜**
- [ ] **Multi-reference ROUGE ì •í™• ê³„ì‚°**
- [ ] **ëŒ€íšŒ ì œì¶œ í˜•ì‹ 100% ì¤€ìˆ˜**
- [ ] **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ì¡´ ëŒ€ë¹„ 120% ì´ë‚´**

### í’ˆì§ˆ ê´€ë¦¬ ê¸°ì¤€
- [ ] **ëª¨ë“  ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ í†µê³¼**
- [ ] **ì—ëŸ¬ ì²˜ë¦¬ ì™„ì „ êµ¬í˜„**
- [ ] **API ë¬¸ì„œ ì™„ì„±**
- [ ] **ì‚¬ìš©ì ê°€ì´ë“œ ì—…ë°ì´íŠ¸**

### ì¦‰ì‹œ ì‹¤í–‰ ê²€ì¦
```bash
# ìµœì¢… í†µí•© ê²€ì¦ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰)
python validate_paths.py          # ê²½ë¡œ ì‹œìŠ¤í…œ ê²€ì¦
python test_rouge_calculator.py   # ROUGE ê³„ì‚° ê²€ì¦  
python test_inference_engine.py   # ì¶”ë¡  ì—”ì§„ ê²€ì¦
python test_experiment_utils.py   # ì‹¤í—˜ ì¶”ì  ê²€ì¦
python test_data_processor.py     # ë°ì´í„° ì²˜ë¦¬ ê²€ì¦
```

ëª¨ë“  ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ê°€ ì„±ê³µí•˜ë©´ **Priority 1 êµ¬í˜„ ì™„ë£Œ**ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.

---

## ğŸ“ êµ¬í˜„ ì§€ì›

### ì¦‰ì‹œ ì‹œì‘ ë°©ë²•
1. **í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰**: ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
2. **ë‹¨ê³„ë³„ êµ¬í˜„**: PathManager â†’ ROUGE â†’ Inference â†’ Experiment â†’ Data ìˆœì„œ
3. **ì¦‰ì‹œ ê²€ì¦**: ê° ë‹¨ê³„ë§ˆë‹¤ í•´ë‹¹ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
4. **ì—ëŸ¬ ì¦‰ì‹œ ìˆ˜ì •**: í…ŒìŠ¤íŠ¸ ì½”ë“œ ì—†ì´ ì§ì ‘ ì‹¤í–‰ìœ¼ë¡œ ë¬¸ì œ í•´ê²°

### êµ¬í˜„ ìš°ì„ ìˆœìœ„
1. **PathManager (ìµœìš°ì„ )**: ëª¨ë“  ë‹¤ë¥¸ ëª¨ë“ˆì˜ ê¸°ë°˜
2. **ROUGE Calculator**: í‰ê°€ ì‹œìŠ¤í…œì˜ í•µì‹¬
3. **Inference Engine**: ì‹¤ì œ ì‚¬ìš©ì˜ í•µì‹¬  
4. **Experiment Utils**: ì²´ê³„ì  ê´€ë¦¬
5. **Data Processor**: ì™„ì „í•œ ë°ì´í„° ì§€ì›

ì´ ìˆœì„œë¡œ êµ¬í˜„í•˜ë©´ **ì˜ì¡´ì„± ë¬¸ì œ ì—†ì´** ì²´ê³„ì ì¸ ê°œë°œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
