# NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œ - ì¢…í•© ê°œë°œ ê°€ì´ë“œ

## ëª©ì°¨
1. [ê°œë°œ í™˜ê²½ ê°œìš”](#ê°œë°œ-í™˜ê²½-ê°œìš”)
2. [í”„ë¡œì íŠ¸ êµ¬ì¡° ì´í•´](#í”„ë¡œì íŠ¸-êµ¬ì¡°-ì´í•´)
3. [ê°œë°œ í™˜ê²½ ì„¤ì •](#ê°œë°œ-í™˜ê²½-ì„¤ì •)
4. [ë°ì´í„° ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°](#ë°ì´í„°-ì²˜ë¦¬-ì›Œí¬í”Œë¡œìš°)
5. [ëª¨ë¸ ê°œë°œ ê³¼ì •](#ëª¨ë¸-ê°œë°œ-ê³¼ì •)
6. [ì‹¤í—˜ ê´€ë¦¬ ì‹œìŠ¤í…œ](#ì‹¤í—˜-ê´€ë¦¬-ì‹œìŠ¤í…œ)
7. [ì„±ëŠ¥ ìµœì í™” ì „ëµ](#ì„±ëŠ¥-ìµœì í™”-ì „ëµ)
8. [ë””ë²„ê¹… ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#ë””ë²„ê¹…-ë°-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
9. [ì½”ë”© í‘œì¤€ ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](#ì½”ë”©-í‘œì¤€-ë°-ë² ìŠ¤íŠ¸-í”„ë™í‹°ìŠ¤)
10. [ê³ ê¸‰ ê°œë°œ ê¸°ë²•](#ê³ ê¸‰-ê°œë°œ-ê¸°ë²•)

---

## ê°œë°œ í™˜ê²½ ê°œìš”

ì´ ì¢…í•© ê°œë°œ ê°€ì´ë“œëŠ” NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œì„ íš¨ê³¼ì ìœ¼ë¡œ ê°œë°œí•˜ê¸° ìœ„í•œ ëª¨ë“  ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì´ˆê¸° í™˜ê²½ ì„¤ì •ë¶€í„° ê³ ê¸‰ ìµœì í™” ê¸°ë²•ê¹Œì§€, ê°œë°œìê°€ ì•Œì•„ì•¼ í•  ëª¨ë“  ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤.

### ê°œë°œ ì² í•™
- **ğŸ”„ ë°˜ë³µì  ê°œë°œ**: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ê³¼ ì§€ì†ì ì¸ ê°œì„ 
- **ğŸ“Š ë°ì´í„° ì¤‘ì‹¬**: ì‹¤í—˜ ê²°ê³¼ì™€ ë©”íŠ¸ë¦­ ê¸°ë°˜ ì˜ì‚¬ê²°ì •
- **ğŸ§ª ì‹¤í—˜ ì¶”ì **: ëª¨ë“  ì‹¤í—˜ê³¼ ê²°ê³¼ì˜ ì²´ê³„ì  ê´€ë¦¬
- **âš¡ ì„±ëŠ¥ ìµœì í™”**: ë©”ëª¨ë¦¬ì™€ ì†ë„ íš¨ìœ¨ì„± ê³ ë ¤
- **ğŸ¤ íŒ€ í˜‘ì—…**: ëª…í™•í•œ ì½”ë”© í‘œì¤€ê³¼ ë¬¸ì„œí™”

---

## í”„ë¡œì íŠ¸ êµ¬ì¡° ì´í•´

### ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
nlp-sum-lyj/
â”œâ”€â”€ code/                           # í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ core/                       # í•µì‹¬ ê¸°ëŠ¥ ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ models/                 # ëª¨ë¸ ì•„í‚¤í…ì²˜
â”‚   â”‚   â”œâ”€â”€ training/               # í•™ìŠµ ê´€ë ¨ ì½”ë“œ
â”‚   â”‚   â””â”€â”€ inference/              # ì¶”ë¡  ì—”ì§„
â”‚   â”œâ”€â”€ utils/                      # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚   â”‚   â”œâ”€â”€ data_utils.py          # ë°ì´í„° ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ metrics.py             # í‰ê°€ ë©”íŠ¸ë¦­
â”‚   â”‚   â”œâ”€â”€ path_utils.py          # ê²½ë¡œ ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ device_utils.py        # ë””ë°”ì´ìŠ¤ ìµœì í™”
â”‚   â”œâ”€â”€ config/                     # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ trainer.py                  # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ inference.py                # ì¶”ë¡  ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ experiments/                # ì‹¤í—˜ ê´€ë¦¬ ì½”ë“œ
â”œâ”€â”€ data/                           # ë°ì´í„° íŒŒì¼
â”‚   â”œâ”€â”€ train.csv                   # í•™ìŠµ ë°ì´í„°
â”‚   â”œâ”€â”€ dev.csv                     # ê²€ì¦ ë°ì´í„°
â”‚   â””â”€â”€ test.csv                    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
â”œâ”€â”€ outputs/                        # ê²°ê³¼ íŒŒì¼
â”‚   â”œâ”€â”€ models/                     # ì €ì¥ëœ ëª¨ë¸
â”‚   â”œâ”€â”€ experiments/                # ì‹¤í—˜ ê²°ê³¼
â”‚   â””â”€â”€ submissions/                # ì œì¶œ íŒŒì¼
â”œâ”€â”€ docs/                           # ë¬¸ì„œ
â””â”€â”€ requirements.txt                # ì˜ì¡´ì„± ëª…ì„¸
```

### ì½”ë“œ ëª¨ë“ˆ ì„¤ê³„ ì›ì¹™

#### 1. í•µì‹¬ ëª¨ë“ˆ ë¶„ë¦¬
```python
# core/models/ - ëª¨ë¸ ì •ì˜
# core/training/ - í•™ìŠµ ë¡œì§
# core/inference/ - ì¶”ë¡  ë¡œì§
# utils/ - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ìœ í‹¸ë¦¬í‹°
```

#### 2. ì„¤ì • ê¸°ë°˜ ê°œë°œ
```python
# config/base_config.yaml - ê¸°ë³¸ ì„¤ì •
# config/models/ - ëª¨ë¸ë³„ ì„¤ì •
# config/experiments/ - ì‹¤í—˜ë³„ ì„¤ì •
```

#### 3. ìƒëŒ€ ê²½ë¡œ ì‹œìŠ¤í…œ
```python
# ëª¨ë“  ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
# ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš© ê¸ˆì§€ (í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„±)
```

---

## ê°œë°œ í™˜ê²½ ì„¤ì •

### 1. ê¸°ë³¸ í™˜ê²½ êµ¬ì„±

#### Python í™˜ê²½ ì„¤ì •
```bash
# Python 3.8+ í™˜ê²½ ìƒì„±
python -m venv venv

# í™˜ê²½ í™œì„±í™”
source venv/bin/activate  # Linux/Mac
# venv\\Scripts\\activate   # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œìš© ì¶”ê°€ íŒ¨í‚¤ì§€
pip install jupyter ipykernel black flake8 pytest
```

#### í•µì‹¬ ì˜ì¡´ì„± í™•ì¸
```python
import torch
import transformers
import pandas as pd
import numpy as np

print(f\"PyTorch: {torch.__version__}\")
print(f\"Transformers: {transformers.__version__}\")
print(f\"CUDA Available: {torch.cuda.is_available()}\")

# MPS (Apple Silicon) í™•ì¸
if hasattr(torch.backends, 'mps'):
    print(f\"MPS Available: {torch.backends.mps.is_available()}\")
```

### 2. ê°œë°œ ë„êµ¬ ì„¤ì •

#### VS Code ì„¤ì •
```json
// .vscode/settings.json
{
    \"python.defaultInterpreterPath\": \"./venv/bin/python\",
    \"python.formatting.provider\": \"black\",
    \"python.linting.enabled\": true,
    \"python.linting.flake8Enabled\": true,
    \"files.exclude\": {
        \"**/__pycache__\": true,
        \"**/*.pyc\": true,
        \"outputs/experiments/\": true
    }
}
```

#### Git ì„¤ì •
```bash
# .gitignore í™•ì¸
echo \"outputs/experiments/\" >> .gitignore
echo \"*.pyc\" >> .gitignore
echo \"__pycache__/\" >> .gitignore
echo \".env\" >> .gitignore

# ì»¤ë°‹ í›… ì„¤ì • (ì˜µì…˜)
cat > .git/hooks/pre-commit << 'EOF'
#!/bin/sh
# ì½”ë“œ í¬ë§·íŒ… í™•ì¸
black --check code/
flake8 code/
EOF
chmod +x .git/hooks/pre-commit
```

### 3. ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •

#### ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹œìŠ¤í…œ
```python
# utils/device_utils.py (êµ¬í˜„ ì˜ˆì‹œ)
import torch
import platform

def get_optimal_device():
    \"\"\"ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€\"\"\"
    
    # CUDA í™•ì¸
    if torch.cuda.is_available():
        return \"cuda\"
    
    # MPS (Apple Silicon) í™•ì¸
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        return \"mps\"
    
    # CPU ì‚¬ìš©
    return \"cpu\"

def get_device_config(device: str) -> dict:
    \"\"\"ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •\"\"\"
    
    configs = {
        \"cuda\": {
            \"batch_size\": 16,
            \"fp16\": True,
            \"dataloader_pin_memory\": True,
            \"torch_dtype\": torch.float16
        },
        \"mps\": {
            \"batch_size\": 8,
            \"fp16\": False,  # MPS float16 ì´ìŠˆ íšŒí”¼
            \"dataloader_pin_memory\": False,
            \"torch_dtype\": torch.float32
        },
        \"cpu\": {
            \"batch_size\": 4,
            \"fp16\": False,
            \"dataloader_pin_memory\": False,
            \"torch_dtype\": torch.float32
        }
    }
    
    return configs.get(device, configs[\"cpu\"])

# ì‚¬ìš© ì˜ˆì‹œ
device = get_optimal_device()
config = get_device_config(device)
print(f\"ê°ì§€ëœ ë””ë°”ì´ìŠ¤: {device}\")
print(f\"ê¶Œì¥ ì„¤ì •: {config}\")
```

---

## ë°ì´í„° ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°

### 1. ë‹¤ì¤‘ ì°¸ì¡° ìš”ì•½ ë°ì´í„° ì²˜ë¦¬

#### ë°ì´í„° ë¡œë”© ì‹œìŠ¤í…œ
```python
# utils/data_utils.py (í•µì‹¬ ê¸°ëŠ¥ ì„¤ëª…)
class DataProcessor:
    \"\"\"ë‹¤ì¤‘ ì°¸ì¡° ìš”ì•½ ë°ì´í„° ì „ìš© í”„ë¡œì„¸ì„œ\"\"\"
    
    def __init__(self, project_root=None):
        \"\"\"
        í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        \"\"\"
        # PathManagerë¥¼ í†µí•œ ìƒëŒ€ ê²½ë¡œ ê´€ë¦¬
        self.path_manager = PathManager(project_root)
        
    def load_multi_reference_data(self, file_path: str) -> pd.DataFrame:
        \"\"\"
        ë‹¤ì¤‘ ì°¸ì¡° ìš”ì•½ ë°ì´í„° ë¡œë”©
        
        ì§€ì› í˜•ì‹:
        1. ê°œë³„ ì»¬ëŸ¼: summary1, summary2, summary3
        2. êµ¬ë¶„ì ë¶„ë¦¬: summary ì»¬ëŸ¼ì— ||| êµ¬ë¶„ìë¡œ ë¶„ë¦¬
        3. JSON í˜•ì‹: summary ì»¬ëŸ¼ì— JSON ë°°ì—´
        \"\"\"
        # ìƒëŒ€ ê²½ë¡œ í•´ê²°
        full_path = self.path_manager.resolve_path(file_path)
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not full_path.exists():
            raise FileNotFoundError(f\"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")
        
        # CSV ë¡œë”©
        df = pd.read_csv(full_path, encoding='utf-8')
        
        # Multi-reference í˜•ì‹ ìë™ ê°ì§€ ë° ë³€í™˜
        df = self._detect_and_convert_format(df)
        
        print(f\"ğŸ“Š Multi-reference ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df)} ìƒ˜í”Œ\")
        return df
    
    def _detect_and_convert_format(self, df: pd.DataFrame) -> pd.DataFrame:
        \"\"\"ë‹¤ì¤‘ ì°¸ì¡° í˜•ì‹ ìë™ ê°ì§€ ë° í‘œì¤€í™”\"\"\"
        
        # í˜•ì‹ 1: ê°œë³„ ì»¬ëŸ¼ (summary1, summary2, summary3)
        if all(col in df.columns for col in ['summary1', 'summary2', 'summary3']):
            df['summaries'] = df[['summary1', 'summary2', 'summary3']].apply(
                lambda x: [str(val) if pd.notna(val) else \"\" for val in x], axis=1
            )
            print(\"âœ… ê°œë³„ ì»¬ëŸ¼ í˜•ì‹ì˜ multi-reference ë°ì´í„° ê°ì§€\")
            
        # í˜•ì‹ 2: êµ¬ë¶„ì ë¶„ë¦¬ (summary ì»¬ëŸ¼ì— ||| êµ¬ë¶„ì)
        elif 'summary' in df.columns:
            df['summaries'] = df['summary'].apply(self._parse_multiple_summaries)
            print(\"âœ… êµ¬ë¶„ì ë¶„ë¦¬ í˜•ì‹ì˜ multi-reference ë°ì´í„° ê°ì§€\")
            
        else:
            raise ValueError(
                \"ì§€ì›ë˜ëŠ” summary í˜•ì‹ì´ ì—†ìŠµë‹ˆë‹¤. \"
                \"summary1,summary2,summary3 ì»¬ëŸ¼ ë˜ëŠ” summary ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\"
            )
        
        return df
    
    def _parse_multiple_summaries(self, summary_text: str) -> list:
        \"\"\"êµ¬ë¶„ìë¡œ ë¶„ë¦¬ëœ ìš”ì•½ë¬¸ íŒŒì‹±\"\"\"
        if pd.isna(summary_text):
            return [\"\", \"\", \"\"]
        
        # ë‹¤ì–‘í•œ êµ¬ë¶„ì ì§€ì›
        separators = ['|||', '##', '---', '\
\
']
        
        for sep in separators:
            if sep in summary_text:
                summaries = [s.strip() for s in summary_text.split(sep)]
                # 3ê°œë¡œ íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
                while len(summaries) < 3:
                    summaries.append(\"\")
                return summaries[:3]
        
        # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ë‹¨ì¼ ìš”ì•½ë¬¸ìœ¼ë¡œ ì²˜ë¦¬
        return [summary_text.strip(), \"\", \"\"]
```

#### ì œì¶œ í˜•ì‹ ë³€í™˜
```python
def export_submission_format(self, 
                            predictions: List[str],
                            fnames: List[str],
                            output_path: str) -> pd.DataFrame:
    \"\"\"
    ëŒ€íšŒ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ì €ì¥
    
    Args:
        predictions: ì˜ˆì¸¡ëœ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
        fnames: íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
    
    Returns:
        submission_df: ì œì¶œ í˜•ì‹ ë°ì´í„°í”„ë ˆì„
    \"\"\"
    if len(predictions) != len(fnames):
        raise ValueError(f\"ì˜ˆì¸¡ê³¼ íŒŒì¼ëª… ê°œìˆ˜ ë¶ˆì¼ì¹˜: {len(predictions)} vs {len(fnames)}\")
    
    # ì œì¶œ í˜•ì‹ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    submission_df = pd.DataFrame({
        'fname': fnames,
        'summary': predictions
    })
    
    # ìƒëŒ€ ê²½ë¡œ í™•ì¸ ë° í•´ê²°
    output_path = Path(output_path)
    if output_path.is_absolute():
        raise ValueError(f\"ì¶œë ¥ ê²½ë¡œëŠ” ìƒëŒ€ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤: {output_path}\")
    
    full_output_path = self.path_manager.resolve_path(output_path)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    full_output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # CSV ì €ì¥ (ëŒ€íšŒ ì œì¶œ í˜•ì‹)
    submission_df.to_csv(full_output_path, index=False, encoding='utf-8')
    
    print(f\"ğŸ’¾ ì œì¶œ íŒŒì¼ ì €ì¥: {output_path}\")
    print(f\"ğŸ“‹ í˜•ì‹: fname, summary ({len(submission_df)} í•­ëª©)\")
    
    return submission_df
```

---

## ëª¨ë¸ ê°œë°œ ê³¼ì •

### 1. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

#### ì§€ì› ëª¨ë¸ ì•„í‚¤í…ì²˜
```python
# core/models/model_factory.py (êµ¬ì¡° ì„¤ëª…)
class ModelFactory:
    \"\"\"ëª¨ë¸ íŒ©í† ë¦¬ í´ë˜ìŠ¤\"\"\"
    
    SUPPORTED_MODELS = {
        \"kobart\": {
            \"base_model\": \"gogamza/kobart-base-v2\",
            \"tokenizer_class\": \"BartTokenizer\",
            \"model_class\": \"BartForConditionalGeneration\",
            \"max_position_embeddings\": 1024
        },
        \"kogpt2\": {
            \"base_model\": \"skt/kogpt2-base-v2\", 
            \"tokenizer_class\": \"GPT2Tokenizer\",
            \"model_class\": \"GPT2LMHeadModel\",
            \"max_position_embeddings\": 1024
        },
        \"kt5\": {
            \"base_model\": \"KETI-AIR/ke-t5-base\",
            \"tokenizer_class\": \"T5Tokenizer\", 
            \"model_class\": \"T5ForConditionalGeneration\",
            \"max_position_embeddings\": 512
        },
        \"mt5\": {
            \"base_model\": \"google/mt5-base\",
            \"tokenizer_class\": \"T5Tokenizer\",
            \"model_class\": \"MT5ForConditionalGeneration\", 
            \"max_position_embeddings\": 1024
        }
    }
    
    @classmethod
    def create_model_and_tokenizer(cls, model_name: str, device: str):
        \"\"\"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ìƒì„±\"\"\"
        
        if model_name not in cls.SUPPORTED_MODELS:
            raise ValueError(f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}\")
        
        config = cls.SUPPORTED_MODELS[model_name]
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        tokenizer = getattr(transformers, config[\"tokenizer_class\"]).from_pretrained(
            config[\"base_model\"]
        )
        
        # ëª¨ë¸ ë¡œë”© ë° ë””ë°”ì´ìŠ¤ ì„¤ì •
        model = getattr(transformers, config[\"model_class\"]).from_pretrained(
            config[\"base_model\"]
        )
        
        # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™”
        device_config = get_device_config(device)
        if device_config.get(\"torch_dtype\"):
            model = model.to(dtype=device_config[\"torch_dtype\"])
        
        model = model.to(device)
        
        return model, tokenizer, config
```

#### ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì„±
```python
# core/models/custom_models.py (ì„¤ê³„ ê°€ì´ë“œ)
class DialogueSummarizationModel:
    \"\"\"ëŒ€í™” ìš”ì•½ ì „ìš© ëª¨ë¸ ë˜í¼\"\"\"
    
    def __init__(self, model_name: str, device: str = \"auto\"):
        \"\"\"
        ëŒ€í™” ìš”ì•½ì— ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™”
        \"\"\"
        if device == \"auto\":
            device = get_optimal_device()
        
        self.device = device
        self.model, self.tokenizer, self.config = ModelFactory.create_model_and_tokenizer(
            model_name, device
        )
        
        # ëŒ€í™” ìš”ì•½ íŠ¹í™” ì„¤ì •
        self._setup_generation_config()
        self._setup_special_tokens()
    
    def _setup_generation_config(self):
        \"\"\"ìƒì„± ì„¤ì • ìµœì í™”\"\"\"
        self.generation_config = {
            \"max_length\": 128,
            \"min_length\": 10,
            \"num_beams\": 4,
            \"length_penalty\": 1.0,
            \"no_repeat_ngram_size\": 3,
            \"early_stopping\": True,
            \"do_sample\": False
        }
    
    def _setup_special_tokens(self):
        \"\"\"íŠ¹ìˆ˜ í† í° ì„¤ì •\"\"\"
        # ëŒ€í™” êµ¬ë¶„ì ì¶”ê°€ (í•„ìš”ì‹œ)
        special_tokens = [\"<speaker1>\", \"<speaker2>\", \"<turn>\"]
        
        if hasattr(self.tokenizer, 'add_special_tokens'):
            self.tokenizer.add_special_tokens({
                \"additional_special_tokens\": special_tokens
            })
            self.model.resize_token_embeddings(len(self.tokenizer))
    
    def preprocess_dialogue(self, dialogue: str) -> str:
        \"\"\"ëŒ€í™” ì „ì²˜ë¦¬\"\"\"
        
        # ê¸°ë³¸ ì •ë¦¬
        dialogue = dialogue.strip()
        
        # í™”ì êµ¬ë¶„ ì •ê·œí™” (ì˜µì…˜)
        dialogue = re.sub(r'í™”ì\\s*(\\d+)\\s*:', r'<speaker\\1>:', dialogue)
        
        # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        dialogue = re.sub(r'\\s+', ' ', dialogue)
        
        return dialogue
    
    def generate_summary(self, dialogue: str, **generation_kwargs) -> str:
        \"\"\"ìš”ì•½ ìƒì„±\"\"\"
        
        # ì „ì²˜ë¦¬
        processed_dialogue = self.preprocess_dialogue(dialogue)
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            processed_dialogue,
            max_length=self.config[\"max_position_embeddings\"],
            truncation=True,
            padding=True,
            return_tensors=\"pt\"
        ).to(self.device)
        
        # ìƒì„± ì„¤ì • ë³‘í•©
        final_config = {**self.generation_config, **generation_kwargs}
        
        # ìš”ì•½ ìƒì„±
        with torch.no_grad():
            if self.device == \"mps\":
                # MPS ìµœì í™”
                with torch.autocast(device_type=\"cpu\", enabled=False):
                    outputs = self.model.generate(**inputs, **final_config)
            else:
                outputs = self.model.generate(**inputs, **final_config)
        
        # ë””ì½”ë”©
        summary = self.tokenizer.decode(
            outputs[0], 
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        return summary.strip()
```

### 2. í•™ìŠµ ê³¼ì • ìµœì í™”

#### í•™ìŠµ ì„¤ì • ê´€ë¦¬
```python
# core/training/trainer_config.py (ì„¤ì • ì˜ˆì‹œ)
class TrainingConfig:
    \"\"\"í•™ìŠµ ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤\"\"\"
    
    def __init__(self, device: str = \"auto\"):
        if device == \"auto\":
            device = get_optimal_device()
        
        self.device = device
        self.device_config = get_device_config(device)
        
        # ê¸°ë³¸ í•™ìŠµ ì„¤ì •
        self.base_config = {
            \"learning_rate\": 5e-5,
            \"num_train_epochs\": 3,
            \"warmup_ratio\": 0.1,
            \"weight_decay\": 0.01,
            \"logging_steps\": 100,
            \"save_steps\": 500,
            \"eval_steps\": 500,
            \"save_total_limit\": 3,
            \"load_best_model_at_end\": True,
            \"metric_for_best_model\": \"rouge_combined_f1\",
            \"greater_is_better\": True,
        }
        
        # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ë³‘í•©
        self.training_args = {**self.base_config, **self.device_config}
    
    def get_training_arguments(self, output_dir: str):
        \"\"\"TrainingArguments ê°ì²´ ìƒì„±\"\"\"
        from transformers import TrainingArguments
        
        return TrainingArguments(
            output_dir=output_dir,
            **self.training_args
        )
    
    def update_config(self, **kwargs):
        \"\"\"ì„¤ì • ì—…ë°ì´íŠ¸\"\"\"
        self.training_args.update(kwargs)
```

#### ë°ì´í„°ì…‹ í´ë˜ìŠ¤
```python
# core/training/dataset.py (êµ¬ì¡° ê°€ì´ë“œ)
class DialogueSummarizationDataset(torch.utils.data.Dataset):
    \"\"\"ëŒ€í™” ìš”ì•½ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\"\"\"
    
    def __init__(self, 
                 dialogues: List[str], 
                 summaries: List[str],
                 tokenizer,
                 max_input_length: int = 512,
                 max_target_length: int = 128):
        
        self.dialogues = dialogues
        self.summaries = summaries
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_target_length = max_target_length
    
    def __len__(self):
        return len(self.dialogues)
    
    def __getitem__(self, idx):
        dialogue = self.dialogues[idx]
        summary = self.summaries[idx]
        
        # ì…ë ¥ í† í¬ë‚˜ì´ì§•
        model_inputs = self.tokenizer(
            dialogue,
            max_length=self.max_input_length,
            truncation=True,
            padding=\"max_length\",
            return_tensors=\"pt\"
        )
        
        # íƒ€ê²Ÿ í† í¬ë‚˜ì´ì§•
        with self.tokenizer.as_target_tokenizer():
            labels = self.tokenizer(
                summary,
                max_length=self.max_target_length,
                truncation=True,
                padding=\"max_length\",
                return_tensors=\"pt\"
            )
        
        model_inputs[\"labels\"] = labels[\"input_ids\"]
        
        # ë°°ì¹˜ ì°¨ì› ì œê±°
        return {k: v.squeeze() for k, v in model_inputs.items()}
```

---

## ì‹¤í—˜ ê´€ë¦¬ ì‹œìŠ¤í…œ

### 1. ì‹¤í—˜ ì¶”ì  ë„êµ¬

#### ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ê´€ë¦¬
```python
# utils/experiment_utils.py (í•µì‹¬ ê¸°ëŠ¥)
class ExperimentTracker:
    \"\"\"ì‹¤í—˜ ì¶”ì  ë° ê´€ë¦¬ í´ë˜ìŠ¤\"\"\"
    
    def __init__(self, experiments_dir: str = \"outputs/experiments\"):
        self.experiments_dir = Path(experiments_dir)
        self.experiments_dir.mkdir(parents=True, exist_ok=True)
        
        self.current_experiment = None
        self.experiment_data = {}
    
    def start_experiment(self, 
                        name: str, 
                        description: str,
                        config: dict) -> str:
        \"\"\"ìƒˆ ì‹¤í—˜ ì‹œì‘\"\"\"
        
        experiment_id = self._generate_experiment_id()
        timestamp = datetime.now().isoformat()
        
        self.experiment_data = {
            \"id\": experiment_id,
            \"name\": name,
            \"description\": description,
            \"config\": config,
            \"start_time\": timestamp,
            \"status\": \"running\",
            \"device\": get_optimal_device(),
            \"metrics\": [],
            \"checkpoints\": [],
            \"final_results\": None
        }
        
        self.current_experiment = experiment_id
        self._save_experiment_data()
        
        print(f\"ğŸ§ª ì‹¤í—˜ ì‹œì‘: {name} (ID: {experiment_id[:8]})\")
        return experiment_id
    
    def log_metrics(self, metrics: dict, step: int = None):
        \"\"\"ë©”íŠ¸ë¦­ ë¡œê¹…\"\"\"
        if not self.current_experiment:
            raise ValueError(\"í™œì„± ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤\")
        
        metric_entry = {
            \"step\": step or len(self.experiment_data[\"metrics\"]),
            \"timestamp\": datetime.now().isoformat(),
            **metrics
        }
        
        self.experiment_data[\"metrics\"].append(metric_entry)
        self._save_experiment_data()
    
    def log_checkpoint(self, checkpoint_path: str, metrics: dict):
        \"\"\"ì²´í¬í¬ì¸íŠ¸ ë¡œê¹…\"\"\"
        checkpoint_entry = {
            \"path\": checkpoint_path,
            \"timestamp\": datetime.now().isoformat(),
            \"metrics\": metrics
        }
        
        self.experiment_data[\"checkpoints\"].append(checkpoint_entry)
        self._save_experiment_data()
    
    def end_experiment(self, 
                      final_metrics: dict, 
                      status: str = \"completed\"):
        \"\"\"ì‹¤í—˜ ì¢…ë£Œ\"\"\"
        if not self.current_experiment:
            raise ValueError(\"í™œì„± ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤\")
        
        self.experiment_data.update({
            \"end_time\": datetime.now().isoformat(),
            \"status\": status,
            \"final_results\": final_metrics
        })
        
        self._save_experiment_data()
        
        print(f\"ğŸ ì‹¤í—˜ ì™„ë£Œ: {self.experiment_data['name']}\")
        print(f\"ğŸ“Š ìµœì¢… ì„±ëŠ¥: {final_metrics}\")
        
        self.current_experiment = None
    
    def _generate_experiment_id(self) -> str:
        \"\"\"ì‹¤í—˜ ID ìƒì„±\"\"\"
        import uuid
        return str(uuid.uuid4())
    
    def _save_experiment_data(self):
        \"\"\"ì‹¤í—˜ ë°ì´í„° ì €ì¥\"\"\"
        if not self.current_experiment:
            return
        
        file_path = self.experiments_dir / f\"{self.current_experiment}.json\"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.experiment_data, f, indent=2, ensure_ascii=False)
    
    def load_experiment(self, experiment_id: str) -> dict:
        \"\"\"ì‹¤í—˜ ë°ì´í„° ë¡œë”©\"\"\"
        file_path = self.experiments_dir / f\"{experiment_id}.json\"
        
        if not file_path.exists():
            raise FileNotFoundError(f\"ì‹¤í—˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {experiment_id}\")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def get_experiment_summary(self) -> pd.DataFrame:
        \"\"\"ëª¨ë“  ì‹¤í—˜ ìš”ì•½ ì¡°íšŒ\"\"\"
        experiments = []
        
        for exp_file in self.experiments_dir.glob(\"*.json\"):
            try:
                with open(exp_file, 'r', encoding='utf-8') as f:
                    exp_data = json.load(f)
                
                # ìµœê³  ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                best_rouge = 0
                if exp_data.get('metrics'):
                    rouge_scores = [m.get('rouge_combined_f1', 0) for m in exp_data['metrics']]
                    best_rouge = max(rouge_scores) if rouge_scores else 0
                
                experiments.append({
                    'id': exp_data['id'][:8],
                    'name': exp_data['name'],
                    'status': exp_data['status'],
                    'device': exp_data.get('device', 'unknown'),
                    'start_time': exp_data['start_time'],
                    'best_rouge_combined_f1': best_rouge
                })
                
            except Exception as e:
                print(f\"âš ï¸ ì‹¤í—˜ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ {exp_file}: {e}\")
        
        df = pd.DataFrame(experiments)
        return df.sort_values('best_rouge_combined_f1', ascending=False)
```

### 2. ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬

#### ëª¨ë¸ ë²„ì „ ê´€ë¦¬
```python
# utils/model_registry.py (êµ¬ì¡° ê°€ì´ë“œ)
class ModelRegistry:
    \"\"\"ëª¨ë¸ ë²„ì „ ë° ì„±ëŠ¥ ê´€ë¦¬\"\"\"
    
    def __init__(self, registry_path: str = \"outputs/model_registry.json\"):
        self.registry_path = Path(registry_path)
        self.registry_path.parent.mkdir(parents=True, exist_ok=True)
        
        self.models = self._load_registry()
    
    def register_model(self,
                      name: str,
                      architecture: str,
                      config: dict,
                      performance: dict,
                      model_path: str = None,
                      experiment_id: str = None) -> str:
        \"\"\"ëª¨ë¸ ë“±ë¡\"\"\"
        
        model_id = self._generate_model_id()
        
        model_info = {
            \"id\": model_id,
            \"name\": name,
            \"architecture\": architecture,
            \"config\": config,
            \"performance\": performance,
            \"model_path\": model_path,
            \"experiment_id\": experiment_id,
            \"created_at\": datetime.now().isoformat(),
            \"tags\": []
        }
        
        self.models[model_id] = model_info
        self._save_registry()
        
        print(f\"ğŸ“ ëª¨ë¸ ë“±ë¡: {name} (ID: {model_id[:8]})\")
        return model_id
    
    def get_best_model(self, 
                      architecture: str = None,
                      metric: str = \"rouge_combined_f1\") -> dict:
        \"\"\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¡°íšŒ\"\"\"
        
        filtered_models = self.models.values()
        
        if architecture:
            filtered_models = [
                m for m in filtered_models 
                if m[\"architecture\"] == architecture
            ]
        
        if not filtered_models:
            return None
        
        # ì„±ëŠ¥ ê¸°ì¤€ ì •ë ¬
        best_model = max(
            filtered_models,
            key=lambda m: m[\"performance\"].get(metric, 0)
        )
        
        return best_model
    
    def get_models_summary(self) -> pd.DataFrame:
        \"\"\"ëª¨ë¸ ìš”ì•½ í…Œì´ë¸”\"\"\"
        
        if not self.models:
            return pd.DataFrame()
        
        model_data = []
        for model_info in self.models.values():
            model_data.append({
                'id': model_info['id'][:8],
                'name': model_info['name'],
                'architecture': model_info['architecture'],
                'rouge_combined_f1': model_info['performance'].get('rouge_combined_f1', 0),
                'created_at': model_info['created_at'][:10]  # ë‚ ì§œë§Œ
            })
        
        df = pd.DataFrame(model_data)
        return df.sort_values('rouge_combined_f1', ascending=False)
```

---

## ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 1. ë©”ëª¨ë¦¬ ìµœì í™”

#### ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
```python
# utils/optimization.py (ìµœì í™” ê°€ì´ë“œ)
class AdaptiveBatchProcessor:
    \"\"\"ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •\"\"\"
    
    def __init__(self, device: str, initial_batch_size: int = 8):
        self.device = device
        self.current_batch_size = initial_batch_size
        self.max_batch_size = self._get_max_batch_size()
        self.min_batch_size = 1
        
        # ì„±ëŠ¥ ì¶”ì 
        self.success_count = 0
        self.oom_count = 0
    
    def _get_max_batch_size(self) -> int:
        \"\"\"ë””ë°”ì´ìŠ¤ë³„ ìµœëŒ€ ë°°ì¹˜ í¬ê¸°\"\"\"
        if self.device == \"mps\":
            return 8  # MPS ë©”ëª¨ë¦¬ ì œì•½
        elif self.device == \"cuda\":
            # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì •
            try:
                total_memory = torch.cuda.get_device_properties(0).total_memory
                memory_gb = total_memory / (1024**3)
                return min(32, int(memory_gb * 2))
            except:
                return 16
        else:
            return 4  # CPU ì œì•½
    
    def process_batch(self, data_loader, process_fn):
        \"\"\"ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬\"\"\"
        
        results = []
        
        for batch in data_loader:
            try:
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹œë„
                batch_results = process_fn(batch)
                results.extend(batch_results)
                
                self.success_count += 1
                
                # ì„±ê³µ ì‹œ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê³ ë ¤
                if (self.success_count % 10 == 0 and 
                    self.current_batch_size < self.max_batch_size):
                    self.current_batch_size = min(
                        self.current_batch_size + 1, 
                        self.max_batch_size
                    )
                    print(f\"ğŸ“ˆ ë°°ì¹˜ í¬ê¸° ì¦ê°€: {self.current_batch_size}\")
                
            except torch.cuda.OutOfMemoryError:
                # OOM ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
                self.oom_count += 1
                old_size = self.current_batch_size
                self.current_batch_size = max(
                    self.current_batch_size // 2, 
                    self.min_batch_size
                )
                
                print(f\"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, ë°°ì¹˜ í¬ê¸° {old_size} â†’ {self.current_batch_size}\")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.device == \"cuda\":
                    torch.cuda.empty_cache()
                
                # ì¤„ì–´ë“  ë°°ì¹˜ë¡œ ì¬ì‹œë„
                continue
        
        return results
```

#### ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
```python
def monitor_memory_usage(func):
    \"\"\"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°\"\"\"
    
    def wrapper(*args, **kwargs):
        import psutil
        import gc
        
        # ì‹œì‘ ë©”ëª¨ë¦¬
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        if torch.cuda.is_available():
            gpu_memory_before = torch.cuda.memory_allocated() / 1024 / 1024
        else:
            gpu_memory_before = 0
        
        try:
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ì¢…ë£Œ ë©”ëª¨ë¦¬
            memory_after = process.memory_info().rss / 1024 / 1024
            memory_diff = memory_after - memory_before
            
            if torch.cuda.is_available():
                gpu_memory_after = torch.cuda.memory_allocated() / 1024 / 1024
                gpu_memory_diff = gpu_memory_after - gpu_memory_before
            else:
                gpu_memory_diff = 0
            
            print(f\"ğŸ’¾ {func.__name__} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:\")
            print(f\"  System: {memory_diff:+.1f} MB\")
            if gpu_memory_diff:
                print(f\"  GPU: {gpu_memory_diff:+.1f} MB\")
            
            return result
            
        finally:
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    return wrapper
```

### 2. í•™ìŠµ ì†ë„ ìµœì í™”

#### ë°ì´í„° ë¡œë”© ìµœì í™”
```python
# utils/data_loading.py (ìµœì í™” ê°€ì´ë“œ)
def create_optimized_dataloader(dataset, 
                               batch_size: int,
                               device: str,
                               num_workers: int = None) -> DataLoader:
    \"\"\"ìµœì í™”ëœ ë°ì´í„°ë¡œë” ìƒì„±\"\"\"
    
    # ë””ë°”ì´ìŠ¤ë³„ ì›Œì»¤ ìˆ˜ ìë™ ì„¤ì •
    if num_workers is None:
        if device == \"cuda\":
            num_workers = min(8, os.cpu_count())
        elif device == \"mps\":
            num_workers = 4  # MPSëŠ” ì œí•œì 
        else:
            num_workers = 2  # CPU
    
    # ë””ë°”ì´ìŠ¤ë³„ ì„¤ì •
    device_config = get_device_config(device)
    pin_memory = device_config.get(\"dataloader_pin_memory\", False)
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=num_workers > 0,
        prefetch_factor=2 if num_workers > 0 else 2
    )
```

---

## ë””ë²„ê¹… ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

#### ë””ë°”ì´ìŠ¤ ê´€ë ¨ ë¬¸ì œ
```python
# utils/debug.py (ë””ë²„ê¹… ë„êµ¬)
def diagnose_device_issues():
    \"\"\"ë””ë°”ì´ìŠ¤ ê´€ë ¨ ë¬¸ì œ ì§„ë‹¨\"\"\"
    
    print(\"ğŸ” ë””ë°”ì´ìŠ¤ ì§„ë‹¨ ì‹œì‘\")
    print(\"=\" * 50)
    
    # í”Œë«í¼ ì •ë³´
    import platform
    print(f\"í”Œë«í¼: {platform.system()} {platform.machine()}\")
    print(f\"Python: {platform.python_version()}\")
    
    # PyTorch ì •ë³´
    print(f\"PyTorch: {torch.__version__}\")
    
    # CUDA ì§„ë‹¨
    print(f\"\
ğŸ–¥ï¸ CUDA ì •ë³´:\")
    print(f\"  ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")
    if torch.cuda.is_available():
        print(f\"  ë²„ì „: {torch.version.cuda}\")
        print(f\"  ë””ë°”ì´ìŠ¤ ìˆ˜: {torch.cuda.device_count()}\")
        print(f\"  í˜„ì¬ ë””ë°”ì´ìŠ¤: {torch.cuda.current_device()}\")
        print(f\"  GPU ì´ë¦„: {torch.cuda.get_device_name()}\")
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory = torch.cuda.get_device_properties(0).total_memory
        print(f\"  ì´ ë©”ëª¨ë¦¬: {memory / 1024**3:.1f} GB\")
    
    # MPS ì§„ë‹¨ (Apple Silicon)
    if hasattr(torch.backends, 'mps'):
        print(f\"\
ğŸ MPS ì •ë³´:\")
        print(f\"  ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}\")
        print(f\"  Built: {torch.backends.mps.is_built()}\")
    
    # ê¶Œì¥ ì„¤ì •
    optimal_device = get_optimal_device()
    config = get_device_config(optimal_device)
    
    print(f\"\
âœ… ê¶Œì¥ ì„¤ì •:\")
    print(f\"  ë””ë°”ì´ìŠ¤: {optimal_device}\")
    print(f\"  ë°°ì¹˜ í¬ê¸°: {config['batch_size']}\")
    print(f\"  FP16: {config['fp16']}\")
    print(f\"  Pin Memory: {config['dataloader_pin_memory']}\")

def test_model_loading(model_name: str = \"kobart\"):
    \"\"\"ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸\"\"\"
    
    print(f\"ğŸ§ª ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸: {model_name}\")
    
    try:
        device = get_optimal_device()
        model, tokenizer, config = ModelFactory.create_model_and_tokenizer(
            model_name, device
        )
        
        print(\"âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ\")
        
        # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        test_input = \"í™”ì1: ì•ˆë…•í•˜ì„¸ìš”\
í™”ì2: ì•ˆë…•í•˜ì„¸ìš”\"
        inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=50)
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f\"âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result[:50]}...\")
        
    except Exception as e:
        print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")
        import traceback
        traceback.print_exc()
```

#### ë°ì´í„° ê´€ë ¨ ë¬¸ì œ
```python
def validate_data_pipeline(data_path: str = \"data/train.csv\"):
    \"\"\"ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²€ì¦\"\"\"
    
    print(f\"ğŸ“Š ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²€ì¦: {data_path}\")
    
    try:
        # ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
        processor = DataProcessor()
        df = processor.load_multi_reference_data(data_path)
        
        print(f\"âœ… ë°ì´í„° ë¡œë”© ì„±ê³µ: {len(df)} ìƒ˜í”Œ\")
        
        # ê¸°ë³¸ í†µê³„
        print(f\"ğŸ“‹ ë°ì´í„° ì •ë³´:\")
        print(f\"  ì»¬ëŸ¼: {list(df.columns)}\")
        print(f\"  ëŒ€í™” í‰ê·  ê¸¸ì´: {df['dialogue'].str.len().mean():.1f}\")
        
        if 'summaries' in df.columns:
            # Multi-reference ìš”ì•½ë¬¸ ì²´í¬
            sample_summaries = df['summaries'].iloc[0]
            print(f\"  ìš”ì•½ë¬¸ ê°œìˆ˜: {len(sample_summaries)}\")
            print(f\"  ì²« ë²ˆì§¸ ìš”ì•½: {sample_summaries[0][:50]}...\")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        validator = DataValidator()
        validation_results = validator.validate_dataset(df)
        validator.print_validation_report(validation_results)
        
    except Exception as e:
        print(f\"âŒ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}\")
        import traceback
        traceback.print_exc()
```

### 2. ì„±ëŠ¥ ë¬¸ì œ ë¶„ì„

#### í”„ë¡œíŒŒì¼ë§ ë„êµ¬
```python
def profile_training_step(trainer, data_loader, num_steps: int = 10):
    \"\"\"í•™ìŠµ ë‹¨ê³„ í”„ë¡œíŒŒì¼ë§\"\"\"
    
    import time
    import cProfile
    import pstats
    
    print(f\"ğŸ“Š í•™ìŠµ ë‹¨ê³„ í”„ë¡œíŒŒì¼ë§ ({num_steps} ìŠ¤í…)\")
    
    # í”„ë¡œíŒŒì¼ëŸ¬ ì„¤ì •
    profiler = cProfile.Profile()
    
    # ì‹œì‘ ì‹œê°„
    start_time = time.time()
    
    profiler.enable()
    
    try:
        # ì§€ì •ëœ ìŠ¤í… ìˆ˜ë§Œí¼ í•™ìŠµ
        for i, batch in enumerate(data_loader):
            if i >= num_steps:
                break
            
            # í•œ ìŠ¤í… ì‹¤í–‰
            trainer.training_step(trainer.model, batch)
            
            if i % 5 == 0:
                print(f\"  ìŠ¤í… {i+1}/{num_steps} ì™„ë£Œ\")
    
    finally:
        profiler.disable()
    
    # ê²°ê³¼ ë¶„ì„
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f\"â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ\")
    print(f\"ğŸš€ ìŠ¤í…ë‹¹ í‰ê· : {total_time/num_steps:.2f}ì´ˆ\")
    
    # í”„ë¡œíŒŒì¼ ê²°ê³¼ ì¶œë ¥
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)  # ìƒìœ„ 10ê°œ í•¨ìˆ˜
```

---

## ì½”ë”© í‘œì¤€ ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ì½”ë“œ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ

#### í•¨ìˆ˜ ì‘ì„± í‘œì¤€
```python
# âœ… ê¶Œì¥ íŒ¨í„´
def process_dialogue_data(input_path: Union[str, Path], 
                         output_path: Union[str, Path],
                         batch_size: int = 16,
                         device: str = \"auto\") -> int:
    \"\"\"
    ëŒ€í™” ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        input_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ) 
        batch_size: ë°°ì¹˜ í¬ê¸°
        device: ì²˜ë¦¬ ë””ë°”ì´ìŠ¤ (\"auto\", \"cuda\", \"mps\", \"cpu\")
    
    Returns:
        int: ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜
        
    Raises:
        ValueError: ê²½ë¡œê°€ ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš°
        FileNotFoundError: ì…ë ¥ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
        
    Example:
        >>> count = process_dialogue_data(\"data/train.csv\", \"outputs/processed.csv\")
        >>> print(f\"ì²˜ë¦¬ëœ ìƒ˜í”Œ: {count}\")
    \"\"\"
    # 1. ì…ë ¥ ê²€ì¦
    if isinstance(input_path, str):
        input_path = Path(input_path)
    if isinstance(output_path, str):
        output_path = Path(output_path)
    
    # 2. ì ˆëŒ€ ê²½ë¡œ ê¸ˆì§€
    if input_path.is_absolute() or output_path.is_absolute():
        raise ValueError(\"ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\")
    
    # core/ensemble/ensemble_model.py (ì•™ìƒë¸” ê°€ì´ë“œ)
    class EnsemblePredictor:
        """ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ì˜ˆì¸¡ê¸°"""
        
        def __init__(self, model_configs: List[dict]):
            """
            Args:
                model_configs: ì•™ìƒë¸”í•  ëª¨ë¸ ì„¤ì • ë¦¬ìŠ¤íŠ¸
                              [{'path': 'model1_path', 'weight': 0.4}, 
                               {'path': 'model2_path', 'weight': 0.6}]
            """
            self.models = []
            self.weights = []
            
            for config in model_configs:
                # ëª¨ë¸ ë¡œë”©
                model = DialogueSummarizationModel.load_from_checkpoint(config['path'])
                weight = config.get('weight', 1.0)
                
                self.models.append(model)
                self.weights.append(weight)
            
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            total_weight = sum(self.weights)
            self.weights = [w / total_weight for w in self.weights]
            
            print(f"ğŸ”— ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {len(self.models)}ê°œ ëª¨ë¸")
        
        def predict_ensemble(self, dialogue: str, strategy: str = "weighted_vote") -> str:
            """ì•™ìƒë¸” ì˜ˆì¸¡"""
            
            # ê° ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ ìƒì„±
            predictions = []
            for model in self.models:
                pred = model.generate_summary(dialogue)
                predictions.append(pred)
            
            # ì•™ìƒë¸” ì „ëµì— ë”°ë¥¸ ê²°í•©
            if strategy == "weighted_vote":
                # ê°€ì¤‘ í‰ê·  ê¸°ë°˜ (ë‹¨ìˆœí™”)
                return self._weighted_average(predictions)
            elif strategy == "longest":
                # ê°€ì¥ ê¸´ ì˜ˆì¸¡ ì„ íƒ
                return max(predictions, key=len)
            elif strategy == "shortest":
                # ê°€ì¥ ì§§ì€ ì˜ˆì¸¡ ì„ íƒ
                return min(predictions, key=len)
            else:
                # ì²« ë²ˆì§¸ ëª¨ë¸ ê²°ê³¼
                return predictions[0]
        
        def _weighted_average(self, predictions: List[str]) -> str:
            """ê°€ì¤‘ í‰ê·  ê¸°ë°˜ ì•™ìƒë¸” (ë‹¨ìˆœí™”)"""
            
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í† í° ë ˆë²¨ì—ì„œ í™•ë¥  ê²°í•©
            # ì—¬ê¸°ì„œëŠ” ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì‚¬ìš©
            best_idx = self.weights.index(max(self.weights))
            return predictions[best_idx]
        
        def predict_batch_ensemble(self, dialogues: List[str]) -> List[str]:
            """ë°°ì¹˜ ì•™ìƒë¸” ì˜ˆì¸¡"""
            
            results = []
            for dialogue in dialogues:
                ensemble_pred = self.predict_ensemble(dialogue)
                results.append(ensemble_pred)
            
            return results
    
    # ì‚¬ìš© ì˜ˆì‹œ
    model_configs = [
        {'path': 'outputs/kobart_model', 'weight': 0.4},
        {'path': 'outputs/kt5_model', 'weight': 0.6}
    ]
    
    ensemble = EnsemblePredictor(model_configs)
    prediction = ensemble.predict_ensemble("í™”ì1: ì•ˆë…•í•˜ì„¸ìš”\ní™”ì2: ì•ˆë…•í•˜ì„¸ìš”")
    ```
    
    ### 4. ê³ ê¸‰ ë°ì´í„° ì¦ê°•
    
    #### ëŒ€í™” ë°ì´í„° ì¦ê°• ê¸°ë²•
    ```python
    # utils/data_augmentation.py (ë°ì´í„° ì¦ê°•)
    class DialogueAugmenter:
        """ëŒ€í™” ë°ì´í„° ì¦ê°• í´ë˜ìŠ¤"""
        
        def __init__(self, augmentation_ratio: float = 0.2):
            """
            Args:
                augmentation_ratio: ì¦ê°•í•  ë°ì´í„° ë¹„ìœ¨
            """
            self.augmentation_ratio = augmentation_ratio
            
            # ë™ì˜ì–´ ì‚¬ì „ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            self.synonyms = {
                'ì•ˆë…•í•˜ì„¸ìš”': ['ì•ˆë…•', 'ë°˜ê°‘ìŠµë‹ˆë‹¤', 'ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ'],
                'ê°ì‚¬í•©ë‹ˆë‹¤': ['ê³ ë§™ìŠµë‹ˆë‹¤', 'ê³ ë§ˆì›Œìš”', 'ê°ì‚¬í•´ìš”'],
                'ë„¤': ['ì˜ˆ', 'ê·¸ë ‡ìŠµë‹ˆë‹¤', 'ë§ìŠµë‹ˆë‹¤'],
                'ì•„ë‹ˆìš”': ['ì•„ë‹™ë‹ˆë‹¤', 'ì•„ë‹ˆì—ìš”', 'ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤']
            }
        
        def augment_dataset(self, dialogues: List[str], summaries: List[str]) -> Tuple[List[str], List[str]]:
            """ë°ì´í„°ì…‹ ì¦ê°•"""
            
            augmented_dialogues = dialogues.copy()
            augmented_summaries = summaries.copy()
            
            num_to_augment = int(len(dialogues) * self.augmentation_ratio)
            
            # ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ ì¦ê°•
            import random
            indices = random.sample(range(len(dialogues)), num_to_augment)
            
            for idx in indices:
                original_dialogue = dialogues[idx]
                original_summary = summaries[idx]
                
                # ë‹¤ì–‘í•œ ì¦ê°• ê¸°ë²• ì ìš©
                aug_techniques = [
                    self._synonym_replacement,
                    self._speaker_permutation,
                    self._sentence_reordering
                ]
                
                for technique in aug_techniques:
                    try:
                        aug_dialogue = technique(original_dialogue)
                        if aug_dialogue != original_dialogue:
                            augmented_dialogues.append(aug_dialogue)
                            augmented_summaries.append(original_summary)
                    except Exception as e:
                        print(f"âš ï¸ ì¦ê°• ì‹¤íŒ¨: {e}")
                        continue
            
            print(f"ğŸ”„ ë°ì´í„° ì¦ê°• ì™„ë£Œ: {len(dialogues)} â†’ {len(augmented_dialogues)}")
            return augmented_dialogues, augmented_summaries
        
        def _synonym_replacement(self, dialogue: str) -> str:
            """ë™ì˜ì–´ ì¹˜í™˜"""
            
            augmented = dialogue
            
            for original, synonyms in self.synonyms.items():
                if original in augmented:
                    import random
                    synonym = random.choice(synonyms)
                    augmented = augmented.replace(original, synonym, 1)  # ì²« ë²ˆì§¸ë§Œ ì¹˜í™˜
            
            return augmented
        
        def _speaker_permutation(self, dialogue: str) -> str:
            """í™”ì ìˆœì„œ ë³€ê²½ (2ëª… ëŒ€í™”ì¸ ê²½ìš°)"""
            
            lines = dialogue.split('\n')
            
            # í™”ì1ê³¼ í™”ì2 êµì²´
            augmented_lines = []
            for line in lines:
                if line.startswith('í™”ì1:'):
                    augmented_lines.append(line.replace('í™”ì1:', 'í™”ì2:'))
                elif line.startswith('í™”ì2:'):
                    augmented_lines.append(line.replace('í™”ì2:', 'í™”ì1:'))
                else:
                    augmented_lines.append(line)
            
            return '\n'.join(augmented_lines)
        
        def _sentence_reordering(self, dialogue: str) -> str:
            """ë¬¸ì¥ ìˆœì„œ ì¬ë°°ì¹˜ (ì£¼ì˜: ì˜ë¯¸ ë³€ê²½ ê°€ëŠ¥)"""
            
            lines = dialogue.split('\n')
            
            if len(lines) <= 2:
                return dialogue  # ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ
            
            # ì¸ì ‘í•œ ë¬¸ì¥ 2ê°œì”© ìˆœì„œ ë°”ê¾¸ê¸°
            import random
            if len(lines) >= 4 and random.random() < 0.5:
                # ì¤‘ê°„ 2ê°œ ë¬¸ì¥ ìˆœì„œ ë°”ê¾¸ê¸°
                mid = len(lines) // 2
                if mid > 0 and mid < len(lines) - 1:
                    lines[mid], lines[mid + 1] = lines[mid + 1], lines[mid]
            
            return '\n'.join(lines)
    ```
    
    ### 5. ëª¨ë¸ í•´ì„ ë° ë¶„ì„
    
    #### ì–´í…ì…˜ ì‹œê°í™”
    ```python
    # utils/model_interpretation.py (ëª¨ë¸ í•´ì„)
    class AttentionVisualizer:
        """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
        
        def __init__(self, model, tokenizer):
            self.model = model
            self.tokenizer = tokenizer
        
        def visualize_attention(self, dialogue: str, summary: str, save_path: str = None):
            """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(dialogue, return_tensors="pt")
            
            # ëª¨ë¸ ì‹¤í–‰ (ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í¬í•¨)
            with torch.no_grad():
                outputs = self.model(**inputs, output_attentions=True)
            
            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
            attention_weights = outputs.attentions[-1]  # ë§ˆì§€ë§‰ ë ˆì´ì–´
            
            # ì‹œê°í™”
            self._plot_attention_heatmap(
                attention_weights[0].mean(dim=0),  # í—¤ë“œ í‰ê· 
                self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]),
                save_path
            )
        
        def _plot_attention_heatmap(self, attention_matrix, tokens, save_path):
            """ì–´í…ì…˜ íˆíŠ¸ë§µ ìƒì„±"""
            
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            plt.figure(figsize=(12, 8))
            
            # íˆíŠ¸ë§µ ìƒì„±
            sns.heatmap(
                attention_matrix.cpu().numpy(),
                xticklabels=tokens,
                yticklabels=tokens,
                cmap='Blues',
                cbar=True
            )
            
            plt.title('Attention Weights Heatmap')
            plt.xlabel('Input Tokens')
            plt.ylabel('Output Tokens')
            plt.xticks(rotation=45)
            plt.yticks(rotation=0)
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"ğŸ“Š ì–´í…ì…˜ ì‹œê°í™” ì €ì¥: {save_path}")
            
            plt.show()
        
        def analyze_token_importance(self, dialogue: str) -> dict:
            """í† í° ì¤‘ìš”ë„ ë¶„ì„"""
            
            inputs = self.tokenizer(dialogue, return_tensors="pt")
            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
            
            with torch.no_grad():
                outputs = self.model(**inputs, output_attentions=True)
            
            # ëª¨ë“  ë ˆì´ì–´ì˜ ì–´í…ì…˜ í‰ê· 
            all_attention = torch.stack(outputs.attentions)
            avg_attention = all_attention.mean(dim=(0, 2))  # ë ˆì´ì–´, í—¤ë“œ í‰ê· 
            
            # ê° í† í°ì˜ ì¤‘ìš”ë„ (ë°›ì€ ì–´í…ì…˜ì˜ í•©)
            token_importance = avg_attention.sum(dim=0).cpu().numpy()
            
            # í† í°-ì¤‘ìš”ë„ ë§¤í•‘
            importance_dict = {
                token: float(importance) 
                for token, importance in zip(tokens, token_importance)
            }
            
            # ì¤‘ìš”ë„ ìˆœ ì •ë ¬
            sorted_importance = sorted(
                importance_dict.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
            
            return {
                'token_importance': importance_dict,
                'top_tokens': sorted_importance[:10],
                'avg_importance': float(token_importance.mean())
            }
    ```
    
    ---
    
    ## ì‹¤ì „ ê°œë°œ ì›Œí¬í”Œë¡œìš°
    
    ### 1. í”„ë¡œì íŠ¸ ì‹œì‘ ì²´í¬ë¦¬ìŠ¤íŠ¸
    
    #### ì´ˆê¸° ì„¤ì • (30ë¶„)
    ```bash
    # 1. í™˜ê²½ í™•ì¸
    python --version  # 3.8+
    git --version
    
    # 2. ê°€ìƒí™˜ê²½ ìƒì„±
    python -m venv venv
    source venv/bin/activate
    
    # 3. ì˜ì¡´ì„± ì„¤ì¹˜
    pip install -r requirements.txt
    
    # 4. ë””ë°”ì´ìŠ¤ í™•ì¸
    python -c "from utils.device_utils import get_optimal_device; print(get_optimal_device())"
    
    # 5. ë°ì´í„° ê²€ì¦
    python validate_data.py
    ```
    
    #### ê°œë°œ í™˜ê²½ ê²€ì¦
    ```python
    # quick_test.py
    if __name__ == "__main__":
        # í•µì‹¬ ëª¨ë“ˆ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
        try:
            from utils.data_utils import DataProcessor
            from utils.metrics import RougeCalculator
            from utils.device_utils import get_optimal_device
            
            print("âœ… ëª¨ë“  ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
            
            # ë””ë°”ì´ìŠ¤ ê°ì§€
            device = get_optimal_device()
            print(f"âœ… ê°ì§€ëœ ë””ë°”ì´ìŠ¤: {device}")
            
            # ê°„ë‹¨í•œ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
            processor = DataProcessor()
            print("âœ… DataProcessor ì´ˆê¸°í™” ì„±ê³µ")
            
            print("ğŸ‰ ê°œë°œ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ í™˜ê²½ ì„¤ì • ë¬¸ì œ: {e}")
            exit(1)
    ```
    
    ### 2. ì¼ì¼ ê°œë°œ ë£¨í‹´
    
    #### ì˜¤ì „ (ëª¨ë¸ ì‹¤í—˜)
    ```python
    # daily_experiment.py
    from utils.experiment_utils import ExperimentTracker
    from datetime import datetime
    
    def run_daily_experiment():
        """ì¼ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        
        tracker = ExperimentTracker()
        
        # ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì‹¤í—˜ëª… ìƒì„±
        today = datetime.now().strftime("%Y%m%d")
        experiment_name = f"daily_experiment_{today}"
        
        # ì‹¤í—˜ ì„¤ì •
        config = {
            "model": "kobart",
            "learning_rate": 5e-5,
            "batch_size": 16,
            "epochs": 3
        }
        
        # ì‹¤í—˜ ì‹œì‘
        exp_id = tracker.start_experiment(
            name=experiment_name,
            description=f"{today} ì¼ì¼ ì‹¤í—˜",
            config=config
        )
        
        try:
            # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰
            results = run_training(config)
            
            # ìµœì¢… ê²°ê³¼ ê¸°ë¡
            tracker.end_experiment(results, "completed")
            
            print(f"âœ… {experiment_name} ì™„ë£Œ")
            return results
            
        except Exception as e:
            tracker.end_experiment({"error": str(e)}, "failed")
            print(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
            return None
    
    if __name__ == "__main__":
        run_daily_experiment()
    ```
    
    #### ì˜¤í›„ (ì„±ëŠ¥ ë¶„ì„)
    ```python
    # analyze_results.py
    from utils.experiment_utils import ExperimentTracker
    from utils.model_registry import ModelRegistry
    
    def analyze_daily_progress():
        """ì¼ì¼ ì§„í–‰ ìƒí™© ë¶„ì„"""
        
        tracker = ExperimentTracker()
        registry = ModelRegistry()
        
        # ì‹¤í—˜ ìš”ì•½
        exp_summary = tracker.get_experiment_summary()
        print("ğŸ“Š ì‹¤í—˜ ìš”ì•½:")
        print(exp_summary.head())
        
        # ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„
        model_summary = registry.get_models_summary()
        print("\nğŸ† ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„:")
        print(model_summary.head())
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = registry.get_best_model()
        if best_model:
            print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
            print(f"  ì´ë¦„: {best_model['name']}")
            print(f"  ì„±ëŠ¥: {best_model['performance']['rouge_combined_f1']:.4f}")
        
        return exp_summary, model_summary
    
    if __name__ == "__main__":
        analyze_daily_progress()
    ```
    
    ### 3. ì£¼ê°„ ë¦¬ë·° ë° ê³„íš
    
    #### ì£¼ê°„ ì„±ê³¼ ë¦¬í¬íŠ¸
    ```python
    # weekly_report.py
    import pandas as pd
    from datetime import datetime, timedelta
    
    def generate_weekly_report():
        """ì£¼ê°„ ì„±ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        tracker = ExperimentTracker()
        registry = ModelRegistry()
        
        # ì´ë²ˆ ì£¼ ì‹¤í—˜ í•„í„°ë§
        week_ago = datetime.now() - timedelta(days=7)
        exp_summary = tracker.get_experiment_summary()
        
        if not exp_summary.empty:
            exp_summary['start_time'] = pd.to_datetime(exp_summary['start_time'])
            this_week = exp_summary[exp_summary['start_time'] > week_ago]
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = {
                "ì£¼ê°„ ì‹¤í—˜ ìˆ˜": len(this_week),
                "ì„±ê³µí•œ ì‹¤í—˜": len(this_week[this_week['status'] == 'completed']),
                "ìµœê³  ì„±ëŠ¥": this_week['best_rouge_combined_f1'].max(),
                "í‰ê·  ì„±ëŠ¥": this_week['best_rouge_combined_f1'].mean(),
                "ì‚¬ìš©ëœ ë””ë°”ì´ìŠ¤": this_week['device'].value_counts().to_dict()
            }
            
            # ë¦¬í¬íŠ¸ ì¶œë ¥
            print("ğŸ“ˆ ì£¼ê°„ ì„±ê³¼ ë¦¬í¬íŠ¸")
            print("=" * 50)
            for key, value in report.items():
                if isinstance(value, float):
                    print(f"{key}: {value:.4f}")
                else:
                    print(f"{key}: {value}")
            
            # ë‹¤ìŒ ì£¼ ê³„íš
            print("\nğŸ“‹ ë‹¤ìŒ ì£¼ ê³„íš:")
            print("1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
            print("2. ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‹¤í—˜")
            print("3. ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©")
            
            return report
        else:
            print("âš ï¸ ì´ë²ˆ ì£¼ ì‹¤í—˜ ë°ì´í„° ì—†ìŒ")
            return {}
    
    if __name__ == "__main__":
        generate_weekly_report()
    ```
    
    ---
    
    ## ë§ˆë¬´ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„
    
    ### ê°œë°œ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸
    
    #### ê¸°ëŠ¥ ì™„ì„±ë„ í™•ì¸
    - [ ] **ë°ì´í„° ì²˜ë¦¬**: Multi-reference í˜•ì‹ ì™„ì „ ì§€ì›
    - [ ] **ëª¨ë¸ í•™ìŠµ**: ëª¨ë“  ì§€ì› ì•„í‚¤í…ì²˜ ì •ìƒ ì‘ë™
    - [ ] **ì„±ëŠ¥ í‰ê°€**: ROUGE ë©”íŠ¸ë¦­ ì •í™•í•œ ê³„ì‚°
    - [ ] **ì‹¤í—˜ ê´€ë¦¬**: ì²´ê³„ì ì¸ ì¶”ì  ë° ë¶„ì„
    - [ ] **ì¶”ë¡  ì‹œìŠ¤í…œ**: ë°°ì¹˜ ì²˜ë¦¬ ë° ì œì¶œ í˜•ì‹ ì§€ì›
    - [ ] **í¬ë¡œìŠ¤ í”Œë«í¼**: Mac MPS / Ubuntu CUDA í˜¸í™˜
    
    #### ì½”ë“œ í’ˆì§ˆ í™•ì¸
    - [ ] **ìƒëŒ€ ê²½ë¡œ**: ëª¨ë“  ê²½ë¡œê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€
    - [ ] **ì—ëŸ¬ ì²˜ë¦¬**: í¬ê´„ì ì¸ ì˜ˆì™¸ ì²˜ë¦¬ êµ¬í˜„
    - [ ] **ë¬¸ì„œí™”**: í•¨ìˆ˜/í´ë˜ìŠ¤ docstring ì™„ì„±
    - [ ] **íƒ€ì… íŒíŠ¸**: ëª¨ë“  í•¨ìˆ˜ì— íƒ€ì… annotation
    - [ ] **í…ŒìŠ¤íŠ¸**: í•µì‹¬ ê¸°ëŠ¥ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
    
    #### ì„±ëŠ¥ ìµœì í™”
    - [ ] **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
    - [ ] **ì†ë„ ìµœì í™”**: ë°ì´í„° ë¡œë” ë° ì¶”ë¡  ì†ë„
    - [ ] **ë””ë°”ì´ìŠ¤ í™œìš©**: GPU/MPS ìµœì í™” ì„¤ì •
    - [ ] **ìºì‹±**: ì¤‘ë³µ ê³„ì‚° ë°©ì§€
    
    ### í–¥í›„ ê°œì„  ë°©í–¥
    
    #### ë‹¨ê¸° ëª©í‘œ (1-2ì£¼)
    1. **ì„±ëŠ¥ í–¥ìƒ**
       - ë” ì •êµí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
       - ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©
       - ì•™ìƒë¸” ëª¨ë¸ ì‹¤í—˜
    
    2. **ì‚¬ìš©ì„± ê°œì„ **
       - ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ
       - API ì„œë²„ êµ¬ì¶•
       - ìë™ ë°°í¬ íŒŒì´í”„ë¼ì¸
    
    #### ì¤‘ê¸° ëª©í‘œ (1ê°œì›”)
    1. **ê³ ê¸‰ ê¸°ëŠ¥**
       - ì‹¤ì‹œê°„ í•™ìŠµ ëª¨ë‹ˆí„°ë§
       - A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
       - ìë™ ëª¨ë¸ ì„ íƒ
    
    2. **í™•ì¥ì„±**
       - ë¶„ì‚° í•™ìŠµ ì§€ì›
       - í´ë¼ìš°ë“œ ë°°í¬
       - ìŠ¤ì¼€ì¼ë§ ìë™í™”
    
    #### ì¥ê¸° ëª©í‘œ (3ê°œì›”+)
    1. **ì—°êµ¬ ë°©í–¥**
       - ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ì‹¤í—˜
       - ë©€í‹°ëª¨ë‹¬ í™•ì¥
       - ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸
    
    2. **í”„ë¡œë•ì…˜í™”**
       - ìš´ì˜ ëª¨ë‹ˆí„°ë§
       - ì„±ëŠ¥ ìµœì í™”
       - ìœ ì§€ë³´ìˆ˜ ìë™í™”
    
    ---
    
    ## ê°œë°œ ë¦¬ì†ŒìŠ¤
    
    ### ì¶”ì²œ ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
    
    #### ê°œë°œ ë„êµ¬
    - **IDE**: VS Code + Python í™•ì¥íŒ©
    - **ë””ë²„ê¹…**: IPython, pdb
    - **í”„ë¡œíŒŒì¼ë§**: cProfile, memory_profiler
    - **ë²„ì „ ê´€ë¦¬**: Git + DVC (ë°ì´í„° ë²„ì „ ê´€ë¦¬)
    
    #### ëª¨ë‹ˆí„°ë§ ë„êµ¬
    - **ì‹¤í—˜ ì¶”ì **: Weights & Biases, MLflow
    - **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: TensorBoard, Prometheus
    - **ë¡œê·¸ ë¶„ì„**: Elasticsearch, Grafana
    
    #### ë°°í¬ ë„êµ¬
    - **ì»¨í…Œì´ë„ˆí™”**: Docker, Kubernetes
    - **CI/CD**: GitHub Actions, Jenkins
    - **ì¸í”„ë¼**: AWS, GCP, Azure
    
    ### í•™ìŠµ ìë£Œ
    
    #### ê³µì‹ ë¬¸ì„œ
    - [Transformers Documentation](https://huggingface.co/docs/transformers)
    - [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
    - [Weights & Biases Guides](https://docs.wandb.ai)
    
    #### ì¶”ì²œ ë…¼ë¬¸
    - BART: Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-training"
    - ROUGE: Lin, "ROUGE: A Package for Automatic Evaluation of Summaries"
    - T5: Raffel et al., "Exploring the Limits of Transfer Learning"
    
    #### ì»¤ë®¤ë‹ˆí‹°
    - [Hugging Face Community](https://huggingface.co/community)
    - [PyTorch Forums](https://discuss.pytorch.org)
    - [Papers with Code](https://paperswithcode.com)
    
    ---
    
    ì´ ì¢…í•© ê°œë°œ ê°€ì´ë“œë¥¼ í†µí•´ NLP ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œì„ íš¨ê³¼ì ìœ¼ë¡œ ê°œë°œí•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ì„¹ì…˜ì˜ ì½”ë“œì™€ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¼ ë‹¨ê³„ë³„ë¡œ êµ¬í˜„í•˜ë©´ì„œ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ì„±í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
    
    **í•µì‹¬ ì›ì¹™ì„ í•­ìƒ ê¸°ì–µí•˜ì„¸ìš”:**
    - ğŸ“‚ **ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©**: í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„±
    - ğŸ¯ **ì‹¤í—˜ ì¤‘ì‹¬**: ëª¨ë“  ë³€ê²½ì‚¬í•­ì„ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì 
    - âš¡ **ì„±ëŠ¥ ìµœì í™”**: ë©”ëª¨ë¦¬ì™€ ì†ë„ íš¨ìœ¨ì„± ê³ ë ¤
    - ğŸ¤ **íŒ€ í˜‘ì—…**: ëª…í™•í•œ ì½”ë”© í‘œì¤€ê³¼ ë¬¸ì„œí™”
    - ğŸ”„ **ì§€ì†ì  ê°œì„ **: ì‘ì€ ë‹¨ìœ„ì˜ ë°˜ë³µì  ê°œë°œ
    
    # 4. ì‹¤ì œ ì²˜ë¦¬
    # ... êµ¬í˜„ ì½”ë“œ ...
    
    return processed_count
```

#### í´ë˜ìŠ¤ ì„¤ê³„ íŒ¨í„´
```python
# âœ… ê¶Œì¥ í´ë˜ìŠ¤ êµ¬ì¡°
class DialogueProcessor:
    \"\"\"ëŒ€í™” ì²˜ë¦¬ í´ë˜ìŠ¤\"\"\"
    
    def __init__(self, config_path: Union[str, Path]):
        \"\"\"
        ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
        \"\"\"
        # 1. ê²½ë¡œ ê²€ì¦
        if isinstance(config_path, str):
            config_path = Path(config_path)
        
        if config_path.is_absolute():
            raise ValueError(f\"ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”: {config_path}\")
        
        # 2. ì„¤ì • ë¡œë”©
        self.config = self._load_config(config_path)
        
        # 3. ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = get_optimal_device()
        
        # 4. ë‚´ë¶€ ìƒíƒœ ì´ˆê¸°í™”
        self._initialize_components()
    
    def _load_config(self, config_path: Path) -> dict:
        \"\"\"ì„¤ì • íŒŒì¼ ë¡œë”© (private ë©”ì„œë“œ)\"\"\"
        # ... êµ¬í˜„ ...
        pass
    
    def _initialize_components(self):
        \"\"\"ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (private ë©”ì„œë“œ)\"\"\"
        # ... êµ¬í˜„ ...
        pass
    
    def process(self, input_data: Any) -> Any:
        \"\"\"ì£¼ìš” ì²˜ë¦¬ í•¨ìˆ˜ (public ë©”ì„œë“œ)\"\"\"
        # ... êµ¬í˜„ ...
        pass
```

### 2. ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´

#### í¬ê´„ì ì¸ ì—ëŸ¬ ì²˜ë¦¬
```python
def robust_model_training(config_path: str, 
                         train_data_path: str,
                         val_data_path: str) -> dict:
    \"\"\"ê²¬ê³ í•œ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜\"\"\"
    
    try:
        # 1. ì…ë ¥ ê²€ì¦
        if not all([config_path, train_data_path, val_data_path]):
            raise ValueError(\"ëª¨ë“  ê²½ë¡œ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤\")
        
        # 2. ì„¤ì • ë¡œë”©
        config = load_config(config_path)
        
        # 3. ë°ì´í„° ë¡œë”©
        train_data = load_training_data(train_data_path)
        val_data = load_validation_data(val_data_path)
        
        # 4. ëª¨ë¸ ì´ˆê¸°í™”
        model = initialize_model(config)
        
        # 5. í•™ìŠµ ì‹¤í–‰
        results = train_model(model, train_data, val_data, config)
        
        return {\"status\": \"success\", \"results\": results}
        
    except ValueError as e:
        print(f\"âŒ ì…ë ¥ ì˜¤ë¥˜: {e}\")
        return {\"status\": \"error\", \"error_type\": \"ValueError\", \"message\": str(e)}
        
    except FileNotFoundError as e:
        print(f\"âŒ íŒŒì¼ ì—†ìŒ: {e}\")
        return {\"status\": \"error\", \"error_type\": \"FileNotFoundError\", \"message\": str(e)}
        
    except torch.cuda.OutOfMemoryError as e:
        print(f\"âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}\")
        torch.cuda.empty_cache()
        return {\"status\": \"error\", \"error_type\": \"OutOfMemoryError\", \"message\": \"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±\"}
        
    except Exception as e:
        print(f\"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}\")
        import traceback
        traceback.print_exc()
        return {\"status\": \"error\", \"error_type\": \"UnknownError\", \"message\": str(e)}
```

### 3. ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

#### êµ¬ì¡°í™”ëœ ë¡œê¹…
```python
import logging
import json
from datetime import datetime

def setup_structured_logging(log_file: str = \"logs/development.log\"):
    \"\"\"êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •\"\"\"
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(log_file).parent.mkdir(parents=True, exist_ok=True)
    
    # ì»¤ìŠ¤í…€ í¬ë§·í„°
    class StructuredFormatter(logging.Formatter):
        def format(self, record):
            log_entry = {
                \"timestamp\": datetime.utcnow().isoformat(),
                \"level\": record.levelname,
                \"module\": record.module,
                \"function\": record.funcName,
                \"line\": record.lineno,
                \"message\": record.getMessage()
            }
            
            # ì¶”ê°€ ì†ì„±ì´ ìˆìœ¼ë©´ í¬í•¨
            if hasattr(record, 'extra_data'):
                log_entry.update(record.extra_data)
            
            return json.dumps(log_entry, ensure_ascii=False)
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger(\"nlp_development\")
    logger.setLevel(logging.INFO)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(StructuredFormatter())
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    ))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ì‚¬ìš© ì˜ˆì‹œ
logger = setup_structured_logging()

def log_experiment_start(experiment_name: str, config: dict):
    \"\"\"ì‹¤í—˜ ì‹œì‘ ë¡œê¹…\"\"\"
    logger.info(
        f\"ì‹¤í—˜ ì‹œì‘: {experiment_name}\",
        extra={'extra_data': {
            'experiment_name': experiment_name,
            'config': config,
            'device': get_optimal_device()
        }}
    )
```

---

## ê³ ê¸‰ ê°œë°œ ê¸°ë²•

### 1. ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ê°œë°œ

#### ROUGE í™•ì¥ ë©”íŠ¸ë¦­
```python
# utils/custom_metrics.py (ê³ ê¸‰ ë©”íŠ¸ë¦­)
class AdvancedRougeCalculator(RougeCalculator):
    \"\"\"í™•ì¥ëœ ROUGE ê³„ì‚°ê¸°\"\"\"
    
    def __init__(self, use_korean_tokenizer: bool = True):
        super().__init__(use_korean_tokenizer)
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        self.custom_metrics = {}
    
    def compute_semantic_similarity(self, 
                                  predictions: List[str],
                                  references: List[str]) -> float:
        \"\"\"ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (ì˜ˆ: BERT Score ê¸°ë°˜)\"\"\"
        
        # ê°„ë‹¨í•œ êµ¬í˜„ ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” BERT Score ì‚¬ìš©)
        similarities = []
        
        for pred, ref in zip(predictions, references):
            # ë‹¨ì–´ ê²¹ì¹˜ëŠ” ì •ë„ ê¸°ë°˜ ê°„ë‹¨í•œ ìœ ì‚¬ë„
            pred_words = set(pred.split())
            ref_words = set(ref.split())
            
            if len(ref_words) == 0:
                similarity = 0.0
            else:
                intersection = len(pred_words & ref_words)
                similarity = intersection / len(ref_words)
            
            similarities.append(similarity)
        
        return sum(similarities) / len(similarities)
    
    def compute_comprehensive_metrics(self, 
                                    predictions: List[str],
                                    references_list: List[List[str]]) -> dict:
        \"\"\"ì¢…í•© ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"
        
        # ê¸°ë³¸ ROUGE ê³„ì‚°
        rouge_scores = self.compute_multi_reference_rouge(predictions, references_list)
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­
        additional_metrics = {}
        
        # ì²« ë²ˆì§¸ ì°¸ì¡°ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
        if references_list:
            first_refs = [refs[0] for refs in references_list if refs]
            semantic_sim = self.compute_semantic_similarity(predictions, first_refs)
            additional_metrics['semantic_similarity'] = semantic_sim
        
        # ìš”ì•½ë¬¸ ê¸¸ì´ ë¶„ì„
        pred_lengths = [len(p.split()) for p in predictions]
        additional_metrics['avg_summary_length'] = sum(pred_lengths) / len(pred_lengths)
        
        # ê²°ê³¼ ê²°í•©
        return {
            **rouge_scores,
            \"additional_metrics\": additional_metrics
        }
```

### 2. ë™ì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

#### ë² ì´ì§€ì•ˆ ìµœì í™” ê¸°ë°˜ íŠœë‹
```python
# utils/hyperparameter_tuning.py (ê³ ê¸‰ íŠœë‹)
class BayesianOptimizer:
    \"\"\"ë² ì´ì§€ì•ˆ ìµœì í™” ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\"\"\"
    
    def __init__(self, parameter_space: dict, objective_metric: str = \"rouge_combined_f1\"):
        \"\"\"
        Args:
            parameter_space: íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ê³µê°„
            objective_metric: ìµœì í™”í•  ë©”íŠ¸ë¦­
        \"\"\"
        self.parameter_space = parameter_space
        self.objective_metric = objective_metric
        self.trials = []
    
    def suggest_parameters(self) -> dict:
        \"\"\"ë‹¤ìŒ ì‹œë„í•  íŒŒë¼ë¯¸í„° ì œì•ˆ\"\"\"
        
        if len(self.trials) < 5:
            # ì´ˆê¸° ëœë¤ ìƒ˜í”Œë§
            return self._random_sample()
        else:
            # ë² ì´ì§€ì•ˆ ìµœì í™” (ê°„ë‹¨í•œ êµ¬í˜„)
            return self._bayesian_sample()
    
    def _random_sample(self) -> dict:
        \"\"\"ëœë¤ íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\"\"\"
        import random
        
        params = {}
        for param_name, param_config in self.parameter_space.items():
            if param_config[\"type\"] == \"uniform\":
                params[param_name] = random.uniform(
                    param_config[\"min\"], 
                    param_config[\"max\"]
                )
            elif param_config[\"type\"] == \"choice\":
                params[param_name] = random.choice(param_config[\"values\"])
            elif param_config[\"type\"] == \"log_uniform\":
                import math
                log_min = math.log(param_config[\"min\"])
                log_max = math.log(param_config[\"max\"])
                params[param_name] = math.exp(random.uniform(log_min, log_max))
        
        return params
    
    def _bayesian_sample(self) -> dict:
        \"\"\"ë² ì´ì§€ì•ˆ ìµœì í™” ê¸°ë°˜ ìƒ˜í”Œë§ (ë‹¨ìˆœí™”)\"\"\"
        
        # ê°€ì¥ ì¢‹ì€ ê²°ê³¼ ê¸°ì¤€ìœ¼ë¡œ ì£¼ë³€ íƒìƒ‰
        best_trial = max(self.trials, key=lambda t: t[\"score\"])
        best_params = best_trial[\"parameters\"]
        
        # ë² ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° ì£¼ë³€ì—ì„œ ë³€í˜•
        params = {}
        for param_name, value in best_params.items():
            param_config = self.parameter_space[param_name]
            
            if param_config[\"type\"] == \"uniform\":
                # ë² ìŠ¤íŠ¸ ê°’ ì£¼ë³€ Â±20% ë²”ìœ„ì—ì„œ ìƒ˜í”Œë§
                noise_range = (param_config[\"max\"] - param_config[\"min\"]) * 0.2
                import random
                new_value = value + random.uniform(-noise_range, noise_range)
                params[param_name] = max(param_config[\"min\"], 
                                       min(param_config[\"max\"], new_value))
            else:
                params[param_name] = value
        
        return params
    
    def report_trial(self, parameters: dict, score: float):
        \"\"\"ì‹¤í—˜ ê²°ê³¼ ë³´ê³ \"\"\"
        self.trials.append({
            \"parameters\": parameters,
            \"score\": score,
            \"timestamp\": datetime.now().isoformat()
        })
    
    def get_best_parameters(self) -> dict:
        \"\"\"ìµœì  íŒŒë¼ë¯¸í„° ì¡°íšŒ\"\"\"
        if not self.trials:
            return None
        
        best_trial = max(self.trials, key=lambda t: t[\"score\"])
        return best_trial[\"parameters\"]

# ì‚¬ìš© ì˜ˆì‹œ
parameter_space = {
    \"learning_rate\": {
        \"type\": \"log_uniform\",
        \"min\": 1e-6,
        \"max\": 1e-3
    },
    \"batch_size\": {
        \"type\": \"choice\", 
        \"values\": [8, 16, 32]
    },
    \"warmup_ratio\": {
        \"type\": \"uniform\",
        \"min\": 0.0,
        \"max\": 0.3
    }
}

optimizer = BayesianOptimizer(parameter_space)

# ìµœì í™” ë£¨í”„
for trial in range(20):
    # ë‹¤ìŒ íŒŒë¼ë¯¸í„° ì œì•ˆ
    params = optimizer.suggest_parameters()
    
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    score = train_and_evaluate_model(params)
    
    # ê²°ê³¼ ë³´ê³ 
    optimizer.report_trial(params, score)
    
    print(f\"Trial {trial+1}: Score {score:.4f}, Params {params}\")

# ìµœì  íŒŒë¼ë¯¸í„°
best_params = optimizer.get_best_parameters()
print(f\"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")
```

### 3. ì•™ìƒë¸” ëª¨ë¸ ê°œë°œ

#### ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”
```python
# core/ensemble/ensemble_model.py (ì•™ìƒë¸” ê°€ì´ë“œ)
class EnsemblePredictor:
    \"\"\"ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ì˜ˆì¸¡ê¸°\"\"\"
    
    def __init__(self, model_configs: List[dict]):
        \"\"\"
        Args:
            model_configs
