            cpu_info = analysis['cpu_memory']
            f.write(f"  CPU Peak Usage: {cpu_info['max_mb']:.1f} MB ({cpu_info['peak_usage_pct']:.1f}%)\n")
            f.write(f"  CPU Average Usage: {cpu_info['avg_mb']:.1f} MB\n")
            
            if self.gpu_available:
                gpu_info = analysis['gpu_memory']
                f.write(f"  GPU Peak Usage: {gpu_info['max_mb']:.1f} MB ({gpu_info['peak_usage_pct']:.1f}%)\n")
                f.write(f"  GPU Average Usage: {gpu_info['avg_mb']:.1f} MB\n")
                
                leak_info = gpu_info['memory_leaks']
                f.write(f"  Memory Leak Status: {leak_info['status']}\n")
                if leak_info['status'] == 'leak_detected':
                    f.write(f"  Estimated Leak Rate: {leak_info['estimated_leak_rate']:.2f} MB/step\n")
            
            f.write("\nRecommendations:\n")
            for i, rec in enumerate(analysis['recommendations'], 1):
                f.write(f"  {i}. {rec}\n")
        
        # ê·¸ë˜í”„ ìƒì„±
        if self.save_plots and self.snapshots:
            self._create_memory_plots(output_dir)
        
        print(f"Memory profiling report saved to: {output_dir}")
    
    def _create_memory_plots(self, output_dir: Path):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê·¸ë˜í”„ ìƒì„±"""
        import matplotlib.pyplot as plt
        plt.style.use('default')
        
        # ë°ì´í„° ì¤€ë¹„
        steps = [s.step for s in self.snapshots]
        cpu_memory = [s.cpu_memory_mb for s in self.snapshots]
        gpu_memory = [s.gpu_memory_mb for s in self.snapshots]
        gpu_cached = [s.gpu_memory_cached_mb for s in self.snapshots]
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # CPU ë©”ëª¨ë¦¬ ê·¸ë˜í”„
        ax1.plot(steps, cpu_memory, 'b-', label='CPU Memory Used', linewidth=2)
        ax1.axhline(y=self.cpu_total_memory, color='r', linestyle='--', alpha=0.7, label='CPU Total Memory')
        ax1.set_xlabel('Training Step')
        ax1.set_ylabel('Memory (MB)')
        ax1.set_title('CPU Memory Usage')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # GPU ë©”ëª¨ë¦¬ ê·¸ë˜í”„
        if self.gpu_available:
            ax2.plot(steps, gpu_memory, 'g-', label='GPU Memory Used', linewidth=2)
            ax2.plot(steps, gpu_cached, 'orange', linestyle=':', label='GPU Memory Cached', linewidth=2)
            ax2.axhline(y=self.gpu_total_memory, color='r', linestyle='--', alpha=0.7, label='GPU Total Memory')
            ax2.set_xlabel('Training Step')
            ax2.set_ylabel('Memory (MB)')
            ax2.set_title('GPU Memory Usage')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        else:
            ax2.text(0.5, 0.5, 'GPU Not Available', ha='center', va='center', transform=ax2.transAxes)
            ax2.set_title('GPU Memory Usage')
        
        plt.tight_layout()
        plot_file = output_dir / "memory_usage_plots.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Memory usage plots saved to: {plot_file}")

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬"""
    
    def __init__(self):
        """ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”"""
        self.results = []
        
    def benchmark_model_loading(self, model_path: str) -> Dict[str, float]:
        """ëª¨ë¸ ë¡œë”© ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        from core.inference import InferenceEngine
        
        start_time = time.time()
        engine = InferenceEngine(model_path)
        loading_time = time.time() - start_time
        
        # ëª¨ë¸ í¬ê¸° ê³„ì‚°
        model_size_mb = sum(p.numel() * p.element_size() for p in engine.model.parameters()) / 1024**2
        
        result = {
            'model_path': model_path,
            'loading_time_sec': loading_time,
            'model_size_mb': model_size_mb,
            'loading_speed_mb_per_sec': model_size_mb / loading_time if loading_time > 0 else 0
        }
        
        self.results.append(result)
        return result
    
    def benchmark_inference_speed(self, 
                                 model_path: str,
                                 test_dialogues: List[str],
                                 batch_sizes: List[int] = [1, 4, 8, 16]) -> Dict[str, Any]:
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        from core.inference import InferenceEngine
        
        engine = InferenceEngine(model_path)
        
        benchmark_results = {
            'model_path': model_path,
            'batch_results': []
        }
        
        for batch_size in batch_sizes:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            test_data = test_dialogues[:batch_size * 10]  # 10 ë°°ì¹˜ í…ŒìŠ¤íŠ¸
            
            # ì›Œë°ì—…
            if test_data:
                engine.predict_batch(test_data[:batch_size], show_progress=False)
            
            # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
            start_time = time.time()
            predictions = engine.predict_batch(test_data, batch_size=batch_size, show_progress=False)
            end_time = time.time()
            
            total_time = end_time - start_time
            samples_per_second = len(test_data) / total_time if total_time > 0 else 0
            
            batch_result = {
                'batch_size': batch_size,
                'total_samples': len(test_data),
                'total_time_sec': total_time,
                'samples_per_second': samples_per_second,
                'avg_time_per_sample': total_time / len(test_data) if test_data else 0
            }
            
            benchmark_results['batch_results'].append(batch_result)
            print(f"Batch size {batch_size}: {samples_per_second:.2f} samples/sec")
        
        return benchmark_results
    
    def find_optimal_batch_size(self, 
                               model_path: str,
                               test_dialogues: List[str],
                               max_batch_size: int = 32) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°"""
        profiler = MemoryProfiler()
        
        # ì ì§„ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€
        optimal_batch_size = 1
        
        for batch_size in [1, 2, 4, 8, 16, 32, 64]:
            if batch_size > max_batch_size:
                break
                
            try:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                profiler.take_snapshot(0, f"batch_size_{batch_size}_start")
                
                # ì¶”ë¡  í…ŒìŠ¤íŠ¸
                benchmark = self.benchmark_inference_speed(
                    model_path, test_dialogues, [batch_size]
                )
                
                profiler.take_snapshot(1, f"batch_size_{batch_size}_end")
                
                # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì²´í¬
                if profiler.gpu_available:
                    memory_usage_pct = (torch.cuda.memory_allocated(0) / 
                                      torch.cuda.get_device_properties(0).total_memory) * 100
                    
                    if memory_usage_pct > 85:  # 85% ì´ìƒ ì‚¬ìš© ì‹œ ì¤‘ë‹¨
                        print(f"Memory usage too high ({memory_usage_pct:.1f}%) for batch size {batch_size}")
                        break
                
                optimal_batch_size = batch_size
                print(f"Batch size {batch_size} is feasible")
                
            except Exception as e:
                print(f"Batch size {batch_size} failed: {e}")
                break
        
        print(f"Recommended optimal batch size: {optimal_batch_size}")
        return optimal_batch_size

# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_memory_profile(trainer, duration_steps: int = 100) -> Dict[str, Any]:
    """ë¹ ë¥¸ ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§"""
    profiler = MemoryProfiler()
    profiler.monitor_training_loop(trainer, duration_steps)
    return profiler.analyze_memory_patterns()

def benchmark_model_performance(model_path: str, 
                               test_data: List[str],
                               output_dir: Optional[str] = None) -> Dict[str, Any]:
    """ëª¨ë¸ ì„±ëŠ¥ ì¢…í•© ë²¤ì¹˜ë§ˆí¬"""
    benchmark = PerformanceBenchmark()
    
    # ëª¨ë¸ ë¡œë”© ë²¤ì¹˜ë§ˆí¬
    loading_result = benchmark.benchmark_model_loading(model_path)
    
    # ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
    inference_result = benchmark.benchmark_inference_speed(model_path, test_data)
    
    # ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
    optimal_batch = benchmark.find_optimal_batch_size(model_path, test_data)
    
    # ê²°ê³¼ ì •ë¦¬
    comprehensive_result = {
        'model_loading': loading_result,
        'inference_speed': inference_result,
        'optimal_batch_size': optimal_batch,
        'recommendations': []
    }
    
    # ê¶Œì¥ì‚¬í•­ ìƒì„±
    if loading_result['loading_time_sec'] > 30:
        comprehensive_result['recommendations'].append(
            "Model loading is slow. Consider model quantization or using smaller models for development."
        )
    
    best_throughput = max(
        batch['samples_per_second'] for batch in inference_result['batch_results']
    )
    if best_throughput < 1.0:
        comprehensive_result['recommendations'].append(
            "Low inference throughput. Consider using GPU acceleration or optimizing model architecture."
        )
    
    # ê²°ê³¼ ì €ì¥
    if output_dir:
        output_dir = PathManager.resolve_path(output_dir)
        PathManager.ensure_dir(output_dir)
        
        import json
        result_file = output_dir / "performance_benchmark.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_result, f, ensure_ascii=False, indent=2)
        
        print(f"Benchmark results saved to: {result_file}")
    
    return comprehensive_result
```

### êµ¬í˜„ ìš°ì„ ìˆœìœ„: ğŸŸ¡ ì„ íƒì 
### ì˜ˆìƒ ì‘ì—… ì‹œê°„: 10-14ì‹œê°„
### ê¶Œì¥ ì‹œê¸°: ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•œ ì‹œì 

---

## ğŸŸ¡ 4. ê³ ê¸‰ ì‹¤í—˜ ê¸°ëŠ¥ (Research Enhancement)

### 4.1 Ablation Study ìë™í™”

#### ê°€ì¹˜ ì œì•ˆ
- ëª¨ë¸ êµ¬ì„± ìš”ì†Œë³„ ê¸°ì—¬ë„ ë¶„ì„
- ìµœì  ì„¤ì • ì¡°í•© ë°œê²¬
- ë…¼ë¬¸ ì‘ì„±ìš© ì‹¤í—˜ ë°ì´í„° ì œê³µ

#### êµ¬í˜„ ë°©ì•ˆ
```python
# config/sweep/ablation_study_sweep.yaml ì™„ì„±
name: "Ablation Study - Component Analysis"
method: "grid"  # ëª¨ë“  ì¡°í•© í…ŒìŠ¤íŠ¸
metric:
  name: "rouge_combined_f1"
  goal: "maximize"

parameters:
  # ì „ì²˜ë¦¬ êµ¬ì„±ìš”ì†Œ
  use_special_tokens:
    values: [true, false]
  
  text_cleaning_level:
    values: ["basic", "advanced", "none"]
  
  # ëª¨ë¸ êµ¬ì„±ìš”ì†Œ
  use_attention_dropout:
    values: [true, false]
  
  attention_dropout_rate:
    values: [0.0, 0.1, 0.3]
  
  # í•™ìŠµ ì „ëµ
  use_warmup:
    values: [true, false]
  
  use_weight_decay:
    values: [true, false]
  
  # ìƒì„± ì „ëµ
  generation_strategy:
    values: ["beam_search", "nucleus_sampling", "top_k"]
  
  length_penalty:
    values: [0.8, 1.0, 1.2, 1.5]

# ì‹¤í—˜ ì œì•½ ì¡°ê±´
constraints:
  - name: sampling_constraint
    condition: generation_strategy != "beam_search"
    then:
      num_beams: 1
      do_sample: true
```

### 4.2 ìë™ A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ

#### êµ¬í˜„ ë°©ì•ˆ
```python
# code/experiments/ab_testing.py (ì‹ ê·œ ìƒì„±)
from typing import List, Dict, Any, Tuple
import numpy as np
from scipy import stats
import pandas as pd

class ABTestFramework:
    """A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self, significance_level: float = 0.05):
        self.significance_level = significance_level
        self.experiments = []
    
    def run_ab_test(self, 
                   model_a_results: List[float],
                   model_b_results: List[float],
                   metric_name: str = "rouge_combined") -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        # ê¸°ë³¸ í†µê³„
        stats_a = {
            'mean': np.mean(model_a_results),
            'std': np.std(model_a_results),
            'n': len(model_a_results)
        }
        
        stats_b = {
            'mean': np.mean(model_b_results),
            'std': np.std(model_b_results),
            'n': len(model_b_results)
        }
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì • (t-test)
        t_stat, p_value = stats.ttest_ind(model_a_results, model_b_results)
        
        # íš¨ê³¼ í¬ê¸° (Cohen's d)
        pooled_std = np.sqrt(((stats_a['n']-1) * stats_a['std']**2 + 
                             (stats_b['n']-1) * stats_b['std']**2) / 
                            (stats_a['n'] + stats_b['n'] - 2))
        cohens_d = (stats_b['mean'] - stats_a['mean']) / pooled_std
        
        # ì‹ ë¢°êµ¬ê°„
        se_diff = pooled_std * np.sqrt(1/stats_a['n'] + 1/stats_b['n'])
        mean_diff = stats_b['mean'] - stats_a['mean']
        ci_lower = mean_diff - 1.96 * se_diff
        ci_upper = mean_diff + 1.96 * se_diff
        
        result = {
            'metric_name': metric_name,
            'model_a_stats': stats_a,
            'model_b_stats': stats_b,
            'difference': mean_diff,
            'difference_pct': (mean_diff / stats_a['mean']) * 100,
            'p_value': p_value,
            'is_significant': p_value < self.significance_level,
            'cohens_d': cohens_d,
            'effect_size': self._interpret_effect_size(cohens_d),
            'confidence_interval': (ci_lower, ci_upper),
            'winner': 'Model B' if mean_diff > 0 and p_value < self.significance_level else 
                     'Model A' if mean_diff < 0 and p_value < self.significance_level else 'No significant difference'
        }
        
        return result
    
    def _interpret_effect_size(self, cohens_d: float) -> str:
        """Cohen's d í•´ì„"""
        abs_d = abs(cohens_d)
        if abs_d < 0.2:
            return "negligible"
        elif abs_d < 0.5:
            return "small"
        elif abs_d < 0.8:
            return "medium"
        else:
            return "large"
```

### êµ¬í˜„ ìš°ì„ ìˆœìœ„: ğŸŸ¡ ì„ íƒì 
### ì˜ˆìƒ ì‘ì—… ì‹œê°„: 8-12ì‹œê°„
### ê¶Œì¥ ì‹œê¸°: ì—°êµ¬ ëª©ì ì´ë‚˜ ë…¼ë¬¸ ì‘ì„±ì´ í•„ìš”í•  ë•Œ

---

## ğŸ“‹ ì„ íƒì  ê°œë°œ ì‚¬í•­ ìš°ì„ ìˆœìœ„

| ê¸°ëŠ¥ | ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ | ê¸°ìˆ ì  ë‚œì´ë„ | êµ¬í˜„ ì‹œê°„ | ê¶Œì¥ ì‹œê¸° |
|------|---------------|---------------|-----------|-----------|
| ì•™ìƒë¸” ì‹œìŠ¤í…œ | ğŸ”¥ğŸ”¥ğŸ”¥ ë†’ìŒ | ğŸ”§ğŸ”§ğŸ”§ ë†’ìŒ | 16-20h | ëŒ€íšŒ í›„ë°˜ |
| ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ | ğŸ”¥ğŸ”¥ ì¤‘ê°„ | ğŸ”§ğŸ”§ ì¤‘ê°„ | 12-16h | í”„ë¡œì íŠ¸ ì¤‘ë°˜ |
| ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ | ğŸ”¥ğŸ”¥ ì¤‘ê°„ | ğŸ”§ğŸ”§ ì¤‘ê°„ | 10-14h | ìµœì í™” í•„ìš”ì‹œ |
| Ablation Study | ğŸ”¥ ë‚®ìŒ | ğŸ”§ ë‚®ìŒ | 8-12h | ì—°êµ¬ ëª©ì  |

---

## ğŸ¯ êµ¬í˜„ ì „ëµ

### Phase 1: ê³ íš¨ê³¼ ê¸°ëŠ¥ ìš°ì„  (Week 3-4)
1. **ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ** - ì‹¤í—˜ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
2. **ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§** - ë¦¬ì†ŒìŠ¤ ìµœì í™”

### Phase 2: ì„±ëŠ¥ ê·¹ëŒ€í™” (Week 5-6)
1. **ì•™ìƒë¸” ì‹œìŠ¤í…œ** - ìµœì¢… ì„±ëŠ¥ í–¥ìƒ
2. **ê³ ê¸‰ ìµœì í™” ë„êµ¬**

### Phase 3: ì—°êµ¬ í™•ì¥ (ì„ íƒì )
1. **Ablation Study ìë™í™”**
2. **A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬**

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë¦¬ì†ŒìŠ¤ ê³ ë ¤**: ì„ íƒì  ê¸°ëŠ¥ë“¤ì€ ì¶”ê°€ ê³„ì‚° ë¦¬ì†ŒìŠ¤ í•„ìš”
2. **ë³µì¡ì„± ê´€ë¦¬**: í•µì‹¬ ê¸°ëŠ¥ ì™„ì„± í›„ ì ì§„ì  ì¶”ê°€
3. **ìœ ì§€ë³´ìˆ˜ì„±**: ì„ íƒì  ê¸°ëŠ¥ë„ ë™ì¼í•œ ì½”ë”© í‘œì¤€ ì ìš©
4. **ë¬¸ì„œí™”**: ê³ ê¸‰ ê¸°ëŠ¥ì¼ìˆ˜ë¡ ìƒì„¸í•œ ë¬¸ì„œí™” í•„ìš”

---

## ğŸ“Š ROI ë¶„ì„

### ë†’ì€ ROI
- **ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ**: ì ì€ ë…¸ë ¥ìœ¼ë¡œ í° íš¨ìœ¨ì„± í–¥ìƒ
- **ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§**: ë©”ëª¨ë¦¬ ìµœì í™”ë¡œ ë” í° ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥

### ì¤‘ê°„ ROI  
- **ì•™ìƒë¸” ì‹œìŠ¤í…œ**: ë†’ì€ ì„±ëŠ¥ í–¥ìƒì´ì§€ë§Œ êµ¬í˜„ ë³µì¡

### ë‚®ì€ ROI
- **Ablation Study**: ì—°êµ¬ ëª©ì ì´ ì•„ë‹ˆë©´ íˆ¬ì ëŒ€ë¹„ íš¨ê³¼ ì œí•œ

---

## ğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­

1. **í•„ìˆ˜ êµ¬í˜„ ì™„ë£Œ í›„ ì‹œì‘**: Priority 1 ì‚¬í•­ë“¤ì´ ì™„ì „íˆ êµ¬í˜„ëœ í›„
2. **ë‹¨ê³„ì  ì ‘ê·¼**: í•œ ë²ˆì— ëª¨ë“  ì„ íƒì  ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ì§€ ë§ê³  í•„ìš”ì— ë”°ë¼
3. **ì„±ëŠ¥ ì¸¡ì •**: ê° ê¸°ëŠ¥ ì¶”ê°€ í›„ ì‹¤ì œ ì„±ëŠ¥ í–¥ìƒ ì¸¡ì •
4. **íŒ€ ì—­ëŸ‰ ê³ ë ¤**: íŒ€ì˜ ê¸°ìˆ  ìˆ˜ì¤€ê³¼ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œê°„ ê³ ë ¤

ê¸°ë³¸ ê¸°ëŠ¥ì´ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•œ í›„, ì‹¤ì œ í•„ìš”ì„±ì— ë”°ë¼ ì„ íƒì ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.