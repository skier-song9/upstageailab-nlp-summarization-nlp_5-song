# AIStages ëŒ€íšŒ í†µí•© ê°€ì´ë“œ - ì™„ì „ ì •ë¦¬

## ëª©ì°¨
1. [í”„ë¡œì íŠ¸ í˜„í™© ë¶„ì„](#1-í”„ë¡œì íŠ¸-í˜„í™©-ë¶„ì„)
2. [ìƒˆë¡œìš´ ë‚´ìš© ìš”ì•½](#2-ìƒˆë¡œìš´-ë‚´ìš©-ìš”ì•½)
3. [ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­](#3-ì¦‰ì‹œ-ì ìš©-ê°€ëŠ¥í•œ-ê°œì„ ì‚¬í•­)
4. [ë‹¨ê³„ë³„ í†µí•© ì „ëµ](#4-ë‹¨ê³„ë³„-í†µí•©-ì „ëµ)
5. [ì‹¤ì „ ì½”ë“œ ì˜ˆì‹œ](#5-ì‹¤ì „-ì½”ë“œ-ì˜ˆì‹œ)
6. [íŒ€ í˜‘ì—… ê°€ì´ë“œ](#6-íŒ€-í˜‘ì—…-ê°€ì´ë“œ)

---

## 1. í”„ë¡œì íŠ¸ í˜„í™© ë¶„ì„

### 1.1 ê¸°ì¡´ ë¬¸ì„œí™” ìƒíƒœ

#### ì´ë¯¸ êµ¬í˜„/ë¬¸ì„œí™”ëœ ë‚´ìš©
- âœ… **ê¸°ë³¸ ë² ì´ìŠ¤ë¼ì¸**: `code/baseline.ipynb`
- âœ… **Solar API í™œìš©**: `code/solar_api.ipynb`
- âœ… **UV íŒ¨í‚¤ì§€ ê´€ë¦¬ì**: `docs/uv_package_manager_guide.md`
- âœ… **í”„ë¡œì íŠ¸ êµ¬ì¡°**: ì²´ê³„ì ì¸ í´ë” êµ¬ì¡°
- âœ… **ê¸°ë³¸ config**: `code/config.yaml`

#### ìƒˆë¡œ ì¶”ê°€ëœ ë¬¸ì„œ
- ğŸ†• **AIStages í™˜ê²½ ì„¤ì •**: ì„œë²„ íŠ¹í™” ì„¤ì •
- ğŸ†• **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: WandB Sweepì„ í†µí•œ ë² ì´ì§€ì•ˆ ìµœì í™” (âœ… êµ¬í˜„ ì™„ë£Œ)
- ğŸ†• **í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„**: ì „ì²˜ë¦¬ ë° ì‹œê°í™”
- ğŸ†• **WandB ì‹¤í—˜ ê´€ë¦¬**: íŒ€ í˜‘ì—… ë„êµ¬

### 1.2 ê°œì„  í•„ìš” ì‚¬í•­

| ì˜ì—­ | í˜„ì¬ ìƒíƒœ | ê°œì„  ë°©ì•ˆ |
| í•˜ì´í¼íŒŒë¼ë¯¸í„° | ìˆ˜ë™ ì¡°ì • | WandB Sweep (ë² ì´ì§€ì•ˆ ìµœì í™”) âœ… |
| í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” | WandB Sweep | êµ¬í˜„ ì™„ë£Œ, ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥ |
| í™˜ê²½ ì„¤ì • | ìˆ˜ë™ pip ì„¤ì¹˜ | UV + ìë™í™” ìŠ¤í¬ë¦½íŠ¸ |
| ì‹¤í—˜ ê´€ë¦¬ | ë¡œì»¬ ì €ì¥ | WandB í†µí•© |

| ë°ì´í„° ë¶„ì„ | ê¸°ë³¸ EDA | ì²´ê³„ì  ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ |

## 2. ìƒˆë¡œìš´ ë‚´ìš© ìš”ì•½

### 2.1 í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„
- **ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹**: 8ê°€ì§€ PII í† í° ì²˜ë¦¬
- **Special Token**: #Person1# ~ #Person7# ì¶”ê°€
- **ì „ì²˜ë¦¬**: êµ¬ì–´ì²´ í‘œí˜„ ì •ì œ (ã…‹ã…‹â†’ì›ƒìŒ)
- **ì‹œê°í™”**: ì›Œë“œí´ë¼ìš°ë“œ, TF-IDF ë¶„ì„

### 2.2 WandB ì‹¤í—˜ ê´€ë¦¬
- **íŒ€ í˜‘ì—…**: ì‹¤ì‹œê°„ ì‹¤í—˜ ê²°ê³¼ ê³µìœ 
- **ìë™ ì¶”ì **: í•˜ì´í¼íŒŒë¼ë¯¸í„°, ë©”íŠ¸ë¦­ ê¸°ë¡
- **Sweep**: ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
- **ì•„í‹°íŒ©íŠ¸**: ëª¨ë¸/ë°ì´í„° ë²„ì „ ê´€ë¦¬
### 2.3 í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (WandB Sweep)
- **í˜„ì¬ ìƒíƒœ**: WandB Sweepì„ í†µí•œ ë² ì´ì§€ì•ˆ ìµœì í™” âœ… êµ¬í˜„ ì™„ë£Œ
- **í•µì‹¬ ê¸°ëŠ¥**: 
  - ë² ì´ì§€ì•ˆ ìµœì í™” (Optunaì™€ ë™ë“±)
  - Hyperband ì¡°ê¸° ì¢…ë£Œ
  - ì‹¤ì‹œê°„ ì‹¤í—˜ ì¶”ì 
- **ì£¼ìš” íŒŒë¼ë¯¸í„°**:
- **ì£¼ìš” íŒŒë¼ë¯¸í„°**: 
  - Learning Rate: 1e-5 ~ 5e-4
  - Batch Size: 8, 16, 32
  - Epochs: 10-30
  - Warmup Ratio: 0.0-0.2

## 3. ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­
## 3. ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­

### 3.1 WandB Sweepìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” âœ…

1. **ì„¤ì¹˜ ë° ë¡œê·¸ì¸**
   ```bash
   pip install wandb
   wandb login
   ```

2. **Sweep ì‹¤í–‰**
   ```bash
   # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
   python code/sweep_runner.py \
     --base-config config/base_config.yaml \
     --sweep-config hyperparameter_sweep \
     --count 50
   ```

3. **Sweep ì„¤ì • íŒŒì¼** (`config/sweep/hyperparameter_sweep.yaml`)
   ```yaml
   method: bayes  # ë² ì´ì§€ì•ˆ ìµœì í™”
   metric:
     name: rouge_combined_f1
     goal: maximize
   
   parameters:
     learning_rate:
       distribution: log_uniform_values
       min: 1.0e-6
       max: 1.0e-4
     
     per_device_train_batch_size:
       values: [8, 16, 32, 64]
   ```

4. **ì¥ì **
   - Optunaì™€ ë™ë“±í•œ ë² ì´ì§€ì•ˆ ìµœì í™”
   - WandB ëŒ€ì‹œë³´ë“œì—ì„œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí‡ë§
   - íŒ€ í˜‘ì—… ë° ê²°ê³¼ ê³µìœ  ìš©ì´
tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
```

### 3.3 ë°ì´í„° ì „ì²˜ë¦¬
```python
# êµ¬ì–´ì²´ ì •ì œ
def clean_dialogue(text):
    text = re.sub(r'ã…‹+', 'ì›ƒìŒ', text)
    text = re.sub(r'ã…+', 'ì›ƒìŒ', text)
    text = re.sub(r'ã… +|ã…œ+', 'ìŠ¬í””', text)
    return text.strip()

train['dialogue_clean'] = train['dialogue'].apply(clean_dialogue)
```

## 4. ë‹¨ê³„ë³„ í†µí•© ì „ëµ

### Phase 1: ê¸°ì´ˆ ê°œì„  (1-2ì¼)
1. **UV í™˜ê²½ ì„¤ì •**
   ```bash
   # UV ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •
   curl -LsSf https://astral.sh/uv/install.sh | sh
   uv pip install -r requirements.txt --system
   ```

2. **ë°ì´í„° ì „ì²˜ë¦¬ ì ìš©**
   ```python
   # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©
   preprocessor = DialoguePreprocessor()
   train_processed = preprocessor.preprocess(train)
   ```

3. **WandB ì„¤ì •**
   ```python
   # WandB ì´ˆê¸°í™”
   wandb.init(
       project="dialogue-summarization",
       entity="your-team-name",
       config=config
   )
   ```

### Phase 2: ì‹¤í— ìµœì í™” (3-4ì¼)
1. **ì²´ê³„ì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰** (í˜„ì¬ ê´€ë¦¬ ë° Grid Search ê¸°ë°˜)
   ```python
   # í˜„ì¬ êµ¬í˜„ëœ Grid Search ë°©ë²•
   learning_rates = [1e-5, 2e-5, 3e-5, 5e-5]
   batch_sizes = [8, 16, 32]
   
   # auto_experiment_runner.py í™œìš©ìœ¼ë¡œ ìˆœì°¨ ì‹¤í— ì‹¤í–‰
   # ë¯¸ë˜: Optuna í†µí•© ì‹œ Bayesian Optimization ì ìš© ì˜ˆì •
   # ë¯¸ë˜: Optuna í†µí•© ì‹œ Bayesian Optimization ì ìš© ì˜ˆì •
   ```

   **í–¥í›„ Optuna í†µí•© ì˜ˆì •ì§€**:
   ```python
   # TODO: í–¥í›„ êµ¬í˜„ ì˜ˆì •
   # def optuna_hp_space(trial):
   #     return {
   #         "learning_rate": trial.suggest_loguniform('lr', 1e-5, 5e-4),
   #         "per_device_train_batch_size": trial.suggest_categorical('bs', [8, 16, 32]),
   #         "num_train_epochs": trial.suggest_int('epochs', 10, 30, step=5)
   #     }
   ```

2. **ë°ì´í„° ì¦ê°•**
   - Paraphrasing
   - Back-translation
   - ë…¸ì´ì¦ˆ ì¶”ê°€

### Phase 3: ê³ ê¸‰ ìµœì í™” (5-7ì¼)
1. **ëª¨ë¸ ì•™ìƒë¸”**
2. **Advanced ì „ì²˜ë¦¬**
3. **Custom Loss Functions**

## 5. ì‹¤ì „ ì½”ë“œ ì˜ˆì‹œ

> **âš ï¸ ì¤‘ìš” ì•ˆë‚´**: ì•„ë˜ ì½”ë“œ ì˜ˆì‹œëŠ” Optuna í†µí•©ì´ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, **í˜„ì¬ ì‹œì ì—ì„œëŠ” ë¯¸êµ¬í˜„ ìƒíƒœ**ì…ë‹ˆë‹¤. 
> ì´ëŠ” í–¥í›„ êµ¬í˜„ ì˜ˆì •ì¸ ê¸°ëŠ¥ì˜ ì„¤ê³„ ë„ë©´ìœ¼ë¡œ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
> **í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥**: `auto_experiment_runner.py`ë¥¼ í†µí•œ ìˆœì°¨ì  ì‹¤í—˜ ì‹¤í–‰, WandB Sweepì„ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
> **ğŸ‰ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥**: WandB Sweepì„ í†µí•´ Optunaì™€ ë™ë“±í•œ ìˆ˜ì¤€ì˜ ë² ì´ì§€ì•ˆ ìµœì í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤!
> ì•„ë˜ ì½”ë“œëŠ” WandB Sweepì„ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜ˆì‹œì…ë‹ˆë‹¤.
```python
# train_integrated.py - í–¥í›„ êµ¬í˜„ ì˜ˆì •ì¸ ê¸°ëŠ¥ë“¤
import wandb
# import optuna  # í–¥í›„ ì¶”ê°€ ì˜ˆì •
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
import pandas as pd
import numpy as np
from rouge_score import rouge_scorer

class IntegratedTrainer:
    def __init__(self, config_path="config_integrated.yaml"):
        self.config = self.load_config(config_path)
        self.setup_environment()
        
    def setup_environment(self):
        """í™˜ê²½ ì„¤ì •"""
        # WandB ì´ˆê¸°í™”
        wandb.init(
            project=self.config['wandb']['project'],
            entity=self.config['wandb']['entity'],
            config=self.config
        )
        
        # Tokenizer & Model
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['model']['name']
        )
        self.add_special_tokens()
        
    def add_special_tokens(self):
        """Special Token ì¶”ê°€"""
        special_tokens = [
            f'#Person{i}#' for i in range(1, 8)
        ] + [
            '#PhoneNumber#', '#Address#', '#DateOfBirth#',
            '#PassportNumber#', '#SSN#', '#CardNumber#',
            '#CarNumber#', '#Email#'
        ]
        
        self.tokenizer.add_special_tokens({
            'additional_special_tokens': special_tokens
        })
        
    def preprocess_data(self, df):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # êµ¬ì–´ì²´ ì •ì œ
        df['dialogue'] = df['dialogue'].apply(self.clean_dialogue)
        
        # í† í°í™”
        def tokenize_function(examples):
            model_inputs = self.tokenizer(
                examples['dialogue'],
                max_length=self.config['model']['max_encoder_length'],
                truncation=True,
                padding="max_length"
            )
            
            with self.tokenizer.as_target_tokenizer():
                labels = self.tokenizer(
                    examples['summary'],
                    max_length=self.config['model']['max_decoder_length'],
                    truncation=True,
                    padding="max_length"
                )
            
            model_inputs["labels"] = labels["input_ids"]
            return model_inputs
        
        return df.map(tokenize_function, batched=True)
    
    @staticmethod
    def clean_dialogue(text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        import re
        text = re.sub(r'ã…‹+', 'ì›ƒìŒ', text)
        text = re.sub(r'ã…+', 'ì›ƒìŒ', text)
        text = re.sub(r'ã… +|ã…œ+', 'ìŠ¬í””', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def compute_metrics(self, eval_pred):
        """ROUGE ë©”íŠ¸ë¦­ ê³„ì‚°"""
        predictions, labels = eval_pred
        
        # ë””ì½”ë”©
        decoded_preds = self.tokenizer.batch_decode(
            predictions, skip_special_tokens=True
        )
        decoded_labels = self.tokenizer.batch_decode(
            labels, skip_special_tokens=True
        )
        
        # ROUGE ê³„ì‚°
        scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'], use_stemmer=False
        )
        
        scores = []
        for pred, label in zip(decoded_preds, decoded_labels):
            score = scorer.score(label, pred)
            scores.append({
                'rouge1': score['rouge1'].fmeasure,
                'rouge2': score['rouge2'].fmeasure,
                'rougeL': score['rougeL'].fmeasure
            })
        
        # í‰ê·  ê³„ì‚°
        result = {
            'rouge1': np.mean([s['rouge1'] for s in scores]),
            'rouge2': np.mean([s['rouge2'] for s in scores]),
            'rougeL': np.mean([s['rougeL'] for s in scores])
        }
        
        # WandB ë¡œê¹…
        wandb.log(result)
        
        return result
    
    def create_trainer(self, trial=None):
        """Trainer ìƒì„±"""
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        if trial:
            hp = self.get_optuna_params(trial)
        else:
            hp = self.config['training']
        
        # Training Arguments
        training_args = Seq2SeqTrainingArguments(
            output_dir=self.config['general']['output_dir'],
            num_train_epochs=hp['num_train_epochs'],
            per_device_train_batch_size=hp['per_device_train_batch_size'],
            learning_rate=hp['learning_rate'],
            warmup_ratio=hp.get('warmup_ratio', 0.1),
            weight_decay=hp.get('weight_decay', 0.01),
            logging_steps=100,
            evaluation_strategy="steps",
            eval_steps=500,
            save_steps=500,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="rougeL",
            greater_is_better=True,
            fp16=True,
            gradient_checkpointing=True,
            predict_with_generate=True,
            generation_max_length=self.config['model']['max_decoder_length'],
            report_to="wandb"
        )
        
        # Model ì´ˆê¸°í™”
        model = AutoModelForSeq2SeqLM.from_pretrained(
            self.config['model']['name']
        )
        model.resize_token_embeddings(len(self.tokenizer))
        
        # Trainer
        return Seq2SeqTrainer(
            model=model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            tokenizer=self.tokenizer,
            compute_metrics=self.compute_metrics
        )
    
    def get_optuna_params(self, trial):
        """Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„°"""
        return {
            'learning_rate': trial.suggest_loguniform('lr', 1e-5, 5e-4),
            'per_device_train_batch_size': trial.suggest_categorical(
                'batch_size', [8, 16, 32]
            ),
            'num_train_epochs': trial.suggest_int('epochs', 10, 30, step=5),
            'warmup_ratio': trial.suggest_float('warmup', 0.0, 0.2),
            'weight_decay': trial.suggest_categorical(
                'weight_decay', [0.0, 0.01, 0.1]
            )
        }
    
    def optuna_objective(self, trial):
        """Optuna ëª©ì  í•¨ìˆ˜"""
        # Trainer ìƒì„±
        trainer = self.create_trainer(trial)
        
        # í•™ìŠµ
        trainer.train()
        
        # í‰ê°€
        metrics = trainer.evaluate()
        
        # WandBì— trial ì •ë³´ ê¸°ë¡
        wandb.log({
            f"trial_{trial.number}/rouge1": metrics['eval_rouge1'],
            f"trial_{trial.number}/rouge2": metrics['eval_rouge2'],
            f"trial_{trial.number}/rougeL": metrics['eval_rougeL']
        })
        
        return metrics['eval_rougeL']
    
    def run_hyperparameter_search(self, n_trials=20):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰"""
        study = optuna.create_study(
            direction="maximize",
            study_name="dialogue-summarization-hp-search"
        )
        
        study.optimize(self.optuna_objective, n_trials=n_trials)
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
        best_params = study.best_params
        wandb.config.update({"best_params": best_params})
        
        print(f"Best parameters: {best_params}")
        print(f"Best ROUGE-L: {study.best_value}")
        
        return best_params
    
    def train_with_best_params(self, best_params=None):
        """ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ"""
        if best_params:
            self.config['training'].update(best_params)
        
        # Trainer ìƒì„± ë° í•™ìŠµ
        trainer = self.create_trainer()
        trainer.train()
        
        # ìµœì¢… í‰ê°€
        final_metrics = trainer.evaluate()
        print(f"Final ROUGE scores: {final_metrics}")
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model(f"{self.config['general']['output_dir']}/best_model")
        
        # WandB Artifact ì €ì¥
        artifact = wandb.Artifact(
            name="dialogue-summarization-model",
            type="model"
        )
        artifact.add_dir(f"{self.config['general']['output_dir']}/best_model")
        wandb.log_artifact(artifact)
        
        return trainer

# ì‹¤í–‰
if __name__ == "__main__":
    # í†µí•© í•™ìŠµê¸° ì´ˆê¸°í™”
    trainer = IntegratedTrainer()
    
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    train_df = pd.read_csv("data/train.csv")
    eval_df = pd.read_csv("data/dev.csv")
    
    trainer.train_dataset = trainer.preprocess_data(train_df)
    trainer.eval_dataset = trainer.preprocess_data(eval_df)
    
    # ì˜µì…˜ 1: í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
    best_params = trainer.run_hyperparameter_search(n_trials=20)
    
    # ì˜µì…˜ 2: ìµœì  íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ
    trainer.train_with_best_params(best_params)
    
    wandb.finish()
```

### 5.2 Config íŒŒì¼ (YAML)
```yaml
# config_integrated.yaml
general:
  project_name: "dialogue-summarization"
  data_path: "./data/"
  output_dir: "./outputs/integrated_experiment"
  seed: 42

model:
  name: "gogamza/kobart-base-v2"
  max_encoder_length: 512
  max_decoder_length: 128

training:
  learning_rate: 3e-5
  per_device_train_batch_size: 16
  num_train_epochs: 20
  warmup_ratio: 0.1
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  fp16: true
  save_total_limit: 3
  early_stopping_patience: 3

wandb:
  project: "dialogue-summarization"
  entity: "nlp-team-5"
  tags: ["kobart", "integrated", "planned-optuna"]
  notes: "í†µí•© ì‹¤í—˜ ê³„íš - í–¥í›„ Optuna + WandB + ì „ì²˜ë¦¬"

optuna:
  n_trials: 20
  direction: "maximize"
  metric: "rougeL"
  pruner: "MedianPruner"
```

## 6. íŒ€ í˜‘ì—… ê°€ì´ë“œ

### 6.1 Git ì›Œí¬í”Œë¡œìš°
```bash
# 1. ìµœì‹  ì½”ë“œ ë™ê¸°í™”
git fetch upstream main
git merge FETCH_HEAD

# 2. ì‹¤í—˜ ë¸Œëœì¹˜ ìƒì„±
git checkout -b exp/hp-tuning-lr

# 3. ì‹¤í—˜ í›„ ì»¤ë°‹
git add -A
git commit -m "feat: Add Optuna hyperparameter tuning"

# 4. Push ë° PR
git push origin exp/hp-tuning-lr
```

### 6.2 ì‹¤í—˜ ëª…ëª… ê·œì¹™
```python
# WandB ì‹¤í—˜ ì´ë¦„
experiment_name = f"{model_type}_{data_version}_lr{lr}_bs{bs}_ep{epochs}"
# ì˜ˆ: kobart_v2_lr3e-5_bs16_ep20

# ë¸Œëœì¹˜ ì´ë¦„
branch_name = f"exp/{feature}-{description}"
# ì˜ˆ: exp/optuna-integration, exp/data-augmentation
```

### 6.3 ì‹¤í—˜ ê¸°ë¡ í…œí”Œë¦¿
```markdown
## ì‹¤í—˜ #001: Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

**ë‚ ì§œ**: 2025-01-27
**ì‹¤í—˜ì**: @username
**WandB Run**: [ë§í¬](https://wandb.ai/...)

### ëª©ì 
- Learning Rate ìµœì ê°’ íƒìƒ‰
- Batch Sizeì™€ ì„±ëŠ¥ ê´€ê³„ ë¶„ì„

### ì„¤ì •
- Model: gogamza/kobart-base-v2
- Optuna Trials: 20
- íƒìƒ‰ ë²”ìœ„:
  - LR: 1e-5 ~ 5e-4
  - BS: [8, 16, 32]
  - Epochs: 10-30

### ê²°ê³¼
- Best LR: 2.3e-5
- Best BS: 16
- Best Epochs: 22
- Final ROUGE-L: 0.4856

### ì¸ì‚¬ì´íŠ¸
1. LRì€ 2e-5 ~ 3e-5 ë²”ìœ„ê°€ ìµœì 
2. BS 32ëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì‹¤íŒ¨
3. 20 epochs ì´í›„ ì„±ëŠ¥ í¬í™”

### ë‹¤ìŒ ë‹¨ê³„
- [ ] Warmup ratio ì¶”ê°€ íƒìƒ‰
- [ ] Learning rate scheduler ì‹¤í—˜
- [ ] ë°ì´í„° ì¦ê°• ì ìš©
```

### 6.4 ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì¼ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ì½”ë“œ ë™ê¸°í™” (git pull)
- [ ] WandB ì‹¤í—˜ ê²°ê³¼ í™•ì¸
- [ ] íŒ€ ìŠ¬ë™ì— ì§„í–‰ìƒí™© ê³µìœ 
- [ ] ë‹¤ìŒ ì‹¤í—˜ ê³„íš ìˆ˜ë¦½

#### ì‹¤í—˜ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
- [ ] ë°ì´í„° ê²½ë¡œ í™•ì¸
- [ ] Config íŒŒì¼ ê²€ì¦
- [ ] WandB í”„ë¡œì íŠ¸ í™•ì¸
- [ ] ì´ì „ ì‹¤í—˜ ê²°ê³¼ ë¦¬ë·°

#### ì œì¶œ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦
- [ ] ì¶”ë¡  ì½”ë“œ í…ŒìŠ¤íŠ¸
- [ ] ì œì¶œ íŒŒì¼ í˜•ì‹ í™•ì¸
- [ ] íŒ€ì› ë¦¬ë·° ì™„ë£Œ

## ê²°ë¡ 

ì´ í†µí•© ê°€ì´ë“œë¥¼ í†µí•´:

1. **ì¦‰ì‹œ ì ìš©**: UV í™˜ê²½ ì„¤ì •, ë°ì´í„° ì „ì²˜ë¦¬, Special Token ì¶”ê°€
2. **ë‹¨ê³„ë³„ ê°œì„ **: WandB â†’ Optuna â†’ ê³ ê¸‰ ê¸°ë²• ìˆœì°¨ ì ìš©
3. **ì²´ê³„ì  ì‹¤í—˜**: ëª…ëª… ê·œì¹™ê³¼ í…œí”Œë¦¿ìœ¼ë¡œ ì¼ê´€ì„± ìœ ì§€
4. **íŒ€ í˜‘ì—… ê°•í™”**: Git, WandBë¡œ íš¨ìœ¨ì  ê³µìœ 

í•µì‹¬ì€ **ì‘ì€ ê°œì„ ë¶€í„° ì‹œì‘**í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ë°œì „ì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê° ë‹¨ê³„ë³„ë¡œ ì„±ëŠ¥ í–¥ìƒì„ ì¸¡ì •í•˜ê³  ê¸°ë¡í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ìµœê³ ì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ë‚´ì„¸ìš”!
