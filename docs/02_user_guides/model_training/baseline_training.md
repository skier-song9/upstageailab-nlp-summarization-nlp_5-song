# Dialogue Summarization ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œ ìƒì„¸ ë¶„ì„

## ëª©ì°¨
1. [ì½”ë“œ ê°œìš”](#1-ì½”ë“œ-ê°œìš”)
2. [í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬](#2-í™˜ê²½-ì„¤ì •-ë°-ë¼ì´ë¸ŒëŸ¬ë¦¬)
3. [Configuration ì„¤ì •](#3-configuration-ì„¤ì •)
4. [ë°ì´í„° ì „ì²˜ë¦¬](#4-ë°ì´í„°-ì „ì²˜ë¦¬)
5. [ë°ì´í„°ì…‹ í´ë˜ìŠ¤](#5-ë°ì´í„°ì…‹-í´ë˜ìŠ¤)
6. [ëª¨ë¸ í•™ìŠµ ì„¤ì •](#6-ëª¨ë¸-í•™ìŠµ-ì„¤ì •)
7. [ëª¨ë¸ ì¶”ë¡ ](#7-ëª¨ë¸-ì¶”ë¡ )
8. [ì£¼ìš” ê°œë… ì„¤ëª…](#8-ì£¼ìš”-ê°œë…-ì„¤ëª…)

---

## 1. ì½”ë“œ ê°œìš”

ì´ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œëŠ” **BART(Bidirectional and Auto-Regressive Transformers)** ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¼ìƒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•ì€:

- **ëª¨ë¸**: KoBART (í•œêµ­ì–´ íŠ¹í™” BART ëª¨ë¸)
- **í”„ë ˆì„ì›Œí¬**: Hugging Face Transformers + PyTorch
- **í•™ìŠµ ê´€ë¦¬**: Weights & Biases (wandb)
- **í‰ê°€ ì§€í‘œ**: ROUGE (ROUGE-1, ROUGE-2, ROUGE-L)

## 2. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì½”ë“œ ìœ„ì¹˜: ì…€ 3)

> ğŸ’¡ **íŒ**: íŒ¨í‚¤ì§€ ì„¤ì¹˜ê°€ ëŠë¦¬ë‹¤ë©´ `uv`ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”! 
> ```bash
> pip install uv
> uv pip install -r requirements.txt  # 10ë°° ì´ìƒ ë¹ ë¦„!
> ```
> ìì„¸í•œ ë‚´ìš©ì€ [uv íŒ¨í‚¤ì§€ ê´€ë¦¬ì ê°€ì´ë“œ](uv_package_manager_guide.md) ì°¸ê³ 

```python
import pandas as pd              # ë°ì´í„° ì²˜ë¦¬
import os                       # íŒŒì¼ ê²½ë¡œ ê´€ë¦¬
import re                       # ì •ê·œí‘œí˜„ì‹
import json                     # JSON ë°ì´í„° ì²˜ë¦¬
import yaml                     # YAML ì„¤ì • íŒŒì¼ ì²˜ë¦¬
from glob import glob           # íŒŒì¼ íŒ¨í„´ ë§¤ì¹­
from tqdm import tqdm           # ì§„í–‰ ìƒí™© í‘œì‹œ
from pprint import pprint       # ì˜ˆìœ ì¶œë ¥
import torch                    # PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
import pytorch_lightning as pl  # PyTorch Lightning
from rouge import Rouge         # ROUGE í‰ê°€ ì§€í‘œ

# Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback

import wandb  # ì‹¤í—˜ ê´€ë¦¬ ë° ì‹œê°í™”
```

### ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ëª…

1. **pandas**: CSV íŒŒì¼ í˜•íƒœì˜ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ê´€ë¦¬
2. **torch**: PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì˜ í•µì‹¬
3. **transformers**: Hugging Faceì˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
4. **rouge**: í…ìŠ¤íŠ¸ ìš”ì•½ í‰ê°€ë¥¼ ìœ„í•œ ROUGE ì ìˆ˜ ê³„ì‚°
5. **wandb**: ëª¨ë¸ í•™ìŠµ ê³¼ì • ì¶”ì  ë° ì‹œê°í™”

## 3. Configuration ì„¤ì •

### 3.1 Config íŒŒì¼ êµ¬ì¡° (ì½”ë“œ ìœ„ì¹˜: ì…€ 5, ì¤„ 1-47)

```python
config_data = {
    "general": {
        "data_path": "../data/",  # ë°ì´í„° ê²½ë¡œ
        "model_name": "digit82/kobart-summarization",  # ì‚¬ìš©í•  ëª¨ë¸
        "output_dir": "./"  # ì¶œë ¥ ë””ë ‰í† ë¦¬
    },
    "tokenizer": {
        "encoder_max_len": 512,  # ì¸ì½”ë” ìµœëŒ€ í† í° ê¸¸ì´
        "decoder_max_len": 100,  # ë””ì½”ë” ìµœëŒ€ í† í° ê¸¸ì´
        "bos_token": f"{tokenizer.bos_token}",  # ì‹œì‘ í† í°
        "eos_token": f"{tokenizer.eos_token}",  # ì¢…ë£Œ í† í°
        "special_tokens": [...]  # íŠ¹ìˆ˜ í† í° ë¦¬ìŠ¤íŠ¸
    },
    "training": {
        "num_train_epochs": 20,  # í•™ìŠµ ì—í­ ìˆ˜
        "learning_rate": 1e-5,   # í•™ìŠµë¥ 
        "per_device_train_batch_size": 50,  # ë°°ì¹˜ í¬ê¸°
        # ... ê¸°íƒ€ í•™ìŠµ ì„¤ì •
    },
    "wandb": {
        "entity": "wandb_repo",
        "project": "project_name",
        "name": "run_name"
    },
    "inference": {
        "ckt_path": "model ckt path",  # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        "batch_size": 32,
        # ... ê¸°íƒ€ ì¶”ë¡  ì„¤ì •
    }
}
```

### 3.2 ì£¼ìš” ì„¤ì • í•­ëª© ì„¤ëª…

#### General ì„¤ì •
- **data_path**: í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì €ì¥ëœ ê²½ë¡œ
- **model_name**: Hugging Face ëª¨ë¸ í—ˆë¸Œì—ì„œ ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ ì´ë¦„
- **output_dir**: í•™ìŠµëœ ëª¨ë¸ê³¼ ë¡œê·¸ê°€ ì €ì¥ë  ê²½ë¡œ

#### Tokenizer ì„¤ì •
- **encoder_max_len**: ì…ë ¥ ëŒ€í™”ë¬¸ì˜ ìµœëŒ€ í† í° ê¸¸ì´ (512)
- **decoder_max_len**: ìƒì„±í•  ìš”ì•½ë¬¸ì˜ ìµœëŒ€ í† í° ê¸¸ì´ (100)
- **special_tokens**: ì‚¬ëŒ êµ¬ë¶„ì(#Person1#, #Person2# ë“±) í¬í•¨

#### Training ì„¤ì •
- **num_train_epochs**: ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë°˜ë³µ í•™ìŠµí• ì§€ (20)
- **learning_rate**: ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì†ë„ (1e-5)
- **per_device_train_batch_size**: GPUë‹¹ í•œ ë²ˆì— ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜ (50)
- **fp16**: 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì‚¬ìš© ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì ˆì•½)

## 4. ë°ì´í„° ì „ì²˜ë¦¬

### 4.1 Preprocess í´ë˜ìŠ¤ (ì½”ë“œ ìœ„ì¹˜: ì…€ 13, ì¤„ 1-37)

```python
class Preprocess:
    def __init__(self, bos_token: str, eos_token: str) -> None:
        self.bos_token = bos_token  # ì‹œì‘ í† í°
        self.eos_token = eos_token  # ì¢…ë£Œ í† í°

    @staticmethod
    def make_set_as_df(file_path, is_train=True):
        """CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
        if is_train:
            df = pd.read_csv(file_path)
            train_df = df[['fname','dialogue','summary']]
            return train_df
        else:
            df = pd.read_csv(file_path)
            test_df = df[['fname','dialogue']]
            return test_df

    def make_input(self, dataset, is_test=False):
        """BART ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë°ì´í„° ê°€ê³µ"""
        if is_test:
            encoder_input = dataset['dialogue']
            decoder_input = [self.bos_token] * len(dataset['dialogue'])
            return encoder_input.tolist(), list(decoder_input)
        else:
            encoder_input = dataset['dialogue']
            # ë””ì½”ë” ì…ë ¥: <s> + ì •ë‹µ ìš”ì•½ë¬¸
            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))
            # ë””ì½”ë” ì¶œë ¥: ì •ë‹µ ìš”ì•½ë¬¸ + </s>
            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)
            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()
```

### 4.2 ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì • ì„¤ëª…

1. **í•™ìŠµ ë°ì´í„° êµ¬ì¡°**:
   - **Encoder ì…ë ¥**: ì›ë³¸ ëŒ€í™”ë¬¸ (dialogue)
   - **Decoder ì…ë ¥**: `<s>` + ì •ë‹µ ìš”ì•½ë¬¸
   - **Decoder ì¶œë ¥**: ì •ë‹µ ìš”ì•½ë¬¸ + `</s>`

2. **í…ŒìŠ¤íŠ¸ ë°ì´í„° êµ¬ì¡°**:
   - **Encoder ì…ë ¥**: ì›ë³¸ ëŒ€í™”ë¬¸
   - **Decoder ì…ë ¥**: `<s>` í† í°ë§Œ (ìš”ì•½ë¬¸ ìƒì„± ì‹œì‘)

### 4.3 prepare_train_dataset í•¨ìˆ˜ (ì½”ë“œ ìœ„ì¹˜: ì…€ 15, ì¤„ 1-45)

```python
def prepare_train_dataset(config, preprocessor, data_path, tokenizer):
    # 1. CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
    train_file_path = os.path.join(data_path,'train.csv')
    val_file_path = os.path.join(data_path,'dev.csv')

    # 2. DataFrame ìƒì„±
    train_data = preprocessor.make_set_as_df(train_file_path)
    val_data = preprocessor.make_set_as_df(val_file_path)

    # 3. ì…ë ¥ ë°ì´í„° ìƒì„±
    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)
    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)

    # 4. í† í°í™” (Tokenization)
    tokenized_encoder_inputs = tokenizer(
        encoder_input_train, 
        return_tensors="pt",      # PyTorch í…ì„œë¡œ ë°˜í™˜
        padding=True,             # íŒ¨ë”© ì¶”ê°€
        add_special_tokens=True,  # íŠ¹ìˆ˜ í† í° ì¶”ê°€
        truncation=True,          # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ì‹œ ìë¥´ê¸°
        max_length=config['tokenizer']['encoder_max_len'],
        return_token_type_ids=False
    )

    # 5. Dataset ê°ì²´ ìƒì„±
    train_inputs_dataset = DatasetForTrain(
        tokenized_encoder_inputs, 
        tokenized_decoder_inputs, 
        tokenized_decoder_outputs,
        len(encoder_input_train)
    )
```

## 5. ë°ì´í„°ì…‹ í´ë˜ìŠ¤

### 5.1 DatasetForTrain í´ë˜ìŠ¤ (ì½”ë“œ ìœ„ì¹˜: ì…€ 14, ì¤„ 1-21)

```python
class DatasetForTrain(Dataset):
    def __init__(self, encoder_input, decoder_input, labels, len):
        self.encoder_input = encoder_input
        self.decoder_input = decoder_input
        self.labels = labels
        self.len = len

    def __getitem__(self, idx):
        # 1. ì¸ì½”ë” ì…ë ¥ ì¤€ë¹„
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        # itemì—ëŠ” 'input_ids'ì™€ 'attention_mask' í¬í•¨

        # 2. ë””ì½”ë” ì…ë ¥ ì¤€ë¹„
        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}
        
        # 3. í‚¤ ì´ë¦„ ë³€ê²½ (ë””ì½”ë”ìš©)
        item2['decoder_input_ids'] = item2['input_ids']
        item2['decoder_attention_mask'] = item2['attention_mask']
        item2.pop('input_ids')
        item2.pop('attention_mask')
        
        # 4. ì¸ì½”ë”ì™€ ë””ì½”ë” ì…ë ¥ ë³‘í•©
        item.update(item2)
        
        # 5. ë ˆì´ë¸”(ì •ë‹µ) ì¶”ê°€
        item['labels'] = self.labels['input_ids'][idx]
        
        return item

    def __len__(self):
        return self.len
```

### 5.2 ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¡° ì„¤ëª…

**ìµœì¢… ë°˜í™˜ ë°ì´í„° êµ¬ì¡°**:
```python
{
    'input_ids': tensor([...]),              # ì¸ì½”ë” ì…ë ¥ í† í°
    'attention_mask': tensor([...]),          # ì¸ì½”ë” ì–´í…ì…˜ ë§ˆìŠ¤í¬
    'decoder_input_ids': tensor([...]),       # ë””ì½”ë” ì…ë ¥ í† í°
    'decoder_attention_mask': tensor([...]),  # ë””ì½”ë” ì–´í…ì…˜ ë§ˆìŠ¤í¬
    'labels': tensor([...])                   # ì •ë‹µ ë ˆì´ë¸”
}
```

## 6. ëª¨ë¸ í•™ìŠµ ì„¤ì •

### 6.1 compute_metrics í•¨ìˆ˜ (ì½”ë“œ ìœ„ì¹˜: ì…€ 16, ì¤„ 1-39)

```python
def compute_metrics(config, tokenizer, pred):
    rouge = Rouge()
    predictions = pred.predictions
    labels = pred.label_ids

    # 1. íŒ¨ë”© í† í°(-100) ì²˜ë¦¬
    predictions[predictions == -100] = tokenizer.pad_token_id
    labels[labels == -100] = tokenizer.pad_token_id

    # 2. í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)
    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)

    # 3. ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ í† í° ì œê±°
    remove_tokens = config['inference']['remove_tokens']
    for token in remove_tokens:
        replaced_predictions = [sentence.replace(token," ") for sentence in replaced_predictions]
        replaced_labels = [sentence.replace(token," ") for sentence in replaced_labels]

    # 4. ROUGE ì ìˆ˜ ê³„ì‚°
    results = rouge.get_scores(replaced_predictions, replaced_labels, avg=True)
    
    # 5. F1 ì ìˆ˜ë§Œ ì¶”ì¶œ
    result = {key: value["f"] for key, value in results.items()}
    return result
```

### 6.2 Trainer ì„¤ì • (ì½”ë“œ ìœ„ì¹˜: ì…€ 17, ì¤„ 1-72)

```python
def load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset):
    # 1. í•™ìŠµ ì¸ì ì„¤ì •
    training_args = Seq2SeqTrainingArguments(
        output_dir=config['general']['output_dir'],
        num_train_epochs=config['training']['num_train_epochs'],
        learning_rate=config['training']['learning_rate'],
        per_device_train_batch_size=config['training']['per_device_train_batch_size'],
        evaluation_strategy="epoch",  # ë§¤ ì—í­ë§ˆë‹¤ í‰ê°€
        save_strategy="epoch",        # ë§¤ ì—í­ë§ˆë‹¤ ì €ì¥
        fp16=True,                    # 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì‚¬ìš©
        predict_with_generate=True,   # ìƒì„± ëª¨ë“œë¡œ í‰ê°€
        # ... ê¸°íƒ€ ì„¤ì •
    )

    # 2. wandb ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
    wandb.init(
        entity=config['wandb']['entity'],
        project=config['wandb']['project'],
        name=config['wandb']['name'],
    )

    # 3. Early Stopping ì½œë°±
    MyCallback = EarlyStoppingCallback(
        early_stopping_patience=3,      # 3ë²ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
        early_stopping_threshold=0.001  # ìµœì†Œ ê°œì„  í­
    )

    # 4. Trainer ìƒì„±
    trainer = Seq2SeqTrainer(
        model=generate_model,
        args=training_args,
        train_dataset=train_inputs_dataset,
        eval_dataset=val_inputs_dataset,
        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),
        callbacks=[MyCallback]
    )
```

### 6.3 ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (ì½”ë“œ ìœ„ì¹˜: ì…€ 18, ì¤„ 1-19)

```python
def load_tokenizer_and_model_for_train(config, device):
    # 1. ëª¨ë¸ ì´ë¦„ ë° ì„¤ì •
    model_name = config['general']['model_name']
    bart_config = BartConfig().from_pretrained(model_name)
    
    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # 3. ëª¨ë¸ ë¡œë“œ
    generate_model = BartForConditionalGeneration.from_pretrained(
        config['general']['model_name'],
        config=bart_config
    )

    # 4. íŠ¹ìˆ˜ í† í° ì¶”ê°€
    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}
    tokenizer.add_special_tokens(special_tokens_dict)

    # 5. ì„ë² ë”© í¬ê¸° ì¡°ì • (íŠ¹ìˆ˜ í† í° ì¶”ê°€ë¡œ ì¸í•´)
    generate_model.resize_token_embeddings(len(tokenizer))
    
    # 6. GPUë¡œ ì´ë™
    generate_model.to(device)
```

## 7. ëª¨ë¸ ì¶”ë¡ 

### 7.1 ì¶”ë¡ ìš© ë°ì´í„° ì¤€ë¹„ (ì½”ë“œ ìœ„ì¹˜: ì…€ 23, ì¤„ 1-24)

```python
def prepare_test_dataset(config, preprocessor, tokenizer):
    # 1. í…ŒìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
    test_file_path = os.path.join(config['general']['data_path'], 'test.csv')
    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)
    test_id = test_data['fname']

    # 2. í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„± (ë””ì½”ë” ì…ë ¥ì€ <s> í† í°ë§Œ)
    encoder_input_test, decoder_input_test = preprocessor.make_input(test_data, is_test=True)

    # 3. í† í°í™”
    test_tokenized_encoder_inputs = tokenizer(
        encoder_input_test,
        return_tensors="pt",
        padding=True,
        add_special_tokens=True,
        truncation=True,
        max_length=config['tokenizer']['encoder_max_len'],
        return_token_type_ids=False
    )

    # 4. ì¶”ë¡ ìš© Dataset ìƒì„±
    test_encoder_inputs_dataset = DatasetForInference(
        test_tokenized_encoder_inputs,
        test_id,
        len(encoder_input_test)
    )
```

### 7.2 ì¶”ë¡  ì‹¤í–‰ (ì½”ë“œ ìœ„ì¹˜: ì…€ 25, ì¤„ 1-51)

```python
def inference(config):
    # 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)

    # 2. ë°ì´í„° ì¤€ë¹„
    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)
    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])

    # 3. ì¶”ë¡  ì‹¤í–‰
    summary = []
    text_ids = []
    
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        for item in tqdm(dataloader):
            text_ids.extend(item['ID'])
            
            # 4. í…ìŠ¤íŠ¸ ìƒì„±
            generated_ids = generate_model.generate(
                input_ids=item['input_ids'].to('cuda:0'),
                no_repeat_ngram_size=2,      # 2-gram ë°˜ë³µ ë°©ì§€
                early_stopping=True,         # ì¡°ê¸° ì¢…ë£Œ
                max_length=100,              # ìµœëŒ€ ìƒì„± ê¸¸ì´
                num_beams=4,                 # ë¹” ì„œì¹˜ í¬ê¸°
            )
            
            # 5. ë””ì½”ë”©
            for ids in generated_ids:
                result = tokenizer.decode(ids)
                summary.append(result)

    # 6. íŠ¹ìˆ˜ í† í° ì œê±°
    remove_tokens = config['inference']['remove_tokens']
    for token in remove_tokens:
        preprocessed_summary = [sentence.replace(token, " ") for sentence in preprocessed_summary]

    # 7. ê²°ê³¼ ì €ì¥
    output = pd.DataFrame({
        "fname": test_data['fname'],
        "summary": preprocessed_summary,
    })
    output.to_csv(os.path.join(result_path, "output.csv"), index=False)
```

## 8. ì£¼ìš” ê°œë… ì„¤ëª…

### 8.1 BART ëª¨ë¸

**BART (Bidirectional and Auto-Regressive Transformers)**ëŠ” Facebook AI Researchì—ì„œ ê°œë°œí•œ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤(Seq2Seq) ëª¨ë¸ì…ë‹ˆë‹¤.

**íŠ¹ì§•**:
- **ì¸ì½”ë”**: ì–‘ë°©í–¥(Bidirectional) - ë¬¸ë§¥ì„ ì™„ì „íˆ ì´í•´
- **ë””ì½”ë”**: ìê¸°íšŒê·€(Auto-Regressive) - ìˆœì°¨ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
- **ì‚¬ì „í•™ìŠµ**: í…ìŠ¤íŠ¸ ì†ìƒ ë³µì› íƒœìŠ¤í¬ë¡œ í•™ìŠµ

**KoBART**:
- í•œêµ­ì–´ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµëœ BART ëª¨ë¸
- 40GB ì´ìƒì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµ
- í•œêµ­ì–´ ìš”ì•½, ë²ˆì—­, ìƒì„±ì— íŠ¹í™”

### 8.2 í† í¬ë‚˜ì´ì € (Tokenizer)

**ì—­í• **: í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ì(í† í° ID)ë¡œ ë³€í™˜

**ì£¼ìš” ê¸°ëŠ¥**:
1. **í† í°í™”**: í…ìŠ¤íŠ¸ â†’ í† í° ë¦¬ìŠ¤íŠ¸
2. **ì¸ì½”ë”©**: í† í° â†’ í† í° ID
3. **ë””ì½”ë”©**: í† í° ID â†’ í…ìŠ¤íŠ¸

**íŠ¹ìˆ˜ í† í°**:
- `<s>` (BOS): ë¬¸ì¥ ì‹œì‘
- `</s>` (EOS): ë¬¸ì¥ ì¢…ë£Œ
- `<pad>`: íŒ¨ë”©
- `#Person1#`, `#Person2#`: í™”ì êµ¬ë¶„

### 8.3 Attention Mask

**ëª©ì **: ëª¨ë¸ì´ ì£¼ëª©í•´ì•¼ í•  í† í°ê³¼ ë¬´ì‹œí•´ì•¼ í•  í† í° êµ¬ë¶„

```python
attention_mask = [1, 1, 1, 1, 0, 0]  # 1: ì‹¤ì œ í† í°, 0: íŒ¨ë”©
```

### 8.4 Teacher Forcing

í•™ìŠµ ì‹œ ë””ì½”ë”ì— ì •ë‹µì„ ì…ë ¥í•˜ëŠ” ê¸°ë²•:

```
ì‹œê°„ t=0: ì…ë ¥ <s>       â†’ ì¶œë ¥ "ì˜¤ëŠ˜"
ì‹œê°„ t=1: ì…ë ¥ "ì˜¤ëŠ˜"    â†’ ì¶œë ¥ "ë‚ ì”¨ê°€"
ì‹œê°„ t=2: ì…ë ¥ "ë‚ ì”¨ê°€"  â†’ ì¶œë ¥ "ì¢‹ë‹¤"
ì‹œê°„ t=3: ì…ë ¥ "ì¢‹ë‹¤"    â†’ ì¶œë ¥ </s>
```

### 8.5 Beam Search

ì¶”ë¡  ì‹œ ì—¬ëŸ¬ í›„ë³´ë¥¼ ë™ì‹œì— ê³ ë ¤í•˜ëŠ” ë””ì½”ë”© ì „ëµ:

```python
num_beams=4  # ìƒìœ„ 4ê°œ í›„ë³´ ìœ ì§€
```

ê° ë‹¨ê³„ì—ì„œ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ 4ê°œì˜ ì‹œí€€ìŠ¤ë¥¼ ìœ ì§€í•˜ë©° ì§„í–‰

### 8.6 Early Stopping

**í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ**:
- `patience=3`: 3 ì—í­ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
- `threshold=0.001`: ìµœì†Œ ê°œì„  í­

**ìƒì„± ì¡°ê¸° ì¢…ë£Œ**:
- EOS í† í° ìƒì„± ì‹œ ì¢…ë£Œ

### 8.7 Mixed Precision Training (fp16)

**ì¥ì **:
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ê°ì†Œ
- í•™ìŠµ ì†ë„ í–¥ìƒ
- ì •í™•ë„ ì†ì‹¤ ìµœì†Œí™”

```python
fp16=True  # 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì‚¬ìš©
```

### 8.8 Gradient Accumulation

ì‘ì€ ë°°ì¹˜ë¥¼ ì—¬ëŸ¬ ë²ˆ ëˆ„ì í•˜ì—¬ í° ë°°ì¹˜ íš¨ê³¼:

```python
gradient_accumulation_steps=1  # 1ìŠ¤í…ë§ˆë‹¤ ì—…ë°ì´íŠ¸
```

ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = `per_device_batch_size` Ã— `gradient_accumulation_steps`

### 8.9 Learning Rate Scheduler

í•™ìŠµë¥ ì„ ë™ì ìœ¼ë¡œ ì¡°ì •:

```python
lr_scheduler_type='cosine'  # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬
warmup_ratio=0.1           # 10% ì›Œë°ì—…
```

### 8.10 Weight Decay

ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ê°€ì¤‘ì¹˜ ì •ê·œí™”:

```python
weight_decay=0.01  # L2 ì •ê·œí™” ê°•ë„
```

## ë§ˆë¬´ë¦¬

ì´ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œëŠ” ëŒ€í™” ìš”ì•½ì„ ìœ„í•œ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤. ì£¼ìš” êµ¬ì„±ìš”ì†ŒëŠ”:

1. **ë°ì´í„° ì „ì²˜ë¦¬**: ëŒ€í™”ë¬¸ì„ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
2. **ëª¨ë¸ í•™ìŠµ**: BART ëª¨ë¸ì„ í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ì— ë§ê²Œ ë¯¸ì„¸ì¡°ì •
3. **í‰ê°€**: ROUGE ì§€í‘œë¡œ ì„±ëŠ¥ ì¸¡ì •
4. **ì¶”ë¡ **: í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒˆë¡œìš´ ëŒ€í™” ìš”ì•½ ìƒì„±

ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ê°œì„  ë°©í–¥:
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸° ë“±)
- ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©
- ë‹¤ë¥¸ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì‹¤í—˜ (T5, GPT ë“±)
- ì•™ìƒë¸” ê¸°ë²• ì ìš©
