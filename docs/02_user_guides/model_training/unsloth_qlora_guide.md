# unsloth QLoRA ê³ ì„±ëŠ¥ íŒŒì¸íŠœë‹ ê°€ì´ë“œ

## ğŸ¯ ê°œìš”

unslothì™€ QLoRAë¥¼ í™œìš©í•œ ê³ íš¨ìœ¨ íŒŒì¸íŠœë‹ ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœëŒ€ 75% ì ˆì•½í•˜ë©´ì„œ í•™ìŠµ ì†ë„ëŠ” 20-30% í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” ì¥ì 

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- **75% ë©”ëª¨ë¦¬ ì ˆì•½**: unsloth ì‚¬ìš© ì‹œ
- **30-50% ë©”ëª¨ë¦¬ ì ˆì•½**: QLoRA ì‚¬ìš© ì‹œ  
- **4-bit ì–‘ìí™”**: ëª¨ë¸ í¬ê¸° ëŒ€í­ ê°ì†Œ

### ì„±ëŠ¥ í–¥ìƒ
- **í•™ìŠµ ì†ë„ 20-30% í–¥ìƒ**: ìµœì í™”ëœ ì»¤ë„ ì‚¬ìš©
- **ë” ê¸´ ìš”ì•½ ìƒì„±**: decoder_max_len 200 ì§€ì›
- **ì •ë°€í•œ ëª¨ë‹ˆí„°ë§**: steps ê¸°ë°˜ í‰ê°€

## âš¡ ìë™ í™˜ê²½ ê°ì§€ ì‹œìŠ¤í…œ

**í˜„ì¬ êµ¬í˜„ëœ í™˜ê²½ ìë™ ê°ì§€ ì‹œìŠ¤í…œ**ì´ Ubuntu + CUDA í™˜ê²½ì—ì„œ **ìë™ìœ¼ë¡œ Unslothë¥¼ í™œì„±í™”**í•©ë‹ˆë‹¤.

### ìë™ í™œì„±í™” ì¡°ê±´
- **OS**: Ubuntu (Linux)
- **CUDA**: ì‚¬ìš© ê°€ëŠ¥
- **GPU ë©”ëª¨ë¦¬**: 6GB ì´ìƒ
- **CUDA ë²„ì „**: 11.8 ì´ìƒ
- **Unsloth íŒ¨í‚¤ì§€**: ì„¤ì¹˜ë¨

### ìë™ ìµœì í™” ì ìš©
- **RTX 3090 (24GB)**: batch_size=12, bf16=True
- **RTX 4080 (16GB)**: batch_size=8, fp16=True  
- **RTX 4070 (12GB)**: batch_size=6, fp16=True
- **RTX 4060 (8GB)**: batch_size=4, fp16=True

---

## ğŸ”§ ì„¤ì • ë°©ë²•

### 1. ìë™ í™˜ê²½ ê°ì§€ (ê¶Œì¥)

#### ì„¤ì • íŒŒì¼ì—ì„œ Unsloth ìë™ í™œì„±í™”
```yaml
# config/my_model.yaml
model:
  architecture: t5
  checkpoint: my-new-t5-model

qlora:
  # use_unslothë¥¼ ëª…ì‹œí•˜ì§€ ì•ŠìŒ â†’ ìë™ ê°ì§€
  use_qlora: true
```

**ê²°ê³¼**: Ubuntu + CUDA í™˜ê²½ì—ì„œ ìë™ìœ¼ë¡œ Unsloth í™œì„±í™”!

### 2. ìˆ˜ë™ ì„¤ì • (ì„ íƒì )

#### Linux í™˜ê²½ (unsloth ì§€ì›)
```bash
# conda í™˜ê²½ í™œì„±í™”
conda activate nlp-sum-latest

# unsloth ì§€ì› í™•ì¸
python -c "
try:
    import unsloth
    print('âœ… unsloth ì‚¬ìš© ê°€ëŠ¥ (ê³ ì„±ëŠ¥ íŒŒì¸íŠœë‹)')
except ImportError:
    print('âŒ unsloth ì—†ìŒ (QLoRA ëª¨ë“œ ì‚¬ìš©)')
"
```

#### macOS/Windows í™˜ê²½ (QLoRA ì§€ì›)
```bash
# QLoRA ì§€ì› í™•ì¸
python -c "
try:
    import peft, bitsandbytes
    print('âœ… QLoRA ì§€ì› (peft + bitsandbytes)')
except ImportError:
    print('âŒ QLoRA ì§€ì› ì—†ìŒ')
"
```

### 2. ì„¤ì • íŒŒì¼ êµ¬ì„±

#### config.yaml ìµœì í™” ì„¤ì •
```yaml
# ì„±ëŠ¥ ìµœì í™” ì„¤ì •
training:
  decoder_max_len: 200              # ë” ê¸´ ìš”ì•½ ìƒì„±
  eval_strategy: steps              # ì •ë°€í•œ ëª¨ë‹ˆí„°ë§
  eval_steps: 400
  gradient_checkpointing: true      # ë©”ëª¨ë¦¬ ì ˆì•½
  torch_empty_cache_steps: 10       # ë©”ëª¨ë¦¬ ì •ë¦¬
  group_by_length: true             # ë°°ì¹˜ íš¨ìœ¨ì„±
  dataloader_num_workers: 8         # ë³‘ë ¬ ì²˜ë¦¬
  
# QLoRA/unsloth ì„¤ì •
qlora:
  use_unsloth: true                 # Linuxì—ì„œ ìë™ í™œì„±í™”
  use_qlora: true                   # 4-bit ì–‘ìí™”
  lora_rank: 16                     # LoRA ë­í¬
  lora_alpha: 32                    # LoRA ì•ŒíŒŒ
  lora_dropout: 0.05                # LoRA ë“œë¡­ì•„ì›ƒ
  
  # íƒ€ê²Ÿ ëª¨ë“ˆ (SOLAR ëª¨ë¸ ìµœì í™”)
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - out_proj
    - fc1
    - fc2
  
  # 4-bit ì–‘ìí™” ì„¤ì •
  load_in_4bit: true
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"
```

---

## ğŸ’¡ ì‚¬ìš©ë²•

### 1. ê¸°ë³¸ í•™ìŠµ ì‹¤í–‰

```python
from core.trainer import DialogueSummarizationTrainer

# íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” (ìë™ìœ¼ë¡œ unsloth/QLoRA ê°ì§€)
trainer = DialogueSummarizationTrainer(config_path="config.yaml")

# ê³ íš¨ìœ¨ í•™ìŠµ ì‹œì‘
trainer.train()

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
trainer.monitor_memory_usage()
```

### 2. ê³ ê¸‰ ì„¤ì •

#### ë©”ëª¨ë¦¬ ì œí•œ í™˜ê²½ì—ì„œì˜ ì„¤ì •
```yaml
# ê·¹í•œ ë©”ëª¨ë¦¬ ì ˆì•½ ì„¤ì •
training:
  per_device_train_batch_size: 1    # ìµœì†Œ ë°°ì¹˜
  gradient_accumulation_steps: 16   # ì‹¤íš¨ ë°°ì¹˜ í¬ê¸° 16
  gradient_checkpointing: true      # í•„ìˆ˜
  fp16: true                        # í˜¼í•© ì •ë°€ë„
  
qlora:
  lora_rank: 8                      # LoRA ë­í¬ ê°ì†Œ
  load_in_4bit: true               # 4-bit ì–‘ìí™” í•„ìˆ˜
```

#### ì„±ëŠ¥ ìš°ì„  ì„¤ì •
```yaml
# ì„±ëŠ¥ ìµœì í™” ì„¤ì • (ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ í™˜ê²½)
training:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  dataloader_num_workers: 8
  
qlora:
  lora_rank: 32                     # ë†’ì€ ë­í¬
  use_unsloth: true                # Linuxì—ì„œ ìµœê³  ì„±ëŠ¥
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

| ì„¤ì • | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ìƒëŒ€ì  ì ˆì•½ |
|------|---------------|-------------|
| ê¸°ë³¸ ì„¤ì • | 24GB | - |
| QLoRA | 12-16GB | 30-50% â†“ |
| unsloth + QLoRA | 6-8GB | 75% â†“ |

### í•™ìŠµ ì†ë„ ë¹„êµ

| ì„¤ì • | í•™ìŠµ ì‹œê°„ | ìƒëŒ€ì  í–¥ìƒ |
|------|-----------|-------------|
| ê¸°ë³¸ ì„¤ì • | 100ë¶„ | - |
| QLoRA | 85ë¶„ | 15% â†‘ |
| unsloth + QLoRA | 70ë¶„ | 30% â†‘ |

### ìš”ì•½ í’ˆì§ˆ ë¹„êµ

| ì„¤ì • | ROUGE-1 | ROUGE-2 | ROUGE-L |
|------|---------|---------|---------|
| ê¸°ë³¸ (decoder_max_len=100) | 0.425 | 0.178 | 0.389 |
| ìµœì í™” (decoder_max_len=200) | 0.451 | 0.195 | 0.412 |

---

## ğŸ›  íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. unsloth ì„¤ì¹˜ ì‹¤íŒ¨ (macOS)
```bash
# ì˜ˆìƒ í˜„ìƒ
ERROR: Could not build wheels for sentencepiece

# í•´ê²°ë°©ë²•: QLoRA ëª¨ë“œ ì‚¬ìš©
# config.yamlì—ì„œ
qlora:
  use_unsloth: false
  use_qlora: true
```

#### 2. CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ì—ëŸ¬ ë©”ì‹œì§€
RuntimeError: CUDA out of memory

# í•´ê²°ë°©ë²•: ë°°ì¹˜ í¬ê¸° ì¡°ì •
training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
```

#### 3. 4-bit ì–‘ìí™” ì˜¤ë¥˜
```bash
# ì—ëŸ¬ ë©”ì‹œì§€
ValueError: bitsandbytes not properly configured

# í•´ê²°ë°©ë²•: ì˜ì¡´ì„± ì¬ì„¤ì¹˜
pip uninstall bitsandbytes
pip install bitsandbytes==0.41.1
```

### ì„±ëŠ¥ ìµœì í™” íŒ

#### 1. ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
```python
# ë©”ëª¨ë¦¬ì— ë§ëŠ” ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
def find_optimal_batch_size(trainer, max_batch_size=16):
    for batch_size in range(1, max_batch_size + 1):
        try:
            trainer.config['training']['per_device_train_batch_size'] = batch_size
            # í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ì‹¤í–‰
            trainer._test_batch()
            print(f"ìµœì  ë°°ì¹˜ í¬ê¸°: {batch_size}")
            return batch_size
        except RuntimeError as e:
            if "out of memory" in str(e):
                continue
            else:
                raise e
    return 1
```

#### 2. LoRA ë­í¬ ìµœì í™”
```python
# LoRA ë­í¬ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
lora_ranks = [4, 8, 16, 32, 64]
results = {}

for rank in lora_ranks:
    config['qlora']['lora_rank'] = rank
    trainer = DialogueSummarizationTrainer(config)
    
    # ë¹ ë¥¸ í•™ìŠµ ë° í‰ê°€
    metrics = trainer.quick_evaluation()
    results[rank] = metrics['rouge_combined_f1']
    
    print(f"LoRA rank {rank}: ROUGE-F1 {metrics['rouge_combined_f1']:.4f}")

# ìµœì  ë­í¬ ì„ íƒ
optimal_rank = max(results, key=results.get)
print(f"ìµœì  LoRA ë­í¬: {optimal_rank}")
```

---

## ğŸ”¬ ê³ ê¸‰ í™œìš©

### 1. ì»¤ìŠ¤í…€ LoRA ì„¤ì •

#### ë„ë©”ì¸ë³„ ìµœì í™”
```yaml
# ëŒ€í™” ìš”ì•½ íŠ¹í™” ì„¤ì •
qlora:
  target_modules:
    - q_proj      # ì–´í…ì…˜ ì¿¼ë¦¬
    - k_proj      # ì–´í…ì…˜ í‚¤  
    - v_proj      # ì–´í…ì…˜ ê°’
    - out_proj    # ì–´í…ì…˜ ì¶œë ¥
    - fc1         # FFN ì²« ë²ˆì§¸ ë ˆì´ì–´
    - fc2         # FFN ë‘ ë²ˆì§¸ ë ˆì´ì–´
    - lm_head     # ì–¸ì–´ ëª¨ë¸ í—¤ë“œ (ìš”ì•½ ìƒì„± ì¤‘ìš”)
```

#### ê³„ì¸µë³„ ë‹¤ë¥¸ LoRA ë­í¬
```python
# ê³ ê¸‰ LoRA ì„¤ì • (ì½”ë“œ ìˆ˜ì • í•„ìš”)
def setup_layer_specific_lora(model):
    # ì–´í…ì…˜ ë ˆì´ì–´: ë†’ì€ ë­í¬
    attention_modules = ['q_proj', 'k_proj', 'v_proj']
    # FFN ë ˆì´ì–´: ì¤‘ê°„ ë­í¬  
    ffn_modules = ['fc1', 'fc2']
    # ì¶œë ¥ ë ˆì´ì–´: ë‚®ì€ ë­í¬
    output_modules = ['lm_head']
    
    lora_config = {
        'attention': {'rank': 32, 'alpha': 64},
        'ffn': {'rank': 16, 'alpha': 32},
        'output': {'rank': 8, 'alpha': 16}
    }
    
    return lora_config
```

### 2. ë™ì  ë©”ëª¨ë¦¬ ê´€ë¦¬

```python
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
import torch
import gc

class MemoryMonitor:
    def __init__(self):
        self.baseline = self.get_memory_usage()
    
    def get_memory_usage(self):
        if torch.cuda.is_available():
            return {
                'allocated': torch.cuda.memory_allocated(0),
                'reserved': torch.cuda.memory_reserved(0),
                'free': torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)
            }
        return {'cpu': torch.tensor(0).storage().nbytes()}
    
    def cleanup_if_needed(self, threshold=0.9):
        current = self.get_memory_usage()
        
        if torch.cuda.is_available():
            usage_ratio = current['allocated'] / current['reserved']
            if usage_ratio > threshold:
                print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {usage_ratio:.1%}, ì •ë¦¬ ì¤‘...")
                torch.cuda.empty_cache()
                gc.collect()
                print("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

# ì‚¬ìš©ë²•
monitor = MemoryMonitor()

# í•™ìŠµ ë£¨í”„ì—ì„œ
for epoch in range(num_epochs):
    for batch in dataloader:
        # í•™ìŠµ ì½”ë“œ
        ...
        
        # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
        if batch_idx % 50 == 0:
            monitor.cleanup_if_needed()
```

---

## ğŸ“š ì¶”ê°€ ìë£Œ

### ê´€ë ¨ ë¬¸ì„œ
- [í™˜ê²½ ì„¤ì • ê°€ì´ë“œ](../../01_getting_started/environment_reset.md)
- [ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ](./baseline_training.md)
- [í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹](./hyperparameter_tuning.md)
- [ì„±ëŠ¥ ë¶„ì„](../evaluation/performance_analysis.md)

### ì™¸ë¶€ ìë£Œ
- [unsloth ê³µì‹ ë¬¸ì„œ](https://github.com/unslothai/unsloth)
- [QLoRA ë…¼ë¬¸](https://arxiv.org/abs/2305.14314)
- [LoRA ê¸°ë²• ì„¤ëª…](https://arxiv.org/abs/2106.09685)
- [PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬](https://huggingface.co/docs/peft)

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- GPUë³„ ìµœì  ì„¤ì • ê°€ì´ë“œ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§
- ë°°ì¹˜ í¬ê¸° ìµœì í™” ì°¨íŠ¸

---

ì´ ê°€ì´ë“œë¥¼ í†µí•´ unslothì™€ QLoRAë¥¼ í™œìš©í•œ ê³ íš¨ìœ¨ íŒŒì¸íŠœë‹ì„ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í™˜ê²½ì— ë”°ë¼ ì ì ˆí•œ ì„¤ì •ì„ ì„ íƒí•˜ì—¬ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì„¸ìš”.
