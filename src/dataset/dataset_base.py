import os
from torch.utils.data import Dataset
import torch
import pandas as pd
from typing import Dict
from transformers import AutoTokenizer

class SummDataset(Dataset):
    """pd.DataFrameì„ torch.utils.data.Datasetìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, tokenized_data, tokenizer, config):
        """
        :param Dict tokenized_data: tokenizer.tokenizeê°€ ì™„ë£Œëœ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°.
        :param transformers.AutoTokenizer tokenizer: tokenizer
        :param Dict config: í˜¹ì‹œ ëª¨ë¥¼ ì¶”ê°€ ê¸°ëŠ¥ì— ëŒ€ë¹„í•œ config ì¸ì.
        """
        self.tokenized_data = tokenized_data
        self.tokenizer = tokenizer
        self.config = config
    def __getitem__(self, index):
        input_ids = self.tokenized_data['input_ids'][index]

        # ì¶”ë¡ ìš© ë°ì´í„°ì…‹ì¸ ê²½ìš° {"input_ids":[[tokens]...], "labels": None} ì„.
        labels = self.tokenized_data['labels'] ### ë¬¸ì œì˜ ì½”ë“œ
        if labels is not None:
            labels = labels[index]
        else: # None ì¸ ê²½ìš°
            labels = None

        # attention_maskë¥¼ ìƒì„± > attention_maskëŠ” DataCollatorê°€ ìë™ìœ¼ë¡œ ìƒì„±.
        # attention_mask = input_ids.ne(self.tokenizer.pad_token_id)
        return dict(input_ids=input_ids, labels=labels)
    def __len__(self):
        return len(self.tokenized_data['input_ids'])

def tokenize_data(df:pd.DataFrame, tokenizer:AutoTokenizer, config:Dict, test:bool=False):
    """pd.DataFrameì—ì„œ dialogueì™€ summaryë¥¼ í† í°í™”í•˜ëŠ” í•¨ìˆ˜

    :param pd.DataFrame df: train, dev, test csv
    :param transformers.AutoTokenizer tokenizer: tokenizer
    :param Dict config: _description_
    :param bool test: Trueì´ë©´ summaryë¥¼ í† í°í™”í•˜ì§€ ì•ŠëŠ”ë‹¤.
    :return _type_: _description_
    """
    dialogues = df['dialogue']
    # tokenize dialogue
    tokenized_dialogues = [
        tokenizer(
            dialogue,
            # padding=False, # DataCollatorForSeq2Seqì—ì„œ ë™ì ìœ¼ë¡œ paddingì„ í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œëŠ” paddingì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
            truncation=True,
            max_length=config['tokenizer']['encoder_max_len'],
            add_special_tokens=True
        )['input_ids'] for dialogue in dialogues.values
    ]
    
    # summary ì²˜ë¦¬ 
    # testì˜ ê²½ìš° summaryê°€ ì—†ìœ¼ë‹ˆ, Noneìœ¼ë¡œ ì¶œë ¥.
    # trainì˜ ê²½ìš° summaryê°€ ìˆìœ¼ë‹ˆ, summaryë¥¼ í† í°í™”í•˜ì—¬ labelsë¥¼ ì±„ìš´ë‹¤. 
    tokenized_summaries = None
    if not test:
        summaries = df['summary']
        tokenized_summaries = [
            tokenizer(
                summary,
                # padding=False, # DataCollatorForSeq2Seqì—ì„œ ë™ì ìœ¼ë¡œ paddingì„ í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œëŠ” paddingì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
                truncation=True,
                max_length=config['tokenizer']['decoder_max_len'],
                add_special_tokens=True
            )['input_ids'] for summary in summaries.values
        ]
        # íŒ¨ë”©ëœ ë¶€ë¶„ì„ -100ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ í•™ìŠµì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
        """
        
        """
        # tokenized_summaries = [[-100 if token == tokenizer.pad_token_id else token for token in summary] for summary in tokenized_summaries]

    out = {'input_ids': tokenized_dialogues, 'labels': tokenized_summaries}
    print("="*15, "tokenizing start" ,"="*15)
    print("tokenizing ëœ ë°ì´í„° í˜•íƒœ ì˜ˆì‹œ")
    print(tokenizer.convert_ids_to_tokens(tokenized_dialogues[-1]))
    print("labelì˜ í˜•íƒœ ì˜ˆì‹œ")
    print(tokenizer.convert_ids_to_tokens(tokenized_summaries[-1]) if tokenized_summaries is not None else "None")
    print("="*15, "tokenizing end" ,"="*15)
    return out
    
def prepare_train_dataset(tokenizer, config, practice=False):
    """train, val, test SummDatasetì„ ì¤€ë¹„

    :param transformers.AutoTokenizer tokenizer: tokenizer
    :param Dict config: _description_
    :param bool practice: Trueì´ë©´, ì½”ë“œ ì‹¤í—˜ìš©ì´ë¯€ë¡œ trainì€ 256, valì€ 10ê°œë§Œ ë°˜í™˜í•œë‹¤.
    :return _type_: _description_
    """
    # load data
    print("="*15, "load train" ,"="*15)
    train_df = Preprocess.make_set_as_df(
        file_path=config['general']['train_data'],
        is_train=True,
        config=config
    )
    print("="*15, "load val" ,"="*15)
    val_df = Preprocess.make_set_as_df(
        file_path=config['general']['val_data'],
        is_train=True,
        config=config
    )
    print("="*15, "load test" ,"="*15)
    test_df = Preprocess.make_set_as_df(
        file_path=config['general']['test_data'],
        is_train=False,
        config=config
    )

    if practice:
        train_df = train_df.iloc[:256]
        val_df = val_df.iloc[:10]
        test_df = test_df.iloc[:10]

    # print data info
    print("="*15, "ë°ì´í„° ê°œìˆ˜" ,"="*15)
    print(f"train_df.shape: {train_df.shape}")
    print(f"val_df.shape: {val_df.shape}")
    print(f"test_df.shape: {test_df.shape}")
    print("="*15, "ë°ì´í„° ê°œìˆ˜" ,"="*15)
    print()

    # tokenize
    print("="*15, "í† í°í™” ì§„í–‰ ì¤‘..." ,"="*15)
    tokenized_train = tokenize_data(df=train_df, tokenizer=tokenizer, config=config, test=False)
    tokenized_val = tokenize_data(df=val_df, tokenizer=tokenizer, config=config, test=False)
    # tokenized_test = tokenize_data(df=test_df, tokenizer=tokenizer, config=config, test=True)
    print("="*15, "í† í°í™” ì™„ë£Œ" ,"="*15)
    print()

    # make SummDataset
    print("="*15, "make SummDataset..." ,"="*15)
    summ_train_dataset = SummDataset(tokenized_data=tokenized_train, tokenizer=tokenizer, config=config)
    summ_val_dataset = SummDataset(tokenized_data=tokenized_val, tokenizer=tokenizer, config=config)
    # summ_test_dataset = SummDataset(tokenized_data=tokenized_test, tokenizer=tokenizer, config=config)
    print("="*15, "SummDataset ì™„ë£Œ" ,"="*15)

    print("="*15, "SummDataset í™•ì¸" ,"="*15)
    out = summ_train_dataset.__getitem__(0)
    print("="*15, "SummDataset í™•ì¸ ì™„ë£Œ" ,"="*15)

    return summ_train_dataset, summ_val_dataset

def prepare_test_dataset(config, tokenizer, val_flag=False, practice=False):

    if val_flag:
        test_file_path = os.path.join(config['general']['data_path'], config['general']['val_data'])
    else:
        test_file_path = os.path.join(config['general']['data_path'], config['general']['test_data'])

    test_df = pd.read_csv(test_file_path)

    if practice:
        test_df = test_df.iloc[:10]

    print('-'*150)
    print(f'test_data:\n{test_df["dialogue"][0]}')
    print('-'*150)

    tokenized_test = tokenize_data(df=test_df, tokenizer=tokenizer, config=config, test=True)
    summ_test_dataset = SummDataset(tokenized_data=tokenized_test, tokenizer=tokenizer, config=config)

    return test_df, summ_test_dataset


### ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ###
import re
from typing import List

# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ë¡œ, ë°ì´í„°ì…‹ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
class Preprocess:
    # í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì„œë“œ
    def __init__(self) -> None:
        pass

    @staticmethod
    # ì‹¤í—˜ì— í•„ìš”í•œ ì»¬ëŸ¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # ì •ì  ë©”ì„œë“œë¡œ, í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì—†ì´ í˜¸ì¶œ ê°€ëŠ¥
    def make_set_as_df(file_path, is_train = True, config=None):
        def load_df(file_path, is_train, config):
            df = pd.read_csv(file_path) # CSV íŒŒì¼ì„ ì½ì–´ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            # ğŸ” ë°œí™”ì ê¸°ë°˜ ì§€ì‹œí‘œí˜„ ë³´ì™„ ì „ì²˜ë¦¬ ì ìš©
            df['dialogue'] = df['dialogue'].apply(resolve_deictic_with_speaker)
            # ğŸ” í…ìŠ¤íŠ¸ í´ë¦° í•¨ìˆ˜
            df['dialogue'] = df['dialogue'].apply(clean_text)

            ### special tokenì— #Topic# ì´ ìˆìœ¼ë©´, ì§€ì‹œì–´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€.
            if config is not None and '#Topic#' in config['tokenizer']['special_tokens']:
                df = df.apply(add_instructions, axis=1)

            ### change \n to SEP token
            if config is not None and config['tokenizer'].get('sep_token', None):
                df['dialogue'] = df['dialogue'].apply(lambda x : apply_sep_token(x, config['tokenizer']['sep_token']))

            # is_train í”Œë˜ê·¸ê°€ Trueì´ë©´ í•™ìŠµìš© ë°ì´í„°ë¡œ ì²˜ë¦¬
            if is_train:
                train_df = df[['fname','dialogue','summary']] # 'fname', 'dialogue', 'summary' ì»¬ëŸ¼ ì„ íƒ
                return train_df # ìƒì„±ëœ í•™ìŠµ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
            # is_train í”Œë˜ê·¸ê°€ Falseì´ë©´ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œ ì²˜ë¦¬
            else:
                test_df = df[['fname','dialogue']] # 'fname', 'dialogue' ì»¬ëŸ¼ ì„ íƒ
                return test_df # ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜

        # ë§Œì•½ file_pathê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ëœë‹¤ë©´ merge í•´ë¼.
        if isinstance(file_path, List):
            df = []
            for fp in file_path:
                df_ = load_df(os.path.join(config['general']['data_path'],fp), is_train, config)
                df.append(df_)
            df = pd.concat(df, axis=0) # í–‰ì„ ëŠ˜ë¦¼

        else: # file_pathê°€ ë‹¨ì¼ ë¬¸ìì—´ì¼ ë•Œ
            df = load_df(os.path.join(config['general']['data_path'],file_path), is_train, config)

        return df


# ì§€ì‹œí‘œí˜„ ë³´ì™„ í•¨ìˆ˜: ì§ì „ ë°œí™”ì ì •ë³´ë¡œ ì§€ì‹œì–´ ëŒ€ì²´
def resolve_deictic_with_speaker(dialogue: str) -> str:
    deictic_phrases = ['ê·¸ ì‚¬ëŒ', 'ì´ ì‚¬ëŒ', 'ê·¸ê±°', 'ì´ê±°', 'ê·¸ê±´', 'ì´ê±´', 'ê±°ê¸°', 'ì €ê¸°', 'ì—¬ê¸°']
    lines = str(dialogue).split('\n')
    resolved = []
    last_speaker = None

    for line in lines:
        line = line.strip()
        if not line:
            continue

        match = re.match(r'^(#Person\d+#):\s*(.*)', line)
        if match:
            speaker = match.group(1)
            utterance = match.group(2)

            for deictic in deictic_phrases:
                if deictic in utterance and last_speaker:
                    utterance = utterance.replace(deictic, f'{last_speaker}ê°€ ë§í•œ')

            last_speaker = speaker
            resolved.append(f"{speaker}: {utterance}")
        else:
            resolved.append(line)

    return '\n'.join(resolved)

# í…ìŠ¤íŠ¸ í´ë¦° í•¨ìˆ˜
def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    
    # ì¤„ë°”ê¿ˆ í‘œí˜„ í†µì¼
    text = text.replace("\\n", "\n").replace("<br>", "\n").replace("</s>", "\n")

    ### íŠ¹ì´ ì¼€ì´ìŠ¤ : train.csvì—ëŠ” 'ã…ã…'ê°€ ì˜¤ì§ 1ê°œ ì¡´ì¬í•œë‹¤. ê·¸ëŸ°ë° ì´ê²ƒì´ #Person2#: ã…ã… ë¼ì„œ ë¹ˆë¬¸ìì—´ë¡œ ëŒ€ì²´í•˜ë©´ ë§ì´ ì—†ì–´ì§„ë‹¤.
    # ë¬¸ë§¥ê³¼ summaryì— ë§ì¶° 'ë‚˜ë„ í–‰ë³µí•´.'ë¡œ ë°”ê¾¼ë‹¤.
    text = text.replace("ã…ã…", "ë‚˜ë„ í–‰ë³µí•´.")

    # ìì†Œë§Œ ìˆëŠ” ë‹¨ì–´ ì œê±° (ì˜ˆ: ã…‹ã…‹, ã…‡ã…‹, ã…œã…œ) > ì´ëª¨í‹°ì½˜
    text = re.sub(r"\b[ã„±-ã…ã…-ã…£]{2,}\b", "", text)

    # ì¤‘ë³µ ì¤„ë°”ê¿ˆ ì œê±°
    text = re.sub(r"\n+", r"\n", text)

    # ì¤‘ë³µ ê³µë°± ì œê±°
    text = re.sub(r"\s+", ' ', text)

    return text.strip()

def add_instructions(row:pd.Series) -> pd.Series:
    """ì§€ì‹œì–´ í”„ë¡¬í”„íŠ¸ ì¶”ê°€.

    :param str dialogue: _description_
    :return str: _description_
    """
    try:
        topic = str(row['topic']).strip()
        dialogue = row['dialogue']
        dialogue = f"#Topic#{topic}\n#Dialogue#{dialogue}"
        row['dialogue'] = dialogue
    ##Topic#','#Dialogue#'
    except:
        return row
    return row

def apply_sep_token(text:str, sep_token:str) -> str:
    return re.sub(r"\n", sep_token, text)